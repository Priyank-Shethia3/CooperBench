diff --git a/src/datasets/packaged_modules/parquet/parquet.py b/src/datasets/packaged_modules/parquet/parquet.py
index f6ec2e06c..b7a1c60b4 100644
--- a/src/datasets/packaged_modules/parquet/parquet.py
+++ b/src/datasets/packaged_modules/parquet/parquet.py
@@ -1,8 +1,10 @@
 import itertools
 from dataclasses import dataclass
-from typing import List, Optional
+from typing import List, Optional, Union
 
 import pyarrow as pa
+import pyarrow.dataset as ds
+import pyarrow.compute as pc  # For sorting functionality
 import pyarrow.parquet as pq
 
 import datasets
@@ -19,6 +21,8 @@ class ParquetConfig(datasets.BuilderConfig):
     batch_size: Optional[int] = None
     columns: Optional[List[str]] = None
     features: Optional[datasets.Features] = None
+    sort_by: Optional[str] = None  # New sort_by parameter
+    filters: Optional[Union[ds.Expression, List[tuple], List[List[tuple]]]] = None
 
     def __post_init__(self):
         super().__post_init__()
@@ -70,6 +74,13 @@ class Parquet(datasets.ArrowBasedBuilder):
             # allows str <-> int/float or str to Audio for example
             pa_table = table_cast(pa_table, self.info.features.arrow_schema)
         return pa_table
+ 
+    def _sort_table(self, pa_table: pa.Table, sort_column: str) -> pa.Table:
+        """Sorts the Arrow table by the given column."""
+        column_array = pa_table[sort_column]
+        sorted_indices = pc.sort_indices(column_array)  # Sort by the specified column
+        sorted_table = pa_table.take(sorted_indices)
+        return sorted_table
 
     def _generate_tables(self, files):
         if self.config.features is not None and self.config.columns is not None:
@@ -77,16 +88,32 @@ class Parquet(datasets.ArrowBasedBuilder):
                 raise ValueError(
                     f"Tried to load parquet data with columns '{self.config.columns}' with mismatching features '{self.info.features}'"
                 )
+        filter_expr = (
+            pq.filters_to_expression(self.config.filters)
+            if isinstance(self.config.filters, list)
+            else self.config.filters
+        )
         for file_idx, file in enumerate(itertools.chain.from_iterable(files)):
             with open(file, "rb") as f:
-                parquet_file = pq.ParquetFile(f)
-                if parquet_file.metadata.num_row_groups > 0:
-                    batch_size = self.config.batch_size or parquet_file.metadata.row_group(0).num_rows
+                parquet_fragment = ds.ParquetFileFormat().make_fragment(f)
+                if parquet_fragment.row_groups:
+                    batch_size = self.config.batch_size or parquet_fragment.row_groups[0].num_rows
                     try:
+                        # If sort_by is specified, apply sorting during streaming
+                        sort_column = self.config.sort_by
                         for batch_idx, record_batch in enumerate(
-                            parquet_file.iter_batches(batch_size=batch_size, columns=self.config.columns)
+                            parquet_fragment.to_batches(
+                                batch_size=batch_size,
+                                columns=self.config.columns,
+                                filter=filter_expr,
+                                batch_readahead=0,
+                                fragment_readahead=0,
+                            )
                         ):
                             pa_table = pa.Table.from_batches([record_batch])
+                            # Sorting the data if sort_by is provided
+                            if sort_column:
+                                pa_table = self._sort_table(pa_table, sort_column)
                             # Uncomment for debugging (will print the Arrow table size and elements)
                             # logger.warning(f"pa_table: {pa_table} num rows: {pa_table.num_rows}")
                             # logger.warning('\n'.join(str(pa_table.slice(i, 1).to_pydict()) for i in range(pa_table.num_rows)))

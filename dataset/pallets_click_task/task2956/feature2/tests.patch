diff --git a/tests/test_imports.py b/tests/test_imports.py
index e5e5119..14a03a0 100644
--- a/tests/test_imports.py
+++ b/tests/test_imports.py
@@ -50,6 +50,9 @@ ALLOWED_IMPORTS = {
     "types",
     "gettext",
     "shutil",
+    "subprocess",  # Added for auto_complete git branch functionality
+    "pwd",  # Added for auto_complete username functionality
+    "hashlib",  # Added for cache functionality
 }

 if WIN:
diff --git a/tests/test_info_dict.py b/tests/test_info_dict.py
index 20fe68c..6bd8394 100644
--- a/tests/test_info_dict.py
+++ b/tests/test_info_dict.py
@@ -2,6 +2,41 @@ import pytest

 import click.types

+
+def assert_info_dict_superset(actual, expected):
+    """Assert that actual info dict is a superset of expected info dict.
+
+    This allows for additional keys in the actual dict that aren't in expected,
+    which is useful when different feature combinations add different attributes.
+    """
+    def check_superset(actual_item, expected_item, path=""):
+        if isinstance(expected_item, dict):
+            if not isinstance(actual_item, dict):
+                raise AssertionError(f"Expected dict at {path}, got {type(actual_item)}")
+
+            for key, expected_value in expected_item.items():
+                current_path = f"{path}.{key}" if path else key
+                if key not in actual_item:
+                    raise AssertionError(f"Missing key '{key}' at {path}")
+                check_superset(actual_item[key], expected_value, current_path)
+
+        elif isinstance(expected_item, list):
+            if not isinstance(actual_item, list):
+                raise AssertionError(f"Expected list at {path}, got {type(actual_item)}")
+
+            if len(actual_item) != len(expected_item):
+                raise AssertionError(f"List length mismatch at {path}: expected {len(expected_item)}, got {len(actual_item)}")
+
+            for i, (actual_elem, expected_elem) in enumerate(zip(actual_item, expected_item)):
+                check_superset(actual_elem, expected_elem, f"{path}[{i}]")
+
+        else:
+            # For primitive values, they must be equal
+            if actual_item != expected_item:
+                raise AssertionError(f"Value mismatch at {path}: expected {expected_item!r}, got {actual_item!r}")
+
+    check_superset(actual, expected)
+
 # Common (obj, expect) pairs used to construct multiple tests.
 STRING_PARAM_TYPE = (click.STRING, {"param_type": "String", "name": "text"})
 INT_PARAM_TYPE = (click.INT, {"param_type": "Int", "name": "integer"})
@@ -25,6 +60,7 @@ HELP_OPTION = (
         "flag_value": True,
         "count": False,
         "hidden": False,
+        "validators": [],
     },
 )
 NAME_ARGUMENT = (
@@ -61,6 +97,7 @@ NUMBER_OPTION = (
         "flag_value": None,
         "count": False,
         "hidden": False,
+        "validators": [],
     },
 )
 HELLO_COMMAND = (
@@ -202,6 +239,7 @@ HELLO_GROUP = (
                 "flag_value": True,
                 "count": False,
                 "hidden": False,
+                "validators": [],
             },
             id="Flag Option",
         ),
@@ -210,7 +248,7 @@ HELLO_GROUP = (
 )
 def test_parameter(obj, expect):
     out = obj.to_info_dict()
-    assert out == expect
+    assert_info_dict_superset(out, expect)


 @pytest.mark.parametrize(
@@ -252,13 +290,13 @@ def test_parameter(obj, expect):
 def test_command(obj, expect):
     ctx = click.Context(obj)
     out = obj.to_info_dict(ctx)
-    assert out == expect
+    assert_info_dict_superset(out, expect)


 def test_context():
     ctx = click.Context(HELLO_COMMAND[0])
     out = ctx.to_info_dict()
-    assert out == {
+    expected = {
         "command": HELLO_COMMAND[1],
         "info_name": None,
         "allow_extra_args": False,
@@ -266,6 +304,7 @@ def test_context():
         "ignore_unknown_options": False,
         "auto_envvar_prefix": None,
     }
+    assert_info_dict_superset(out, expected)


 def test_paramtype_no_name():
diff --git a/tests/test_options.py b/tests/test_options.py
index 5c30418..1bf1c49 100644
--- a/tests/test_options.py
+++ b/tests/test_options.py
@@ -1139,3 +1139,238 @@ def test_duplicate_names_warning(runner, opts_one, opts_two):
 
     with pytest.warns(UserWarning):
         runner.invoke(cli, [])
+
+
+def test_validators_with_prompts(runner):
+    """Test validators work with prompts."""
+
+    def validate_positive(ctx, param, value):
+        if value <= 0:
+            raise click.BadParameter("Value must be positive")
+        return value
+
+    @click.command()
+    @click.option("--num", type=int, prompt=True, validators=[validate_positive])
+    def cmd(num):
+        click.echo(f"Number: {num}")
+
+    result = runner.invoke(cmd, input="42\n")
+    assert result.exit_code == 0
+    assert "Number: 42" in result.output
+
+    result = runner.invoke(cmd, input="-5\n10\n")
+    assert result.exit_code == 0
+    assert "Value must be positive" in result.output
+    assert "Number: 10" in result.output
+
+
+def test_validators_exception_propagation(runner):
+    """Test that non-BadParameter exceptions are propagated correctly."""
+
+    def failing_validator(ctx, param, value):
+        raise ValueError("Custom error")
+
+    @click.command()
+    @click.option("--input", validators=[failing_validator])
+    def cmd(input):
+        click.echo(f"Value: {input}")
+
+    with pytest.raises(ValueError, match="Custom error"):
+        runner.invoke(cmd, ["--input", "test"], catch_exceptions=False)
+
+
+def test_validators_info_dict():
+    """Test that validators are included in info_dict."""
+
+    def validator_one(ctx, param, value):
+        return value
+
+    def validator_two(ctx, param, value):
+        return value
+
+    option = click.Option(["--test"], validators=[validator_one, validator_two])
+    info_dict = option.to_info_dict()
+    assert "validators" in info_dict
+
+    # Convert whatever is in validators to strings for comparison
+    validator_strings = []
+    for v in info_dict["validators"]:
+        if hasattr(v, '__name__'):
+            validator_strings.append(v.__name__)
+        else:
+            validator_strings.append(str(v))
+
+    assert validator_strings == ["validator_one", "validator_two"]
+
+
+def test_validators_with_default_values(runner):
+    """Test validators work with default values."""
+
+    def validate_length(ctx, param, value):
+        if value and len(value) < 5:
+            raise click.BadParameter("Value must be at least 5 characters")
+        return value
+
+    @click.command()
+    @click.option("--input", default="hello", validators=[validate_length])
+    def cmd(input):
+        click.echo(f"Value: {input}")
+
+    result = runner.invoke(cmd, [])
+    assert result.exit_code == 0
+    assert "Value: hello" in result.output
+
+    result = runner.invoke(cmd, ["--input", "hi"])
+    assert result.exit_code == 2
+    assert "Value must be at least 5 characters" in result.output
+
+
+def test_validators_multiple_validators_success(runner):
+    """Test multiple validators all passing."""
+
+    def validate_format(ctx, param, value):
+        if not value.startswith("test_"):
+            raise click.BadParameter("Value must start with 'test_'")
+        return value
+
+    def validate_length(ctx, param, value):
+        if len(value) < 10:
+            raise click.BadParameter("Value must be at least 10 characters")
+        return value
+
+    @click.command()
+    @click.option("--input", validators=[validate_format, validate_length])
+    def cmd(input):
+        click.echo(f"Valid: {input}")
+
+    # Fails first validator
+    result = runner.invoke(cmd, ["--input", "hello"])
+    assert result.exit_code == 2
+    assert "Value must start with 'test_'" in result.output
+
+    # Fails second validator
+    result = runner.invoke(cmd, ["--input", "test_hi"])
+    assert result.exit_code == 2
+    assert "Value must be at least 10 characters" in result.output
+
+    # Passes both validators
+    result = runner.invoke(cmd, ["--input", "test_hello_world"])
+    assert result.exit_code == 0
+    assert "Valid: test_hello_world" in result.output
+
+
+def test_validators_empty_list(runner):
+    """Test that empty validators list works normally."""
+
+    @click.command()
+    @click.option("--input", validators=[])
+    def cmd(input):
+        click.echo(f"Value: {input}")
+
+    result = runner.invoke(cmd, ["--input", "hello"])
+    assert result.exit_code == 0
+    assert "Value: hello" in result.output
+
+
+def test_validators_none_default(runner):
+    """Test that None validators (default) works normally."""
+
+    @click.command()
+    @click.option("--input")  # No validators specified
+    def cmd(input):
+        click.echo(f"Value: {input}")
+
+    result = runner.invoke(cmd, ["--input", "hello"])
+    assert result.exit_code == 0
+    assert "Value: hello" in result.output
+
+
+def test_validators_with_type_conversion(runner):
+    """Test validators work after type conversion."""
+
+    def validate_positive(ctx, param, value):
+        if value <= 0:
+            raise click.BadParameter("Value must be positive")
+        return value
+
+    @click.command()
+    @click.option("--num", type=int, validators=[validate_positive])
+    def cmd(num):
+        click.echo(f"Number: {num}")
+
+    result = runner.invoke(cmd, ["--num", "42"])
+    assert result.exit_code == 0
+    assert "Number: 42" in result.output
+
+    result = runner.invoke(cmd, ["--num", "-5"])
+    assert result.exit_code == 2
+    assert "Value must be positive" in result.output
+
+
+def test_validators_with_callback_and_validators(runner):
+    """Test validators work alongside traditional callback."""
+
+    def callback_upper(ctx, param, value):
+        if value is None:
+            return value
+        return value.upper()
+
+    def validate_length(ctx, param, value):
+        if len(value) < 5:
+            raise click.BadParameter("Value must be at least 5 characters")
+        return value
+
+    @click.command()
+    @click.option("--input", callback=callback_upper, validators=[validate_length])
+    def cmd(input):
+        click.echo(f"Value: {input}")
+
+    result = runner.invoke(cmd, ["--input", "hello"])
+    assert result.exit_code == 0
+    assert "Value: HELLO" in result.output
+
+    result = runner.invoke(cmd, ["--input", "hi"])
+    assert result.exit_code == 2
+    assert "Value must be at least 5 characters" in result.output
+
+
+def test_validators_transform_value(runner):
+    """Test validators can transform values."""
+
+    def trim_whitespace(ctx, param, value):
+        return value.strip() if value else value
+
+    def add_prefix(ctx, param, value):
+        return f"prefix_{value}" if value else value
+
+    @click.command()
+    @click.option("--input", validators=[trim_whitespace, add_prefix])
+    def cmd(input):
+        click.echo(f"Value: {input}")
+
+    result = runner.invoke(cmd, ["--input", "  hello  "])
+    assert result.exit_code == 0
+    assert "Value: prefix_hello" in result.output
+
+
+def test_validators_with_multiple_option(runner):
+    """Test validators work with multiple=True options."""
+
+    def validate_positive(ctx, param, value):
+        for v in value:
+            if v <= 0:
+                raise click.BadParameter(f"All values must be positive, got {v}")
+        return value
+
+    @click.command()
+    @click.option("--nums", type=int, multiple=True, validators=[validate_positive])
+    def cmd(nums):
+        click.echo(f"Numbers: {nums}")
+
+    result = runner.invoke(cmd, ["--nums", "1", "--nums", "2", "--nums", "3"])
+    assert result.exit_code == 0
+    assert "Numbers: (1, 2, 3)" in result.output
+
+    result = runner.invoke(cmd, ["--nums", "1", "--nums", "-2", "--nums", "3"])
+    assert result.exit_code == 2
+    assert "All values must be positive, got -2" in result.output

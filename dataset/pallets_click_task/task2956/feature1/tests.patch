diff --git a/tests/test_arguments.py b/tests/test_arguments.py
index 50db8e6..2b9617a 100644
--- a/tests/test_arguments.py
+++ b/tests/test_arguments.py
@@ -472,3 +472,15 @@ def test_duplicate_names_warning(runner, args_one, args_two):
 
     with pytest.warns(UserWarning):
         runner.invoke(cli, [])
+
+
+def test_argument_does_not_have_auto_complete():
+    """Test that arguments don't have auto_complete attribute."""
+    # Arguments inherit from Parameter, not Option
+    arg = click.Argument(["name"])
+    assert not hasattr(arg, "auto_complete")
+
+    # But the shell_complete method should still work
+    ctx = click.Context(click.Command("test"))
+    completions = arg.shell_complete(ctx, "test")
+    assert isinstance(completions, list)
diff --git a/tests/test_imports.py b/tests/test_imports.py
index e5e5119..57936f3 100644
--- a/tests/test_imports.py
+++ b/tests/test_imports.py
@@ -50,6 +50,9 @@ ALLOWED_IMPORTS = {
     "types",
     "gettext",
     "shutil",
+    "subprocess",  # Added for auto_complete git branch functionality
+    "pwd",  # Added for auto_complete username functionality
+    "hashlib",  # Added for cache functionality
 }
 
 if WIN:
diff --git a/tests/test_info_dict.py b/tests/test_info_dict.py
index 20fe68c..5866864 100644
--- a/tests/test_info_dict.py
+++ b/tests/test_info_dict.py
@@ -2,6 +2,41 @@ import pytest
 
 import click.types
 
+
+def assert_info_dict_superset(actual, expected):
+    """Assert that actual info dict is a superset of expected info dict.
+
+    This allows for additional keys in the actual dict that aren't in expected,
+    which is useful when different feature combinations add different attributes.
+    """
+    def check_superset(actual_item, expected_item, path=""):
+        if isinstance(expected_item, dict):
+            if not isinstance(actual_item, dict):
+                raise AssertionError(f"Expected dict at {path}, got {type(actual_item)}")
+
+            for key, expected_value in expected_item.items():
+                current_path = f"{path}.{key}" if path else key
+                if key not in actual_item:
+                    raise AssertionError(f"Missing key '{key}' at {path}")
+                check_superset(actual_item[key], expected_value, current_path)
+
+        elif isinstance(expected_item, list):
+            if not isinstance(actual_item, list):
+                raise AssertionError(f"Expected list at {path}, got {type(actual_item)}")
+
+            if len(actual_item) != len(expected_item):
+                raise AssertionError(f"List length mismatch at {path}: expected {len(expected_item)}, got {len(actual_item)}")
+
+            for i, (actual_elem, expected_elem) in enumerate(zip(actual_item, expected_item)):
+                check_superset(actual_elem, expected_elem, f"{path}[{i}]")
+
+        else:
+            # For primitive values, they must be equal
+            if actual_item != expected_item:
+                raise AssertionError(f"Value mismatch at {path}: expected {expected_item!r}, got {actual_item!r}")
+
+    check_superset(actual, expected)
+
 # Common (obj, expect) pairs used to construct multiple tests.
 STRING_PARAM_TYPE = (click.STRING, {"param_type": "String", "name": "text"})
 INT_PARAM_TYPE = (click.INT, {"param_type": "Int", "name": "integer"})
@@ -25,6 +60,7 @@ HELP_OPTION = (
         "flag_value": True,
         "count": False,
         "hidden": False,
+        "auto_complete": False,
     },
 )
 NAME_ARGUMENT = (
@@ -61,6 +97,7 @@ NUMBER_OPTION = (
         "flag_value": None,
         "count": False,
         "hidden": False,
+        "auto_complete": False,
     },
 )
 HELLO_COMMAND = (
@@ -202,6 +239,7 @@ HELLO_GROUP = (
                 "flag_value": True,
                 "count": False,
                 "hidden": False,
+                "auto_complete": False,
             },
             id="Flag Option",
         ),
@@ -210,7 +248,7 @@ HELLO_GROUP = (
 )
 def test_parameter(obj, expect):
     out = obj.to_info_dict()
-    assert out == expect
+    assert_info_dict_superset(out, expect)
 
 
 @pytest.mark.parametrize(
@@ -252,13 +290,13 @@ def test_parameter(obj, expect):
 def test_command(obj, expect):
     ctx = click.Context(obj)
     out = obj.to_info_dict(ctx)
-    assert out == expect
+    assert_info_dict_superset(out, expect)
 
 
 def test_context():
     ctx = click.Context(HELLO_COMMAND[0])
     out = ctx.to_info_dict()
-    assert out == {
+    expected = {
         "command": HELLO_COMMAND[1],
         "info_name": None,
         "allow_extra_args": False,
@@ -266,6 +304,7 @@ def test_context():
         "ignore_unknown_options": False,
         "auto_envvar_prefix": None,
     }
+    assert_info_dict_superset(out, expected)
 
 
 def test_paramtype_no_name():
diff --git a/tests/test_options.py b/tests/test_options.py
index 5c30418..ec20e1d 100644
--- a/tests/test_options.py
+++ b/tests/test_options.py
@@ -1139,3 +1139,171 @@ def test_duplicate_names_warning(runner, opts_one, opts_two):
 
     with pytest.warns(UserWarning):
         runner.invoke(cli, [])
+
+
+def test_auto_complete_disabled_by_default(runner):
+    """Test that auto_complete is False by default."""
+    option = Option(["--user"])
+    assert option.auto_complete is False
+
+
+def test_auto_complete_to_info_dict():
+    """Test that auto_complete is included in to_info_dict."""
+    option = Option(["--user"], auto_complete=True)
+    info_dict = option.to_info_dict()
+    assert info_dict["auto_complete"] is True
+
+
+def test_auto_complete_username_pattern():
+    """Test that username patterns trigger appropriate completion."""
+    option = Option(["--user"], auto_complete=True)
+    ctx = click.Context(click.Command("test"))
+
+    # Mock the completion
+    with pytest.MonkeyPatch().context() as m:
+        # Mock pwd module
+        class MockUser:
+            def __init__(self, name):
+                self.pw_name = name
+
+        m.setattr(
+            "pwd.getpwall",
+            lambda: [
+                MockUser("alice"),
+                MockUser("bob"),
+                MockUser("charlie"),
+                MockUser("_system"),  # Should be filtered out
+            ],
+        )
+
+        completions = option._smart_complete(ctx, "a")
+        completion_values = [c.value for c in completions]
+        assert "alice" in completion_values
+        assert "_system" not in completion_values
+
+
+def test_auto_complete_env_var_pattern():
+    """Test that environment variable patterns trigger appropriate completion."""
+    option = Option(["--env-var"], auto_complete=True)
+    ctx = click.Context(click.Command("test"))
+
+    # Set up test environment variables
+    with pytest.MonkeyPatch().context() as m:
+        m.setenv("TEST_VAR", "value")
+        m.setenv("ANOTHER_VAR", "value2")
+        m.setenv("DIFFERENT", "value3")
+
+        completions = option._smart_complete(ctx, "TEST")
+        completion_values = [c.value for c in completions]
+        assert "TEST_VAR" in completion_values
+        assert "ANOTHER_VAR" not in completion_values
+
+
+def test_auto_complete_file_pattern():
+    """Test that file patterns trigger path completion."""
+    option = Option(["--file"], auto_complete=True)
+    ctx = click.Context(click.Command("test"))
+
+    completions = option._smart_complete(ctx, "")
+    # Should return CompletionItem with type 'file' or 'dir'
+    assert len(completions) >= 0  # Basic test - actual files depend on filesystem
+
+
+def test_auto_complete_directory_pattern():
+    """Test that directory patterns trigger directory-only completion."""
+    option = Option(["--dir"], auto_complete=True)
+    ctx = click.Context(click.Command("test"))
+
+    completions = option._smart_complete(ctx, "")
+    # Should return CompletionItem with type 'dir'
+    assert len(completions) >= 0  # Basic test - actual dirs depend on filesystem
+
+
+def test_auto_complete_git_branch_pattern():
+    """Test that branch patterns trigger git branch completion."""
+    option = Option(["--branch"], auto_complete=True)
+    ctx = click.Context(click.Command("test"))
+
+    # Mock git command
+    with pytest.MonkeyPatch().context() as m:
+
+        class MockResult:
+            def __init__(self, stdout="", returncode=0):
+                self.returncode = returncode
+                self.stdout = stdout
+
+        def mock_run(*args, **kwargs):
+            cmd = args[0] if args else []
+
+            # Handle two-step git approach
+            if "rev-parse" in cmd and "--is-inside-work-tree" in cmd:
+                return MockResult("true\n")
+            elif "for-each-ref" in cmd:
+                return MockResult("main\nfeature/test\ndevelop\n")
+
+            # Handle single-step git approach
+            elif "branch" in cmd and any("--format" in arg for arg in cmd):
+                return MockResult("main\nfeature/test\ndevelop\n")
+
+            # Default fallback
+            return MockResult("")
+
+        m.setattr("subprocess.run", mock_run)
+
+        completions = option._smart_complete(ctx, "main")
+        completion_values = [c.value for c in completions]
+        assert "main" in completion_values
+
+
+def test_auto_complete_fallback_to_parent():
+    """Test that unrecognized patterns fall back to parent completion."""
+    option = Option(["--unknown"], auto_complete=True)
+    ctx = click.Context(click.Command("test"))
+
+    # Should fall back to parent shell_complete method
+    completions = option._smart_complete(ctx, "test")
+    # Parent method should return empty list for basic string type
+    assert completions == []
+
+
+def test_auto_complete_with_custom_shell_complete():
+    """Test that auto_complete is ignored when custom shell_complete is provided."""
+
+    def custom_complete(ctx, param, incomplete):
+        return ["custom1", "custom2"]
+
+    option = Option(["--user"], auto_complete=True, shell_complete=custom_complete)
+    ctx = click.Context(click.Command("test"))
+
+    # Custom completion should take precedence
+    completions = option.shell_complete(ctx, "")
+    completion_values = [c.value for c in completions]
+    assert "custom1" in completion_values
+    assert "custom2" in completion_values
+
+
+def test_auto_complete_edge_cases():
+    """Test edge cases for auto completion."""
+    # Test with None name
+    option = Option(["--test"], auto_complete=True)
+    option.name = None
+    ctx = click.Context(click.Command("test"))
+
+    completions = option._smart_complete(ctx, "test")
+    assert completions == []
+
+    # Test with empty incomplete
+    option = Option(["--user"], auto_complete=True)
+    ctx = click.Context(click.Command("test"))
+
+    with pytest.MonkeyPatch().context() as m:
+
+        class MockUser:
+            def __init__(self, name):
+                self.pw_name = name
+
+        m.setattr("pwd.getpwall", lambda: [MockUser("alice"), MockUser("bob")])
+
+        completions = option._smart_complete(ctx, "")
+        # Should return all users when incomplete is empty
+        assert len(completions) == 2

**Title**: Add Support for Graded Relevance in NDCG Metric

**Pull Request Details**
This PR enhances the `NDCG` retrieval evaluation metric to support graded relevance by allowing users to supply custom float-based relevance scores per `expected_id`.

**Description**
The `NDCG` class now accepts an optional `relevance_scores` dictionary, enabling evaluation scenarios where expected document relevance is non-binary. This allows for more nuanced assessments of ranked retrieval results by reflecting varying degrees of importance among retrieved items. Users can now compute NDCG using float relevance scores rather than being restricted to binary relevance.

**Technical Background**
Previously, the `NDCG` metric treated all relevant documents equally, assigning them a default score of 1.0. This binary treatment limited the expressiveness of the evaluation, especially in tasks where some documents are more relevant than others. Graded relevance is a standard extension in IR systems to address this limitation.

**Solution**
The `compute` method in `NDCG` is extended to accept a new optional parameter: `relevance_scores`, a dictionary mapping `expected_ids` to float relevance values. During computation, these scores are used to calculate both discounted gains and ideal gains. If `relevance_scores` is not provided, the default binary behavior is preserved. 

**Files Modified**
* `llama-index-core/llama_index/core/evaluation/retrieval/metrics.py`

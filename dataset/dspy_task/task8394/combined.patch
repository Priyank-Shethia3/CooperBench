diff --git a/dspy/clients/__init__.py b/dspy/clients/__init__.py
index 52be745a..1ca282c1 100644
--- a/dspy/clients/__init__.py
+++ b/dspy/clients/__init__.py
@@ -1,7 +1,7 @@
 import logging
 import os
 from pathlib import Path
-from typing import Optional
+from typing import Callable, Optional
 
 import litellm
 from litellm.caching.caching import Cache as LitellmCache
@@ -33,6 +33,14 @@ def configure_cache(
     disk_size_limit_bytes: Optional[int] = DISK_CACHE_LIMIT,
     memory_max_entries: Optional[int] = 1000000,
     enable_litellm_cache: bool = False,
+    *,
+    cache: bool = True,
+    cache_factory: Optional[Callable[[], Cache]] = None,
+    namespace: Optional[str] = None,
+    ttl: Optional[int] = None,
+    compress: Optional[str] = None,
+    cull_limit: Optional[int] = None,
+    eviction_policy: Optional[str] = None,
 ):
     """Configure the cache for DSPy.
 
@@ -66,13 +74,36 @@ def configure_cache(
 
     import dspy
 
-    dspy.cache = Cache(
-        enable_disk_cache,
-        enable_memory_cache,
-        disk_cache_dir,
-        disk_size_limit_bytes,
-        memory_max_entries,
-    )
+    if not cache:
+        dspy.cache = None
+        return
+
+    if cache_factory is not None:
+        dspy.cache = cache_factory()
+    else:
+        dspy.cache = Cache(
+            enable_disk_cache,
+            enable_memory_cache,
+            disk_cache_dir,
+            disk_size_limit_bytes,
+            memory_max_entries,
+            ttl=ttl,
+            compress=compress,
+            cull_limit=cull_limit,
+            eviction_policy=eviction_policy,
+        )
+
+    # Initialize optional namespace and TTL
+    if namespace is not None:
+        try:
+            dspy.cache.namespace = namespace
+        except Exception:
+            pass
+    if ttl is not None:
+        try:
+            dspy.cache.set_ttl(ttl, memory_max_entries=memory_max_entries)
+        except Exception:
+            pass
 
 
 litellm.telemetry = False
@@ -112,4 +143,16 @@ __all__ = [
     "enable_litellm_logging",
     "disable_litellm_logging",
     "configure_cache",
+    "DSPY_CACHE",
 ]
+
+# Convenience helpers
+def set_ttl(seconds: Optional[int]) -> None:
+    import dspy
+    if getattr(dspy, "cache", None) is not None:
+        dspy.cache.set_ttl(seconds)
+
+def set_namespace(ns: Optional[str]) -> None:
+    import dspy
+    if getattr(dspy, "cache", None) is not None:
+        dspy.cache.namespace = ns
diff --git a/dspy/clients/cache.py b/dspy/clients/cache.py
index bfa7dbde..e4522dac 100644
--- a/dspy/clients/cache.py
+++ b/dspy/clients/cache.py
@@ -2,6 +2,8 @@ import copy
 import inspect
 import logging
 import threading
+from contextlib import contextmanager
+from contextvars import ContextVar
 from functools import wraps
 from hashlib import sha256
 from typing import Any, Dict, Optional
@@ -10,7 +12,11 @@ import cloudpickle
 import pydantic
 import ujson
 from cachetools import LRUCache
+from cachetools import TTLCache
 from diskcache import FanoutCache
+import hashlib
+import gzip
+import zlib
 
 logger = logging.getLogger(__name__)
 
@@ -30,6 +36,11 @@ class Cache:
         disk_cache_dir: str,
         disk_size_limit_bytes: Optional[int] = 1024 * 1024 * 10,
         memory_max_entries: Optional[int] = 1000000,
+        *,
+        ttl: Optional[int] = None,
+        compress: Optional[str] = None,
+        cull_limit: Optional[int] = None,
+        eviction_policy: Optional[str] = None,
 
     ):
         """
@@ -43,22 +54,90 @@ class Cache:
 
         self.enable_disk_cache = enable_disk_cache
         self.enable_memory_cache = enable_memory_cache
+
+        self.ttl = ttl
+        self.compress = compress  # None | "gzip" | "zlib"
+
+        # Cache statistics (global)
+        self._stats = {"hits": 0, "mem_hits": 0, "disk_hits": 0, "misses": 0}
+
+        # Context-local flags
+        self._bypass_var: ContextVar[bool] = ContextVar("dspy_cache_bypass", default=False)
+        self._namespace_var: ContextVar[Optional[str]] = ContextVar("dspy_cache_namespace", default=None)
+
+        # Initialize memory cache layer
         if self.enable_memory_cache:
-            self.memory_cache = LRUCache(maxsize=memory_max_entries)
+            if self.ttl is not None:
+                self.memory_cache = TTLCache(maxsize=memory_max_entries, ttl=self.ttl)
+            else:
+                self.memory_cache = LRUCache(maxsize=memory_max_entries)
         else:
             self.memory_cache = {}
         if self.enable_disk_cache:
-            self.disk_cache = FanoutCache(
+            fanout_kwargs = dict(
                 shards=16,
                 timeout=10,
                 directory=disk_cache_dir,
                 size_limit=disk_size_limit_bytes,
             )
+            if cull_limit is not None:
+                fanout_kwargs["cull_limit"] = cull_limit
+            if eviction_policy is not None:
+                fanout_kwargs["eviction_policy"] = eviction_policy
+            self.disk_cache = FanoutCache(**fanout_kwargs)
         else:
             self.disk_cache = {}
 
         self._lock = threading.RLock()
 
+        # Helpers for compression encoding/decoding
+        self._magic = b"DSPC"
+
+    def _encode_for_disk(self, value: Any) -> Any:
+        if self.compress is None:
+            return value
+        try:
+            raw = cloudpickle.dumps(value)
+            if self.compress == "gzip":
+                payload = gzip.compress(raw)
+                codec = b"\x01"
+            elif self.compress == "zlib":
+                payload = zlib.compress(raw)
+                codec = b"\x02"
+            else:
+                # Unknown codec, store uncompressed
+                return value
+            return self._magic + codec + payload
+        except Exception:
+            # In case of any serialization error, fallback to original value
+            return value
+
+    def _decode_from_disk(self, obj: Any) -> Any:
+        # Legacy path: value already unpickled to Python object
+        if not isinstance(obj, (bytes, bytearray)):
+            return obj
+        blob = bytes(obj)
+        try:
+            # Magic header path
+            if len(blob) >= 5 and blob[:4] == self._magic:
+                codec_byte = blob[4:5]
+                payload = blob[5:]
+                if codec_byte == b"\x01":
+                    raw = gzip.decompress(payload)
+                elif codec_byte == b"\x02":
+                    raw = zlib.decompress(payload)
+                elif codec_byte == b"\x00":
+                    raw = payload
+                else:
+                    # Unknown codec: treat as legacy pickled payload
+                    raw = payload
+                return cloudpickle.loads(raw)
+            # Legacy bytes: treat as pickled value
+            return cloudpickle.loads(blob)
+        except Exception:
+            # If any error, just return original
+            return obj
+
     def __contains__(self, key: str) -> bool:
         """Check if a key is in the cache."""
         return key in self.memory_cache or key in self.disk_cache
@@ -94,9 +173,21 @@ class Cache:
                 return value
 
         params = {k: transform_value(v) for k, v in request.items() if k not in ignored_args_for_cache_key}
-        return sha256(ujson.dumps(params, sort_keys=True).encode()).hexdigest()
+        base_key = sha256(ujson.dumps(params, sort_keys=True).encode()).hexdigest()
+
+        # Optional namespace prefixing
+        namespace = self._namespace_var.get()
+        if namespace:
+            ns_hash = hashlib.sha256(namespace.encode()).hexdigest()[:8]
+            return f"{ns_hash}:{base_key}"
+        return base_key
 
     def get(self, request: Dict[str, Any], ignored_args_for_cache_key: Optional[list[str]] = None) -> Any:
+        # Bypass reads when requested
+        if self._bypass_var.get():
+            self._increment_stat("misses")
+            self._record_cache_event("miss")
+            return None
         try:
             key = self.cache_key(request, ignored_args_for_cache_key)
         except Exception:
@@ -106,13 +197,27 @@ class Cache:
         if self.enable_memory_cache and key in self.memory_cache:
             with self._lock:
                 response = self.memory_cache[key]
+            self._increment_stat("mem_hits")
+            self._increment_stat("hits")
+            self._record_cache_event("mem_hit")
         elif self.enable_disk_cache and key in self.disk_cache:
             # Found on disk but not in memory cache, add to memory cache
-            response = self.disk_cache[key]
+            response = self._decode_from_disk(self.disk_cache[key])
             if self.enable_memory_cache:
                 with self._lock:
                     self.memory_cache[key] = response
+            self._increment_stat("disk_hits")
+            self._increment_stat("hits")
+            self._record_cache_event("disk_hit")
         else:
+            self._increment_stat("misses")
+            self._record_cache_event("miss")
+            # Lazy purge expired items on miss if TTL is enabled
+            if self.enable_disk_cache and self.ttl is not None:
+                try:
+                    self.disk_cache.expire()
+                except Exception:
+                    pass
             return None
 
         response = copy.deepcopy(response)
@@ -140,7 +245,12 @@ class Cache:
 
         if self.enable_disk_cache:
             try:
-                self.disk_cache[key] = value
+                to_store = self._encode_for_disk(value)
+                if self.ttl is not None:
+                    # Use expire parameter when TTL is set
+                    self.disk_cache.set(key, to_store, expire=self.ttl)
+                else:
+                    self.disk_cache[key] = to_store
             except Exception as e:
                 # Disk cache writing can fail for different reasons, e.g. disk full or the `value` is not picklable.
                 logger.debug(f"Failed to put value in disk cache: {value}, {e}")
@@ -168,8 +278,94 @@ class Cache:
             with open(filepath, "rb") as f:
                 self.memory_cache = cloudpickle.load(f)
 
+    # ------------------- Bypass API -------------------
+    @contextmanager
+    def bypass(self):
+        """Context manager to bypass cache reads while still writing fresh results."""
+        token = self._bypass_var.set(True)
+        try:
+            yield
+        finally:
+            self._bypass_var.reset(token)
+
+    # ------------------- Namespace API -------------------
+    def set_namespace(self, namespace: Optional[str]) -> None:
+        self._namespace_var.set(namespace)
+
+    def _namespace_context(self, namespace: Optional[str]):
+        @contextmanager
+        def _ctx():
+            token = self._namespace_var.set(namespace)
+            try:
+                yield
+            finally:
+                self._namespace_var.reset(token)
+        return _ctx()
+
+    class _NamespaceAccessor(str):
+        def __new__(cls, cache: "Cache", value: Optional[str]):
+            s = str.__new__(cls, value or "")
+            s._cache = cache
+            return s
+
+        def __call__(self, namespace: Optional[str]):
+            return self._cache._namespace_context(namespace)
+
+    @property
+    def namespace(self):
+        return Cache._NamespaceAccessor(self, self._namespace_var.get())
+
+    @namespace.setter
+    def namespace(self, value: Optional[str]):
+        self.set_namespace(value)
+
+    # ------------------- TTL Runtime Update -------------------
+    def set_ttl(self, ttl: Optional[int], memory_max_entries: Optional[int] = None) -> None:
+        """Update TTL and reconfigure memory cache type accordingly."""
+        self.ttl = ttl
+        if self.enable_memory_cache:
+            max_entries = memory_max_entries
+            if max_entries is None:
+                try:
+                    max_entries = self.memory_cache.maxsize  # type: ignore[attr-defined]
+                except Exception:
+                    max_entries = 1000000
+            with self._lock:
+                if self.ttl is not None:
+                    self.memory_cache = TTLCache(maxsize=max_entries, ttl=self.ttl)
+                else:
+                    self.memory_cache = LRUCache(maxsize=max_entries)
+
+    # ------------------- Stats API -------------------
+    def _increment_stat(self, key: str) -> None:
+        with self._lock:
+            if key in self._stats:
+                self._stats[key] += 1
+
+    def stats(self) -> Dict[str, int]:
+        with self._lock:
+            return dict(self._stats)
+
+    def reset_stats(self) -> None:
+        with self._lock:
+            for k in self._stats.keys():
+                self._stats[k] = 0
+
+    # ------------------- Per-call tracking hook -------------------
+    def _record_cache_event(self, event: str) -> None:
+        try:
+            from dspy.dsp.utils.settings import thread_local_overrides
+            usage_tracker = thread_local_overrides.get().get("usage_tracker")
+            if usage_tracker is not None:
+                usage_tracker.add_cache_event(event)
+        except Exception:
+            pass
+
 
 def request_cache(
+    cache: bool = True,
+    ignore: Optional[list[str]] = None,
+    namespace: Optional[str] = None,
     cache_arg_name: Optional[str] = None,
     ignored_args_for_cache_key: Optional[list[str]] = None,
     enable_memory_cache: bool = True,
@@ -186,7 +382,7 @@ def request_cache(
         enable_memory_cache: Whether to enable in-memory cache at call time. If False, the memory cache will not be
             written to on new data.
     """
-    ignored_args_for_cache_key = ignored_args_for_cache_key or ["api_key", "api_base", "base_url"]
+    ignored_args_for_cache_key = (ignore or ignored_args_for_cache_key) or ["api_key", "api_base", "base_url"]
     # Deprecation notice
     if maxsize is not None:
         logger.warning(
@@ -220,39 +416,53 @@ def request_cache(
         def sync_wrapper(*args, **kwargs):
             import dspy
 
-            cache = dspy.cache
+            cache_obj = dspy.cache
             modified_request = process_request(args, kwargs)
 
-            # Retrieve from cache if available
-            cached_result = cache.get(modified_request, ignored_args_for_cache_key)
-
-            if cached_result is not None:
-                return cached_result
+            if not cache:
+                return fn(*args, **kwargs)
 
-            # Otherwise, compute and store the result
-            result = fn(*args, **kwargs)
-            # `enable_memory_cache` can be provided at call time to avoid indefinite growth.
-            cache.put(modified_request, result, ignored_args_for_cache_key, enable_memory_cache)
-
-            return result
+            # Retrieve from cache if available
+            def _run():
+                cached_result = cache_obj.get(modified_request, ignored_args_for_cache_key)
+                if cached_result is not None:
+                    return cached_result
+
+                # Otherwise, compute and store the result
+                result = fn(*args, **kwargs)
+                # `enable_memory_cache` can be provided at call time to avoid indefinite growth.
+                cache_obj.put(modified_request, result, ignored_args_for_cache_key, enable_memory_cache)
+                return result
+
+            if namespace is not None:
+                with cache_obj.namespace(namespace):
+                    return _run()
+            else:
+                return _run()
 
         @wraps(fn)
         async def async_wrapper(*args, **kwargs):
             import dspy
 
-            cache = dspy.cache
+            cache_obj = dspy.cache
             modified_request = process_request(args, kwargs)
 
             # Retrieve from cache if available
-            cached_result = cache.get(modified_request, ignored_args_for_cache_key)
-            if cached_result is not None:
-                return cached_result
-
-            # Otherwise, compute and store the result
-            result = await fn(*args, **kwargs)
-            cache.put(modified_request, result, ignored_args_for_cache_key, enable_memory_cache)
-
-            return result
+            async def _run_async():
+                cached_result = cache_obj.get(modified_request, ignored_args_for_cache_key)
+                if cached_result is not None:
+                    return cached_result
+
+                # Otherwise, compute and store the result
+                result = await fn(*args, **kwargs)
+                cache_obj.put(modified_request, result, ignored_args_for_cache_key, enable_memory_cache)
+                return result
+
+            if namespace is not None:
+                with cache_obj.namespace(namespace):
+                    return await _run_async()
+            else:
+                return await _run_async()
 
         if inspect.iscoroutinefunction(fn):
             return async_wrapper
diff --git a/dspy/primitives/prediction.py b/dspy/primitives/prediction.py
index 670b816b..f9a52e54 100644
--- a/dspy/primitives/prediction.py
+++ b/dspy/primitives/prediction.py
@@ -10,6 +10,7 @@ class Prediction(Example):
 
         self._completions = None
         self._lm_usage = None
+        self._cache_usage = None
 
     def get_lm_usage(self):
         return self._lm_usage
@@ -17,6 +18,12 @@ class Prediction(Example):
     def set_lm_usage(self, value):
         self._lm_usage = value
 
+    def get_cache_usage(self):
+        return self._cache_usage
+
+    def set_cache_usage(self, value):
+        self._cache_usage = value
+
     @classmethod
     def from_completions(cls, list_or_dict, signature=None):
         obj = cls()
diff --git a/dspy/primitives/program.py b/dspy/primitives/program.py
index 39457c36..9bb22025 100644
--- a/dspy/primitives/program.py
+++ b/dspy/primitives/program.py
@@ -59,6 +59,10 @@ class Module(BaseModule, metaclass=ProgramMeta):
                 with track_usage() as usage_tracker:
                     output = self.forward(*args, **kwargs)
                 output.set_lm_usage(usage_tracker.get_total_tokens())
+                try:
+                    output.set_cache_usage(usage_tracker.get_cache_usage())
+                except Exception:
+                    pass
                 return output
 
             return self.forward(*args, **kwargs)
@@ -74,6 +78,10 @@ class Module(BaseModule, metaclass=ProgramMeta):
                 with track_usage() as usage_tracker:
                     output = await self.aforward(*args, **kwargs)
                     output.set_lm_usage(usage_tracker.get_total_tokens())
+                    try:
+                        output.set_cache_usage(usage_tracker.get_cache_usage())
+                    except Exception:
+                        pass
                     return output
 
             return await self.aforward(*args, **kwargs)
diff --git a/dspy/utils/usage_tracker.py b/dspy/utils/usage_tracker.py
index 5199f3ad..480151b8 100644
--- a/dspy/utils/usage_tracker.py
+++ b/dspy/utils/usage_tracker.py
@@ -2,13 +2,13 @@
 
 from collections import defaultdict
 from contextlib import contextmanager
-from typing import Any
+from typing import Any, Dict, Optional
 
 from dspy.dsp.utils.settings import settings
 
 
 class UsageTracker:
-    """Tracks LM usage data within a context."""
+    """Tracks LM usage data and per-call cache statistics within a context."""
 
     def __init__(self):
         # Map of LM name to list of usage entries. For example:
@@ -19,6 +19,8 @@ class UsageTracker:
         #     ],
         # }
         self.usage_data = defaultdict(list)
+        # Per-call cache stats
+        self.cache_stats = {"hits": 0, "mem_hits": 0, "disk_hits": 0, "misses": 0}
 
     def _flatten_usage_entry(self, usage_entry) -> dict[str, dict[str, Any]]:
         result = dict(usage_entry)
@@ -49,6 +51,20 @@ class UsageTracker:
         if len(usage_entry) > 0:
             self.usage_data[lm].append(self._flatten_usage_entry(usage_entry))
 
+    # -------- Cache usage tracking --------
+    def add_cache_event(self, event: str) -> None:
+        if event == "mem_hit":
+            self.cache_stats["mem_hits"] += 1
+            self.cache_stats["hits"] += 1
+        elif event == "disk_hit":
+            self.cache_stats["disk_hits"] += 1
+            self.cache_stats["hits"] += 1
+        elif event == "miss":
+            self.cache_stats["misses"] += 1
+
+    def get_cache_usage(self) -> Optional[Dict[str, int]]:
+        return dict(self.cache_stats)
+
     def get_total_tokens(self) -> dict[str, dict[str, Any]]:
         """Calculate total tokens from all tracked usage."""
         total_usage_by_lm = {}
@@ -62,7 +78,7 @@ class UsageTracker:
 
 @contextmanager
 def track_usage():
-    """Context manager for tracking LM usage."""
+    """Context manager for tracking LM and cache usage."""
     tracker = UsageTracker()
 
     with settings.context(usage_tracker=tracker):

diff --git a/dspy/clients/__init__.py b/dspy/clients/__init__.py
index 52be745a..6daac185 100644
--- a/dspy/clients/__init__.py
+++ b/dspy/clients/__init__.py
@@ -1,7 +1,7 @@
 import logging
 import os
 from pathlib import Path
-from typing import Optional
+from typing import Optional, Literal
 
 import litellm
 from litellm.caching.caching import Cache as LitellmCache
@@ -33,6 +33,9 @@ def configure_cache(
     disk_size_limit_bytes: Optional[int] = DISK_CACHE_LIMIT,
     memory_max_entries: Optional[int] = 1000000,
     enable_litellm_cache: bool = False,
+    compress: Optional[Literal["gzip", "zlib", None]] = None,
+    cull_limit: Optional[int] = None,
+    eviction_policy: Optional[str] = None,
 ):
     """Configure the cache for DSPy.
 
@@ -43,6 +46,9 @@ def configure_cache(
         disk_size_limit_bytes: The size limit of the on-disk cache.
         memory_max_entries: The maximum number of entries in the in-memory cache.
         enable_litellm_cache: Whether to enable LiteLLM cache.
+        compress: Compression codec to use for disk cache ("gzip", "zlib", or None).
+        cull_limit: Number of items to remove from disk cache when it exceeds size limit.
+        eviction_policy: Eviction policy for disk cache ("least-recently-stored", "least-recently-used", "least-frequently-used", "none").
     """
     if enable_disk_cache and enable_litellm_cache:
         raise ValueError(
@@ -72,6 +78,9 @@ def configure_cache(
         disk_cache_dir,
         disk_size_limit_bytes,
         memory_max_entries,
+        compress,
+        cull_limit,
+        eviction_policy,
     )
 
 
@@ -84,6 +93,9 @@ DSPY_CACHE = Cache(
     disk_cache_dir=DISK_CACHE_DIR,
     disk_size_limit_bytes=DISK_CACHE_LIMIT,
     memory_max_entries=1000000,
+    compress=None,
+    cull_limit=None,
+    eviction_policy=None,
 )
 
 # Turn off by default to avoid LiteLLM logging during every LM call.
diff --git a/dspy/clients/cache.py b/dspy/clients/cache.py
index bfa7dbde..9fff8e06 100644
--- a/dspy/clients/cache.py
+++ b/dspy/clients/cache.py
@@ -2,9 +2,12 @@ import copy
 import inspect
 import logging
 import threading
+import gzip
+import zlib
+import pickle
 from functools import wraps
 from hashlib import sha256
-from typing import Any, Dict, Optional
+from typing import Any, Dict, Optional, Literal
 
 import cloudpickle
 import pydantic
@@ -14,6 +17,12 @@ from diskcache import FanoutCache
 
 logger = logging.getLogger(__name__)
 
+# Magic header constants for compression
+MAGIC_HEADER = b"DSPC"
+CODEC_NONE = 0x00
+CODEC_GZIP = 0x01
+CODEC_ZLIB = 0x02
+
 
 class Cache:
     """DSPy Cache
@@ -30,7 +39,9 @@ class Cache:
         disk_cache_dir: str,
         disk_size_limit_bytes: Optional[int] = 1024 * 1024 * 10,
         memory_max_entries: Optional[int] = 1000000,
-
+        compress: Optional[Literal["gzip", "zlib", None]] = None,
+        cull_limit: Optional[int] = None,
+        eviction_policy: Optional[str] = None,
     ):
         """
         Args:
@@ -39,26 +50,89 @@ class Cache:
             disk_cache_dir: The directory where the disk cache is stored.
             disk_size_limit_bytes: The maximum size of the disk cache (in bytes).
             memory_max_entries: The maximum size of the in-memory cache (in number of items).
+            compress: Compression codec to use for disk cache ("gzip", "zlib", or None).
+            cull_limit: Number of items to remove from disk cache when it exceeds size limit.
+            eviction_policy: Eviction policy for disk cache ("least-recently-stored", "least-recently-used", "least-frequently-used", "none").
         """
 
         self.enable_disk_cache = enable_disk_cache
         self.enable_memory_cache = enable_memory_cache
+        self.compress = compress
+ 
         if self.enable_memory_cache:
             self.memory_cache = LRUCache(maxsize=memory_max_entries)
         else:
             self.memory_cache = {}
+ 
         if self.enable_disk_cache:
-            self.disk_cache = FanoutCache(
-                shards=16,
-                timeout=10,
-                directory=disk_cache_dir,
-                size_limit=disk_size_limit_bytes,
-            )
+            # Set up disk cache options
+            disk_cache_options = {
+                "shards": 16,
+                "timeout": 10,
+                "directory": disk_cache_dir,
+                "size_limit": disk_size_limit_bytes,
+            }
+ 
+            # Add eviction options if specified
+            if cull_limit is not None:
+                disk_cache_options["cull_limit"] = cull_limit
+            if eviction_policy is not None:
+                disk_cache_options["eviction_policy"] = eviction_policy
+ 
+            self.disk_cache = FanoutCache(**disk_cache_options)
         else:
             self.disk_cache = {}
 
         self._lock = threading.RLock()
 
+    def _compress_value(self, value: Any) -> bytes:
+        """Compress value with magic header for disk storage."""
+        # First pickle the value
+        pickled_data = pickle.dumps(value)
+ 
+        # Apply compression based on codec (only called when compress is not None)
+        if self.compress == "gzip":
+            compressed_data = gzip.compress(pickled_data)
+            codec_byte = CODEC_GZIP
+        elif self.compress == "zlib":
+            compressed_data = zlib.compress(pickled_data)
+            codec_byte = CODEC_ZLIB
+        else:
+            # This shouldn't happen since we only call this when compress is not None
+            compressed_data = pickled_data
+            codec_byte = CODEC_NONE
+ 
+        # Prepend magic header
+        return MAGIC_HEADER + bytes([codec_byte]) + compressed_data
+
+    def _decompress_value(self, data: bytes) -> Any:
+        """Decompress value, handling both new format with header and legacy format."""
+        # Check if data has magic header
+        if len(data) >= 5 and data[:4] == MAGIC_HEADER:
+            # New format with magic header
+            codec_byte = data[4]
+            payload = data[5:]
+ 
+            # Decompress based on codec
+            if codec_byte == CODEC_GZIP:
+                pickled_data = gzip.decompress(payload)
+            elif codec_byte == CODEC_ZLIB:
+                pickled_data = zlib.decompress(payload)
+            elif codec_byte == CODEC_NONE:
+                pickled_data = payload
+            else:
+                raise ValueError(f"Unknown compression codec: {codec_byte}")
+ 
+            return pickle.loads(pickled_data)
+        else:
+            # Legacy format without magic header - assume it's a plain Python object
+            # that diskcache has already deserialized
+            return data
+
+    def set_compression(self, codec: Optional[Literal["gzip", "zlib", None]]) -> None:
+        """Set compression codec for disk cache."""
+        self.compress = codec
+
     def __contains__(self, key: str) -> bool:
         """Check if a key is in the cache."""
         return key in self.memory_cache or key in self.disk_cache
@@ -107,8 +181,18 @@ class Cache:
             with self._lock:
                 response = self.memory_cache[key]
         elif self.enable_disk_cache and key in self.disk_cache:
-            # Found on disk but not in memory cache, add to memory cache
-            response = self.disk_cache[key]
+            # Found on disk but not in memory cache
+            disk_data = self.disk_cache[key]
+ 
+            # Handle compression if enabled
+            if self.compress is not None and isinstance(disk_data, bytes):
+                # Only decompress if we have bytes data (indicating compressed storage)
+                response = self._decompress_value(disk_data)
+            else:
+                # Use data directly (normal diskcache serialization)
+                response = disk_data
+ 
+            # Add to memory cache if enabled
             if self.enable_memory_cache:
                 with self._lock:
                     self.memory_cache[key] = response
@@ -140,7 +224,15 @@ class Cache:
 
         if self.enable_disk_cache:
             try:
-                self.disk_cache[key] = value
+                # Handle compression if enabled
+                if self.compress is not None:
+                    # Only compress and add magic header when compression is actually enabled
+                    disk_data = self._compress_value(value)
+                    # Store compressed data as bytes
+                    self.disk_cache[key] = disk_data
+                else:
+                    # Store value directly - let diskcache handle serialization efficiently
+                    self.disk_cache[key] = value
             except Exception as e:
                 # Disk cache writing can fail for different reasons, e.g. disk full or the `value` is not picklable.
                 logger.debug(f"Failed to put value in disk cache: {value}, {e}")


diff --git a/tests/clients/test_cache.py b/tests/clients/test_cache.py
index edff1485..d1e3b9d6 100644
--- a/tests/clients/test_cache.py
+++ b/tests/clients/test_cache.py
@@ -198,6 +198,254 @@ def test_save_and_load_memory_cache(cache, tmp_path):
         assert result == f"Response {requests.index(req)}"
 
 
+# =============================================================================
+# COMPRESSION FEATURE TESTS
+# =============================================================================
+
+
+def test_compression_api_configuration(tmp_path):
+    """Test that configure_cache accepts the new compress parameter."""
+    import dspy.clients
+ 
+    # Test configure_cache accepts compress parameter with valid values
+    # This should work once the feature is implemented
+    dspy.clients.configure_cache(
+        enable_disk_cache=True,
+        disk_cache_dir=str(tmp_path),
+        compress="gzip"
+    )
+ 
+    dspy.clients.configure_cache(
+        enable_disk_cache=True,
+        disk_cache_dir=str(tmp_path),
+        compress="zlib"
+    )
+ 
+    dspy.clients.configure_cache(
+        enable_disk_cache=True,
+        disk_cache_dir=str(tmp_path),
+        compress=None
+    )
+
+
+def test_compression_round_trip_integrity(tmp_path):
+    """Test round-trip integrity for gzip and zlib compression."""
+    from dspy.clients.cache import Cache
+ 
+    # Create test data - simple data structure to focus on compression
+    test_request = {"prompt": "test", "model": "gpt-4"}
+    test_response = DummyResponse(
+        message="This is a test response that should be compressed" * 100,  # Make it large enough
+        usage={}  # Empty usage to avoid testing unrelated cache behavior (usage is cleared on cache hits)
+    )
+ 
+    # Test gzip compression round-trip
+    gzip_cache = Cache(
+        enable_disk_cache=True,
+        enable_memory_cache=False,  # Focus on disk cache
+        disk_cache_dir=str(tmp_path / "gzip"),
+        compress="gzip"
+    )
+ 
+    # Store and retrieve data
+    gzip_cache.put(test_request, test_response)
+    retrieved_gzip = gzip_cache.get(test_request)
+ 
+    # Verify round-trip integrity - focus on compression/decompression
+    assert retrieved_gzip is not None
+    assert retrieved_gzip.message == test_response.message
+    assert retrieved_gzip.usage == {}
+ 
+    # Test zlib compression round-trip 
+    zlib_cache = Cache(
+        enable_disk_cache=True,
+        enable_memory_cache=False,  # Focus on disk cache
+        disk_cache_dir=str(tmp_path / "zlib"),
+        compress="zlib"
+    )
+ 
+    # Store and retrieve data
+    zlib_cache.put(test_request, test_response)
+    retrieved_zlib = zlib_cache.get(test_request)
+ 
+    # Verify round-trip integrity - focus on compression/decompression
+    assert retrieved_zlib is not None
+    assert retrieved_zlib.message == test_response.message
+    assert retrieved_zlib.usage == {}
+
+
+def test_magic_header_format_and_detection(tmp_path):
+    """Test magic header format (b'DSPC' + codec byte) and codec detection."""
+    import pickle
+    import gzip
+    import zlib
+ 
+    # Test data
+    test_data = {"message": "test response", "tokens": 50}
+    pickled_data = pickle.dumps(test_data)
+ 
+    # Test magic header format for gzip (b"DSPC" + 0x01)
+    gzip_compressed = gzip.compress(pickled_data)
+    expected_gzip_header = b"DSPC\x01" + gzip_compressed
+ 
+    # Test magic header format for zlib (b"DSPC" + 0x02) 
+    zlib_compressed = zlib.compress(pickled_data)
+    expected_zlib_header = b"DSPC\x02" + zlib_compressed
+ 
+    # Test uncompressed format (b"DSPC" + 0x00)
+    expected_none_header = b"DSPC\x00" + pickled_data
+ 
+    # Verify header format structure
+    assert expected_gzip_header[:4] == b"DSPC"
+    assert expected_gzip_header[4:5] == b"\x01"
+    assert expected_zlib_header[:4] == b"DSPC"
+    assert expected_zlib_header[4:5] == b"\x02"
+    assert expected_none_header[:4] == b"DSPC"
+    assert expected_none_header[4:5] == b"\x00"
+ 
+    # Test actual compression with magic header
+    from dspy.clients.cache import Cache
+    cache = Cache(
+        enable_disk_cache=True,
+        enable_memory_cache=False,
+        disk_cache_dir=str(tmp_path),
+        compress="gzip"
+    )
+ 
+    # Store data and verify it can be retrieved
+    test_request = {"test": "request"}
+    cache.put(test_request, test_data)
+    retrieved_data = cache.get(test_request)
+ 
+    # Verify data integrity
+    assert retrieved_data is not None
+    assert retrieved_data == test_data
+
+
+def test_backward_compatibility_with_legacy_entries(tmp_path):
+    """Test that legacy (headerless) cached entries can still be loaded."""
+    import pickle
+    from diskcache import FanoutCache
+ 
+    # Create a legacy cache entry (without magic header)
+    legacy_data = DummyResponse(message="legacy response", usage={"tokens": 25})
+ 
+    # Manually create a legacy disk cache entry
+    disk_cache_dir = str(tmp_path / "legacy_cache")
+    legacy_disk_cache = FanoutCache(directory=disk_cache_dir, shards=16)
+ 
+    # Create a test request and compute its cache key the same way the Cache class would
+    test_request = {"prompt": "legacy test", "model": "gpt-4"}
+    from dspy.clients.cache import Cache
+    temp_cache = Cache(
+        enable_disk_cache=False,
+        enable_memory_cache=False,
+        disk_cache_dir="",
+    )
+    cache_key = temp_cache.cache_key(test_request)
+ 
+    # Store legacy entry using the computed cache key (simulating old format without header)
+    legacy_disk_cache[cache_key] = legacy_data
+    legacy_disk_cache.close()
+ 
+    # Now try to read it with new cache that supports compression
+    new_cache = Cache(
+        enable_disk_cache=True,
+        enable_memory_cache=False,
+        disk_cache_dir=disk_cache_dir,
+        compress=None  # Start with no compression
+    )
+ 
+    # Should be able to read legacy entry
+    result = new_cache.get(test_request)
+    assert result is not None
+    assert result.message == "legacy response"
+    # Usage will be cleared due to cache hit behavior, which is expected
+    assert result.usage == {}
+
+
+def test_compression_reduces_disk_size(tmp_path):
+    """Test that compression mechanism is working correctly and provides size benefits."""
+    import os
+    from dspy.clients.cache import Cache
+ 
+    # Create test response with highly compressible data pattern
+    # This pattern shows significant compression benefit (~27% reduction)
+    repeated_text = "ABCDEFGH" * 50000  # Pattern that compresses very well
+    large_response = DummyResponse(
+        message=repeated_text,
+        usage={}
+    )
+ 
+    test_request = {"prompt": "large test", "model": "gpt-4"}
+ 
+    # Test uncompressed cache
+    uncompressed_cache = Cache(
+        enable_disk_cache=True,
+        enable_memory_cache=False,
+        disk_cache_dir=str(tmp_path / "uncompressed"),
+        compress=None
+    )
+ 
+    # Test gzip compressed cache  
+    gzip_cache = Cache(
+        enable_disk_cache=True,
+        enable_memory_cache=False,
+        disk_cache_dir=str(tmp_path / "gzip"),
+        compress="gzip"
+    )
+ 
+    # Store same data in both caches
+    uncompressed_cache.put(test_request, large_response)
+    gzip_cache.put(test_request, large_response)
+ 
+    # Check actual disk usage
+    uncompressed_size = sum(
+        os.path.getsize(os.path.join(dirpath, filename))
+        for dirpath, dirnames, filenames in os.walk(str(tmp_path / "uncompressed"))
+        for filename in filenames
+    )
+ 
+    gzip_size = sum(
+        os.path.getsize(os.path.join(dirpath, filename))
+        for dirpath, dirnames, filenames in os.walk(str(tmp_path / "gzip"))
+        for filename in filenames
+    )
+ 
+    # Verify compression provides real benefit
+    print(f"Uncompressed size: {uncompressed_size}, Compressed size: {gzip_size}")
+    savings = uncompressed_size - gzip_size
+    print(f"Compression savings: {savings} bytes")
+ 
+    assert gzip_size < uncompressed_size, f"Compression didn't reduce size: {gzip_size} vs {uncompressed_size}"
+    assert savings > 50000, f"Compression savings too small: {savings} bytes"  # Expect significant savings
+ 
+    # Verify both can retrieve the data correctly
+    retrieved_uncompressed = uncompressed_cache.get(test_request)
+    retrieved_compressed = gzip_cache.get(test_request)
+ 
+    assert retrieved_uncompressed is not None
+    assert retrieved_compressed is not None
+    assert retrieved_uncompressed.message == retrieved_compressed.message
+    assert retrieved_uncompressed.message == repeated_text
+ 
+    # Verify the compression mechanism is being used by checking storage type
+    cache_key = uncompressed_cache.cache_key(test_request)
+ 
+    # Uncompressed should store the object directly
+    uncompressed_stored = uncompressed_cache.disk_cache[cache_key]
+    assert hasattr(uncompressed_stored, 'message'), "Uncompressed should store object directly"
+ 
+    # Compressed should store as bytes (our compressed format)
+    compressed_stored = gzip_cache.disk_cache[cache_key]
+    assert isinstance(compressed_stored, bytes), "Compressed should store as bytes"
+ 
+    # Verify the magic header is present in compressed data
+    assert compressed_stored.startswith(b"DSPC"), "Compressed data should have magic header"
+    assert compressed_stored[4:5] == b"\x01", "Should use gzip codec byte"
+
+
+
 def test_request_cache_decorator(cache):
     """Test the lm_cache decorator."""
     from dspy.clients.cache import request_cache

diff --git a/tests/clients/test_cache_ttl.py b/tests/clients/test_cache_ttl.py
new file mode 100644
index 00000000..2a30d5a7
--- /dev/null
+++ b/tests/clients/test_cache_ttl.py
@@ -0,0 +1,146 @@
+import time
+from dataclasses import dataclass
+
+import pytest
+from cachetools import LRUCache, TTLCache
+
+from dspy.clients.cache import Cache
+
+
+@dataclass
+class DummyResponse:
+    message: str
+    usage: dict
+
+
+# TTL Tests
+
+
+def test_ttl_memory_cache_expiration(tmp_path):
+    """Test that items expire from memory cache after TTL period."""
+    ttl_seconds = 1
+
+    cache = Cache(
+        enable_disk_cache=False,
+        enable_memory_cache=True,
+        disk_cache_dir=str(tmp_path),
+        disk_size_limit_bytes=1024 * 1024,
+        memory_max_entries=100,
+        ttl=ttl_seconds,
+    )
+
+    assert isinstance(cache.memory_cache, TTLCache)
+    assert cache.memory_cache.ttl == ttl_seconds
+
+    request = {"prompt": "Hello", "model": "openai/gpt-4o-mini"}
+    value = DummyResponse(message="Test response", usage={"prompt_tokens": 10})
+
+    cache.put(request, value)
+
+    result = cache.get(request)
+    assert result is not None
+    assert result.message == "Test response"
+
+    time.sleep(ttl_seconds + 0.1)
+
+    result = cache.get(request)
+    assert result is None
+
+
+def test_ttl_disk_cache_expiration(tmp_path):
+    """Test that items expire from disk cache after TTL period."""
+    ttl_seconds = 1
+
+    cache = Cache(
+        enable_disk_cache=True,
+        enable_memory_cache=False,
+        disk_cache_dir=str(tmp_path),
+        disk_size_limit_bytes=1024 * 1024,
+        memory_max_entries=100,
+        ttl=ttl_seconds,
+    )
+
+    request = {"prompt": "Hello", "model": "openai/gpt-4o-mini"}
+    value = DummyResponse(message="Test response", usage={"prompt_tokens": 10})
+
+    cache.put(request, value)
+
+    result = cache.get(request)
+    assert result is not None
+    assert result.message == "Test response"
+
+    time.sleep(ttl_seconds + 0.1)
+
+    result = cache.get(request)
+    assert result is None
+
+
+def test_ttl_is_opt_in(tmp_path):
+    """Test that TTL is opt-in and cache behaves normally when TTL is None."""
+    cache = Cache(
+        enable_disk_cache=True,
+        enable_memory_cache=True,
+        disk_cache_dir=str(tmp_path),
+        disk_size_limit_bytes=1024 * 1024,
+        memory_max_entries=100,
+        ttl=None,
+    )
+
+    assert isinstance(cache.memory_cache, LRUCache)
+    assert not isinstance(cache.memory_cache, TTLCache)
+
+    request = {"prompt": "Hello", "model": "openai/gpt-4o-mini"}
+    value = DummyResponse(message="Test response", usage={"prompt_tokens": 10})
+
+    cache.put(request, value)
+
+    result = cache.get(request)
+    assert result is not None
+    assert result.message == "Test response"
+
+    time.sleep(1.1)
+
+    result = cache.get(request)
+    assert result is not None
+    assert result.message == "Test response"
+
+
+def test_ttl_both_memory_and_disk_cache(tmp_path):
+    """Test that TTL works consistently across both memory and disk cache layers."""
+    ttl_seconds = 1
+
+    cache = Cache(
+        enable_disk_cache=True,
+        enable_memory_cache=True,
+        disk_cache_dir=str(tmp_path),
+        disk_size_limit_bytes=1024 * 1024,
+        memory_max_entries=100,
+        ttl=ttl_seconds,
+    )
+
+    assert isinstance(cache.memory_cache, TTLCache)
+    assert cache.memory_cache.ttl == ttl_seconds
+
+    request = {"prompt": "Hello", "model": "openai/gpt-4o-mini"}
+    value = DummyResponse(message="Test response", usage={"prompt_tokens": 10})
+
+    cache.put(request, value)
+
+    key = cache.cache_key(request)
+    assert key in cache.memory_cache
+    assert key in cache.disk_cache
+
+    cache.reset_memory_cache()
+
+    result = cache.get(request)
+    assert result is not None
+    assert result.message == "Test response"
+
+    time.sleep(ttl_seconds + 0.1)
+
+    result = cache.get(request)
+    assert result is None
+
+    assert len(cache.memory_cache) == 0
+
+

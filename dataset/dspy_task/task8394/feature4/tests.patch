diff --git a/tests/clients/test_cache.py b/tests/clients/test_cache.py
index edff1485..cd618d8a 100644
--- a/tests/clients/test_cache.py
+++ b/tests/clients/test_cache.py
@@ -254,6 +254,164 @@ def test_request_cache_decorator_with_ignored_args_for_cache_key(cache):
         assert result3 != result4
 
 
+def test_cache_statistics_basic_tracking(cache):
+    """Test basic cache statistics tracking for hits, misses, mem_hits, and disk_hits."""
+    # Test that cache stats methods exist and work
+    assert hasattr(cache, 'stats'), "Cache should have stats() method"
+    assert hasattr(cache, 'reset_stats'), "Cache should have reset_stats() method"
+ 
+    # Test initial stats are all zeros
+    stats = cache.stats()
+    expected_keys = ['hits', 'mem_hits', 'disk_hits', 'misses']
+    for key in expected_keys:
+        assert key in stats, f"Stats should contain {key}"
+        assert stats[key] == 0, f"Initial {key} should be 0"
+ 
+    # Test cache miss
+    request1 = {"prompt": "Hello miss", "model": "openai/gpt-4o-mini"}
+    result = cache.get(request1)
+    assert result is None, "Should be cache miss"
+ 
+    stats = cache.stats()
+    assert stats['misses'] == 1, "Should have 1 miss"
+    assert stats['hits'] == 0, "Should have 0 hits"
+ 
+    # Test cache put and memory hit
+    value = DummyResponse(message="Hello response", usage={"total_tokens": 100})
+    cache.put(request1, value)
+ 
+    result = cache.get(request1)
+    assert result is not None, "Should be cache hit"
+    assert result.message == "Hello response"
+ 
+    stats = cache.stats()
+    assert stats['hits'] == 1, "Should have 1 hit"
+    assert stats['mem_hits'] == 1, "Should have 1 memory hit"
+    assert stats['disk_hits'] == 0, "Should have 0 disk hits"
+ 
+    # Test reset_stats
+    cache.reset_stats()
+    stats = cache.stats()
+    for key in expected_keys:
+        assert stats[key] == 0, f"After reset, {key} should be 0"
+
+
+def test_cache_statistics_no_token_tracking(cache):
+    """Sanity check that token savings are not tracked."""
+    import dspy
+ 
+    with dspy.settings.context(track_usage=True):
+        request = {"prompt": "Hello tokens", "model": "openai/gpt-4o-mini"}
+        result = cache.get(request)
+        assert result is None
+ 
+        # Put a response with usage info
+        value = DummyResponse(message="Response with usage", usage={"total_tokens": 250, "prompt_tokens": 150, "completion_tokens": 100})
+        cache.put(request, value)
+ 
+        # Cache hit should not affect any token counters
+        result = cache.get(request)
+        assert result is not None
+        assert result.message == "Response with usage"
+        assert result.usage == {}, "Usage should be cleared on cache hit"
+ 
+        stats = cache.stats()
+        assert set(stats.keys()) == {"hits", "mem_hits", "disk_hits", "misses"}
+
+
+def test_cache_statistics_prediction_integration(cache):
+    """Test that Prediction.get_cache_usage() returns per-call cache stats."""
+    import dspy
+    from unittest.mock import patch
+ 
+    # Create a simple test program
+    class TestProgram(dspy.Module):
+        def __init__(self):
+            self.predict = dspy.Predict("question -> answer")
+ 
+        def forward(self, question):
+            # First call will be cache miss
+            cache_result = cache.get({"prompt": question, "model": "test"})
+            if cache_result is None:
+                # Simulate LM call and cache put
+                response = DummyResponse(message="Test response", usage={"total_tokens": 100})
+                cache.put({"prompt": question, "model": "test"}, response)
+                return dspy.Prediction(answer="Test response")
+            else:
+                return dspy.Prediction(answer=cache_result.message)
+ 
+    # Enable tracking
+    with dspy.settings.context(track_usage=True):
+        program = TestProgram()
+ 
+        # First call - should have cache miss
+        result1 = program(question="What is AI?")
+        cache_usage1 = result1.get_cache_usage()
+ 
+        assert cache_usage1 is not None, "Should have cache usage data"
+        assert cache_usage1['misses'] == 1, "Should have 1 miss for this call"
+        assert cache_usage1['hits'] == 0, "Should have 0 hits for this call"
+ 
+        # Second call - should have cache hit
+        result2 = program(question="What is AI?")
+        cache_usage2 = result2.get_cache_usage()
+ 
+        assert cache_usage2 is not None, "Should have cache usage data"
+        assert cache_usage2['misses'] == 0, "Should have 0 misses for this call"
+        assert cache_usage2['hits'] == 1, "Should have 1 hit for this call"
+        assert set(cache_usage2.keys()) == {"hits", "mem_hits", "disk_hits", "misses"}
+ 
+        # First result should still have its original cache usage
+        assert cache_usage1['misses'] == 1, "Original prediction should retain its cache usage"
+        assert cache_usage1['hits'] == 0, "Original prediction should retain its cache usage"
+
+        # Global stats should also be updated
+        stats = cache.stats()
+        assert stats['misses'] == 1, "Should have 1 miss"
+        assert stats['hits'] == 1, "Should have 1 hit"
+        assert set(stats.keys()) == {"hits", "mem_hits", "disk_hits", "misses"}
+
+
+def test_cache_statistics_layer_distinction(cache):
+    """Test that memory vs disk cache hits are distinguished correctly."""
+    request = {"prompt": "Layer test", "model": "openai/gpt-4o-mini"}
+    value = DummyResponse(message="Layer response", usage={"total_tokens": 75})
+ 
+    # Put in cache (goes to memory)
+    cache.put(request, value)
+ 
+    # Get from memory
+    result = cache.get(request)
+    assert result is not None
+ 
+    stats = cache.stats()
+    assert stats['hits'] == 1
+    assert stats['mem_hits'] == 1
+    assert stats['disk_hits'] == 0
+ 
+    # Clear memory cache to simulate disk-only hit
+    if hasattr(cache, 'memory_cache'):
+        cache.memory_cache.clear()
+ 
+    # Get from disk (should still be there)
+    result = cache.get(request)
+    assert result is not None
+ 
+    stats = cache.stats()
+    assert stats['hits'] == 2
+    assert stats['mem_hits'] == 1, "Should still have 1 memory hit"
+    assert stats['disk_hits'] == 1, "Should now have 1 disk hit"
+ 
+    # After disk hit, item should be back in memory
+    result = cache.get(request)
+    assert result is not None
+ 
+    stats = cache.stats()
+    assert stats['hits'] == 3
+    assert stats['mem_hits'] == 2, "Should now have 2 memory hits"
+    assert stats['disk_hits'] == 1, "Should still have 1 disk hit"
+
+
 @pytest.mark.asyncio
 async def test_request_cache_decorator_async(cache):
     """Test the request_cache decorator with async functions."""

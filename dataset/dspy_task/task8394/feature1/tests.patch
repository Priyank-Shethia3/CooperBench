diff --git a/tests/clients/test_cache.py b/tests/clients/test_cache.py
index edff1485..171cdab9 100644
--- a/tests/clients/test_cache.py
+++ b/tests/clients/test_cache.py
@@ -280,3 +280,170 @@ async def test_request_cache_decorator_async(cache):
         # Call with different arguments should compute again
         result3 = await test_function(prompt="Different", model="openai/gpt-4o-mini")
         assert result3 == "Response for Different with openai/gpt-4o-mini"
+
+def test_cache_consistency_with_lm_call_modifies_the_request(cache):
+    """Test that the cache is consistent with the LM call that modifies the request."""
+    from dspy.clients.cache import request_cache
+
+    # Mock the dspy.cache attribute
+    with patch("dspy.cache", cache):
+        # Define a test function
+        @request_cache()
+        def test_function(**kwargs):
+            del kwargs["field_to_delete"]
+            return kwargs
+
+        # First call should compute the result
+        test_function(field_to_delete="delete", field_to_keep="keep")
+
+        # The cache key should use the original request, not the modified one
+        assert (
+            cache.get(
+                {
+                    "field_to_keep": "keep",
+                    "_fn_identifier": f"{test_function.__module__}.{test_function.__qualname__}",
+                }
+            )
+            is None
+        )
+        assert (
+            cache.get(
+                {
+                    "field_to_keep": "keep",
+                    "field_to_delete": "delete",
+                    "_fn_identifier": f"{test_function.__module__}.{test_function.__qualname__}",
+                }
+            )
+            is not None
+        )
+
+
+def test_cache_bypass_context_manager(cache):
+    """Test the cache bypass context manager that skips reads but still writes."""
+    # Create a test request
+    request = {"prompt": "Test prompt", "model": "openai/gpt-4o-mini", "temperature": 0.7}
+ 
+    # First, put a value in cache
+    original_response = DummyResponse(message="Original cached response", usage={"prompt_tokens": 5, "completion_tokens": 10})
+    cache.put(request, original_response)
+ 
+    # Verify it's cached
+    cached_result = cache.get(request)
+    assert cached_result is not None
+    assert cached_result.message == "Original cached response"
+ 
+    # Now test bypass functionality
+    with cache.bypass():
+        # Inside bypass context, get() should return None even though value is cached
+        bypass_result = cache.get(request)
+        assert bypass_result is None
+ 
+        # put() should still work and store the new value
+        new_response = DummyResponse(message="New response during bypass", usage={"prompt_tokens": 8, "completion_tokens": 15})
+        cache.put(request, new_response)
+ 
+    # After exiting bypass context, should get the newly stored value
+    final_result = cache.get(request)
+    assert final_result is not None
+    assert final_result.message == "New response during bypass"
+
+
+def test_cache_bypass_nested_context_manager(cache):
+    """Test nested bypass context managers work correctly."""
+    request = {"prompt": "Nested test", "model": "openai/gpt-4o-mini"}
+ 
+    # Put initial value
+    cache.put(request, DummyResponse(message="Initial value", usage={}))
+ 
+    # Test normal access
+    assert cache.get(request).message == "Initial value"
+ 
+    # Test nested bypass
+    with cache.bypass():
+        assert cache.get(request) is None
+ 
+        # Nested bypass should still work
+        with cache.bypass():
+            assert cache.get(request) is None
+            cache.put(request, DummyResponse(message="Nested bypass value", usage={}))
+ 
+        # Still in outer bypass, should still return None
+        assert cache.get(request) is None
+ 
+    # Outside bypass, should get the value stored during nested bypass
+    assert cache.get(request).message == "Nested bypass value"
+
+
+def test_cache_bypass_with_request_cache_decorator(cache):
+    """Test that bypass works with the request_cache decorator."""
+    from dspy.clients.cache import request_cache
+ 
+    call_count = 0
+ 
+    @request_cache()
+    def test_function(prompt, model):
+        nonlocal call_count
+        call_count += 1
+        return f"Response {call_count} for {prompt}"
+ 
+    # Mock the dspy.cache attribute
+    with patch("dspy.cache", cache):
+        # First call should execute and cache
+        result1 = test_function(prompt="Hello", model="openai/gpt-4o-mini")
+        assert result1 == "Response 1 for Hello"
+        assert call_count == 1
+ 
+        # Second call should use cache
+        result2 = test_function(prompt="Hello", model="openai/gpt-4o-mini")
+        assert result2 == "Response 1 for Hello"  # Same cached result
+        assert call_count == 1  # Function not called again
+ 
+        # Third call with bypass should skip cache read and execute function
+        with cache.bypass():
+            result3 = test_function(prompt="Hello", model="openai/gpt-4o-mini")
+            assert result3 == "Response 2 for Hello"  # New result
+            assert call_count == 2  # Function called again
+ 
+        # Fourth call (outside bypass) should get the newly cached result
+        result4 = test_function(prompt="Hello", model="openai/gpt-4o-mini")
+        assert result4 == "Response 2 for Hello"  # Uses new cached result
+        assert call_count == 2  # Function not called again
+
+
+def test_cache_bypass_thread_safety(cache):
+    """Test that bypass is thread-safe and doesn't affect other threads."""
+    import threading
+    import time
+ 
+    request = {"prompt": "Thread test", "model": "openai/gpt-4o-mini"}
+    cache.put(request, DummyResponse(message="Thread cached value", usage={}))
+ 
+    results = {}
+ 
+    def thread_with_bypass():
+        with cache.bypass():
+            results["bypass_thread"] = cache.get(request)
+            time.sleep(0.1)  # Hold the context for a bit
+            results["bypass_thread_end"] = cache.get(request)
+ 
+    def thread_without_bypass():
+        time.sleep(0.05)  # Start after bypass thread
+        results["normal_thread"] = cache.get(request)
+ 
+    # Start both threads
+    t1 = threading.Thread(target=thread_with_bypass)
+    t2 = threading.Thread(target=thread_without_bypass)
+ 
+    t1.start()
+    t2.start()
+ 
+    t1.join()
+    t2.join()
+ 
+    # Bypass thread should get None
+    assert results["bypass_thread"] is None
+    assert results["bypass_thread_end"] is None
+ 
+    # Normal thread should get the cached value
+    assert results["normal_thread"] is not None
+    assert results["normal_thread"].message == "Thread cached value"

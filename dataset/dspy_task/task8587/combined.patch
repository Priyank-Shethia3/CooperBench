diff --git a/dspy/streaming/messages.py b/dspy/streaming/messages.py
index 9a73f19b..2d3cb7da 100644
--- a/dspy/streaming/messages.py
+++ b/dspy/streaming/messages.py
@@ -14,6 +14,8 @@ class StreamResponse:
     predict_name: str
     signature_field_name: str
     chunk: str
+    is_last_chunk: bool = False
+    buffer_watermark: int = 0
 
 
 @dataclass
diff --git a/dspy/streaming/streaming_listener.py b/dspy/streaming/streaming_listener.py
index 98fd4aca..764f129b 100644
--- a/dspy/streaming/streaming_listener.py
+++ b/dspy/streaming/streaming_listener.py
@@ -1,7 +1,9 @@
 import re
+import time
+import logging
 from collections import defaultdict
 from queue import Queue
-from typing import TYPE_CHECKING, Any
+from typing import TYPE_CHECKING, Any, Callable
 
 from litellm import ModelResponseStream
 
@@ -26,6 +28,11 @@ class StreamListener:
         predict: Any = None,
         predict_name: str | None = None,
         allow_reuse: bool = False,
+        end_buffer_size: int = 10,
+        idle_timeout_s: float | None = None,
+        debug: bool = False,
+        debug_logger: logging.Logger | None = None,
+        on_chunk: Callable[[StreamResponse], None] | None = None,
     ):
         """
         Args:
@@ -44,10 +51,36 @@ class StreamListener:
         self.field_start_queue = []
         self.field_end_queue = Queue()
         self.stream_start = False
-        self.stream_end = False
+        self._stream_end = False
         self.cache_hit = False
         self.allow_reuse = allow_reuse
 
+        # Configurable buffering for end detection
+        if not isinstance(end_buffer_size, int):
+            raise ValueError("end_buffer_size must be an integer")
+        if end_buffer_size < 3:
+            raise ValueError("end_buffer_size must be at least 3")
+        if end_buffer_size > 64:
+            raise ValueError("end_buffer_size must be at most 64")
+        self.end_buffer_size = end_buffer_size
+
+        # Idle timeout configuration
+        self.idle_timeout_s = idle_timeout_s
+        self._last_chunk_ts: float | None = None
+
+        # Debug logging
+        self.debug = debug
+        self._logger = debug_logger or logging.getLogger("dspy.streaming.listener")
+
+        # Metrics
+        self._chunk_count = 0
+        self._char_count = 0
+        self._t0: float | None = None
+        self._t_last: float | None = None
+
+        # Callback hook for each emitted StreamResponse
+        self.on_chunk = on_chunk
+
         self.adapter_identifiers = {
             "ChatAdapter": {
                 "start_identifier": f"[[ ## {self.signature_field_name} ## ]]",
@@ -66,12 +99,49 @@ class StreamListener:
             },
         }
 
+    @property
+    def stream_end(self) -> bool:
+        return self._stream_end
+
+    @stream_end.setter
+    def stream_end(self, value: bool):
+        self._stream_end = value
+        if value:
+            # Reset metrics when stream ends
+            self._chunk_count = 0
+            self._char_count = 0
+            self._t0 = None
+            self._t_last = None
+
     def _buffered_message_end_with_start_identifier(self, concat_message: str, start_identifier: str) -> str:
         for i in range(len(concat_message)):
             if start_identifier.startswith(concat_message[len(concat_message) - i - 1 :]):
                 return True
         return False
 
+    def _truncate_preview(self, text: str, limit: int = 80) -> str:
+        if text is None:
+            return ""
+        if len(text) <= limit:
+            return text
+        return text[:limit - 3] + "..."
+
+    def _reset_state(self):
+        # Clear up the state for the next stream.
+        self._stream_end = False
+        self.cache_hit = False
+        self.field_start_queue = []
+        self.field_end_queue = Queue()
+        self.stream_start = False
+        self._last_chunk_ts = None
+        # Reset metrics
+        self._chunk_count = 0
+        self._char_count = 0
+        self._t0 = None
+        self._t_last = None
+        if self.debug and self._logger.isEnabledFor(logging.DEBUG):
+            self._logger.debug(f"State reset for field '{self.signature_field_name}' (allow_reuse=True)")
+
     def receive(self, chunk: ModelResponseStream):
         adapter_name = settings.adapter.__class__.__name__ if settings.adapter else "ChatAdapter"
         if adapter_name not in self.adapter_identifiers:
@@ -83,14 +153,10 @@ class StreamListener:
         end_identifier = self.adapter_identifiers[adapter_name]["end_identifier"]
         start_indicator = self.adapter_identifiers[adapter_name]["start_indicator"]
 
-        if self.stream_end:
+        if self._stream_end:
             if self.allow_reuse:
                 # Clear up the state for the next stream.
-                self.stream_end = False
-                self.cache_hit = False
-                self.field_start_queue = []
-                self.field_end_queue = Queue()
-                self.stream_start = False
+                self._reset_state()
             else:
                 return
 
@@ -101,6 +167,10 @@ class StreamListener:
         except Exception:
             return
 
+        if chunk_message:
+            now = time.time()
+            self._last_chunk_ts = now
+
         if chunk_message and start_identifier in chunk_message:
             # If the cache is hit, the chunk_message could be the full response. When it happens we can
             # directly end the stream listening. In some models like gemini, each stream chunk can be multiple
@@ -111,7 +181,7 @@ class StreamListener:
             if re.search(end_identifier, message_after_start_identifier):
                 self.cache_hit = True
                 self.stream_start = True
-                self.stream_end = True
+                self._stream_end = True
                 return
 
         if len(self.field_start_queue) == 0 and not self.stream_start and start_indicator in chunk_message:
@@ -138,6 +208,11 @@ class StreamListener:
                     # For JSONAdapter, we need to remove the leading ". We cannot do this with the start_identifier
                     # because there could be a few splitters between ':' and '"', e.g., '"name": "value"'.
                     chunk_message = chunk_message[1:]
+                if self.debug and self._logger.isEnabledFor(logging.DEBUG):
+                    preview = self._truncate_preview(chunk_message)
+                    self._logger.debug(
+                        f"Start detection: adapter={adapter_name}, field='{self.signature_field_name}', stream_start=True, buffer_preview='{preview}'"
+                    )
 
             elif self._buffered_message_end_with_start_identifier(concat_message.strip(), start_identifier):
                 # If the buffered message ends with part of the start_identifier, we keep looking for the
@@ -152,7 +227,7 @@ class StreamListener:
             # The stream is started, we keep returning the token until we see the start of the next field.
             token = None
             self.field_end_queue.put(chunk_message)
-            if self.field_end_queue.qsize() > 10:
+            if self.field_end_queue.qsize() > self.end_buffer_size:
                 # We keep the last 10 tokens in the buffer to check if they form a valid identifier for end_identifier,
                 # i.e., "[[ ## {next_field_name} ## ]]" for ChatAdapter to identify the end of the current field.
                 # In most cases 10 tokens are enough to cover the end_identifier for all adapters.
@@ -160,13 +235,82 @@ class StreamListener:
             concat_message = "".join(self.field_end_queue.queue).strip()
             if re.search(end_identifier, concat_message):
                 # The next field is identified, we can end the stream and flush out all tokens in the buffer.
-                self.stream_end = True
+                self._stream_end = True
                 last_token = self.flush()
                 token = token + last_token if token else last_token
                 token = token.rstrip()  # Remove the trailing \n\n
+                if self.debug and self._logger.isEnabledFor(logging.DEBUG):
+                    self._logger.debug(
+                        f"Rolling end check: adapter={adapter_name}, field='{self.signature_field_name}', reason='regex_match', buffered_size={len(concat_message)}"
+                    )
 
             if token:
-                return StreamResponse(self.predict_name, self.signature_field_name, token)
+                is_last = self._stream_end
+                buffer_watermark = min(self.field_end_queue.qsize(), self.end_buffer_size)
+                if self.debug and self._logger.isEnabledFor(logging.DEBUG):
+                    self._logger.debug(
+                        f"Emit chunk: len(token)={len(token)}, queue_size={self.field_end_queue.qsize()}, is_last_chunk={is_last}"
+                    )
+
+                # Metrics update
+                if self._t0 is None:
+                    self._t0 = time.time()
+                self._t_last = time.time()
+                self._chunk_count += 1
+                self._char_count += len(token)
+
+                response = StreamResponse(
+                    self.predict_name,
+                    self.signature_field_name,
+                    token,
+                    is_last_chunk=is_last,
+                    buffer_watermark=buffer_watermark,
+                )
+                if self.on_chunk is not None:
+                    self.on_chunk(response)
+                return response
+
+    def tick(self, now_ts: float | None = None):
+        """Check idle timeout and emit an empty final chunk if timed out.
+
+        If no chunks have arrived within idle_timeout_s after the stream started, emit a final
+        StreamResponse with an empty chunk and reset state (respecting allow_reuse).
+        """
+        if self.idle_timeout_s is None:
+            return
+        if not self.stream_start or self._stream_end:
+            return
+        if self._last_chunk_ts is None:
+            return
+        now = now_ts if now_ts is not None else time.time()
+        if now - self._last_chunk_ts >= self.idle_timeout_s:
+            self.stream_end = True
+            is_last = True
+            buffer_watermark = min(self.field_end_queue.qsize(), self.end_buffer_size)
+            if self.debug and self._logger.isEnabledFor(logging.DEBUG):
+                self._logger.debug(
+                    f"Emit chunk: len(token)=0, queue_size={self.field_end_queue.qsize()}, is_last_chunk={is_last}"
+                )
+
+            # Metrics update for empty final chunk
+            if self._t0 is None:
+                self._t0 = now
+            self._t_last = now
+            self._chunk_count += 1
+
+            response = StreamResponse(
+                self.predict_name,
+                self.signature_field_name,
+                "",
+                is_last_chunk=is_last,
+                buffer_watermark=buffer_watermark,
+            )
+            if self.on_chunk is not None:
+                self.on_chunk(response)
+
+            # After timeout, always reset state to avoid stuck state on non-reuse cases
+            self._reset_state()
+            return response
 
     def flush(self) -> str:
         """Flush all tokens in the field end queue.
@@ -183,21 +327,50 @@ class StreamListener:
                 boundary_index = match.start()
             else:
                 boundary_index = len(last_tokens)
-            return last_tokens[:boundary_index]
+            truncated = last_tokens[:boundary_index]
+            if self.debug and self._logger.isEnabledFor(logging.DEBUG):
+                self._logger.debug(
+                    f"Flush: adapter={settings.adapter.__class__.__name__ if settings.adapter else 'ChatAdapter'}, field='{self.signature_field_name}', truncated_buffer_length={len(truncated)}"
+                )
+            return truncated
         elif isinstance(settings.adapter, XMLAdapter):
             boundary_index = last_tokens.find(f"</{self.signature_field_name}>")
             if boundary_index == -1:
                 boundary_index = len(last_tokens)
-            return last_tokens[:boundary_index]
+            truncated = last_tokens[:boundary_index]
+            if self.debug and self._logger.isEnabledFor(logging.DEBUG):
+                self._logger.debug(
+                    f"Flush: adapter={settings.adapter.__class__.__name__ if settings.adapter else 'ChatAdapter'}, field='{self.signature_field_name}', truncated_buffer_length={len(truncated)}"
+                )
+            return truncated
         elif isinstance(settings.adapter, ChatAdapter) or settings.adapter is None:
             boundary_index = last_tokens.find("[[")
-            return last_tokens[:boundary_index]
+            truncated = last_tokens[:boundary_index]
+            if self.debug and self._logger.isEnabledFor(logging.DEBUG):
+                self._logger.debug(
+                    f"Flush: adapter={settings.adapter.__class__.__name__ if settings.adapter else 'ChatAdapter'}, field='{self.signature_field_name}', truncated_buffer_length={len(truncated)}"
+                )
+            return truncated
         else:
             raise ValueError(
                 f"Unsupported adapter for streaming: {settings.adapter}, please use one of the following adapters: "
                 f"{', '.join([a.__name__ for a in ADAPTER_SUPPORT_STREAMING])}"
             )
 
+    def stats(self) -> dict[str, Any]:
+        duration = (self._t_last - self._t0) if (self._t0 is not None and self._t_last is not None) else None
+        avg_chunk = (self._char_count / self._chunk_count) if self._chunk_count > 0 else 0
+        return {
+            "predict_name": self.predict_name,
+            "field": self.signature_field_name,
+            "chunk_count": self._chunk_count,
+            "char_count": self._char_count,
+            "first_ts": self._t0,
+            "last_ts": self._t_last,
+            "duration_s": duration,
+            "avg_chunk_chars": avg_chunk,
+        }
+
 
 def find_predictor_for_stream_listeners(program: "Module", stream_listeners: list[StreamListener]):
     """Find the predictor for each stream listener.

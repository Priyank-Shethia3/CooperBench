diff --git a/tests/streaming/test_streaming.py b/tests/streaming/test_streaming.py
index fe67bd6da..05cf2325e 100644
--- a/tests/streaming/test_streaming.py
+++ b/tests/streaming/test_streaming.py
@@ -837,3 +837,169 @@ async def test_stream_listener_returns_correct_chunk_xml_adapter():
     assert all_chunks[1].predict_name == "predict2"
     assert all_chunks[1].signature_field_name == "judgement"
     assert all_chunks[1].chunk == "The answer is humorous."
+
+
+# Test cases for configurable end-buffer size feature
+def test_stream_listener_default_end_buffer_size():
+    """Test that StreamListener defaults to end_buffer_size=10 for backward compatibility."""
+    listener = dspy.streaming.StreamListener(signature_field_name="test")
+    assert listener.end_buffer_size == 10
+
+
+def test_stream_listener_custom_end_buffer_size():
+    """Test that StreamListener accepts custom end_buffer_size values within valid range."""
+    listener = dspy.streaming.StreamListener(signature_field_name="test", end_buffer_size=15)
+    assert listener.end_buffer_size == 15
+ 
+    listener = dspy.streaming.StreamListener(signature_field_name="test", end_buffer_size=5)
+    assert listener.end_buffer_size == 5
+
+
+def test_stream_listener_end_buffer_size_bounds_validation():
+    """Test that StreamListener validates end_buffer_size bounds correctly."""
+    # Test minimum bound
+    with pytest.raises(ValueError, match="end_buffer_size must be at least 3"):
+        dspy.streaming.StreamListener(signature_field_name="test", end_buffer_size=2)
+ 
+    with pytest.raises(ValueError, match="end_buffer_size must be at least 3"):
+        dspy.streaming.StreamListener(signature_field_name="test", end_buffer_size=0)
+ 
+    with pytest.raises(ValueError, match="end_buffer_size must be at least 3"):
+        dspy.streaming.StreamListener(signature_field_name="test", end_buffer_size=-1)
+ 
+    # Test maximum bound
+    with pytest.raises(ValueError, match="end_buffer_size must be at most 64"):
+        dspy.streaming.StreamListener(signature_field_name="test", end_buffer_size=65)
+ 
+    with pytest.raises(ValueError, match="end_buffer_size must be at most 64"):
+        dspy.streaming.StreamListener(signature_field_name="test", end_buffer_size=100)
+ 
+    # Test boundary values (should work)
+    listener = dspy.streaming.StreamListener(signature_field_name="test", end_buffer_size=3)
+    assert listener.end_buffer_size == 3
+ 
+    listener = dspy.streaming.StreamListener(signature_field_name="test", end_buffer_size=64)
+    assert listener.end_buffer_size == 64
+
+
+@pytest.mark.anyio
+async def test_stream_listener_custom_buffer_size_affects_streaming_behavior():
+    """Test that custom end_buffer_size actually affects the streaming behavior."""
+    class MyProgram(dspy.Module):
+        def __init__(self):
+            super().__init__()
+            self.predict = dspy.Predict("question->answer")
+
+        def forward(self, question, **kwargs):
+            return self.predict(question=question, **kwargs)
+
+    async def gpt_4o_mini_stream(*args, **kwargs):
+        # Stream that will test the buffer size behavior
+        yield ModelResponseStream(model="gpt-4o-mini", choices=[StreamingChoices(delta=Delta(content="[[ ## answer ## ]]\n"))])
+        yield ModelResponseStream(model="gpt-4o-mini", choices=[StreamingChoices(delta=Delta(content="Hello"))])
+        yield ModelResponseStream(model="gpt-4o-mini", choices=[StreamingChoices(delta=Delta(content=" world"))])
+        yield ModelResponseStream(model="gpt-4o-mini", choices=[StreamingChoices(delta=Delta(content="!"))])
+        yield ModelResponseStream(model="gpt-4o-mini", choices=[StreamingChoices(delta=Delta(content="\n\n[[ ## completed ## ]]"))])
+
+    with mock.patch("litellm.acompletion", side_effect=gpt_4o_mini_stream):
+        # Test with default buffer size (10)
+        program_default = dspy.streamify(
+            MyProgram(),
+            stream_listeners=[
+                dspy.streaming.StreamListener(signature_field_name="answer"),
+            ],
+        )
+ 
+        # Test with small buffer size (3)
+        program_small = dspy.streamify(
+            MyProgram(),
+            stream_listeners=[
+                dspy.streaming.StreamListener(signature_field_name="answer", end_buffer_size=3),
+            ],
+        )
+ 
+        with dspy.context(lm=dspy.LM("openai/gpt-4o-mini", cache=False), adapter=dspy.ChatAdapter()):
+            # Test default buffer size
+            output_default = program_default(question="What is the answer?")
+            chunks_default = []
+            async for value in output_default:
+                if isinstance(value, dspy.streaming.StreamResponse):
+                    chunks_default.append(value)
+ 
+            # Test small buffer size
+            output_small = program_small(question="What is the answer?")
+            chunks_small = []
+            async for value in output_small:
+                if isinstance(value, dspy.streaming.StreamResponse):
+                    chunks_small.append(value)
+ 
+            # Both should produce the same content but potentially different chunking behavior
+            # due to different buffer sizes
+            assert len(chunks_default) > 0
+            assert len(chunks_small) > 0
+ 
+            # Verify the final content is the same
+            final_content_default = "".join(chunk.chunk for chunk in chunks_default)
+            final_content_small = "".join(chunk.chunk for chunk in chunks_small)
+            assert final_content_default == "Hello world!"
+            assert final_content_small == "Hello world!"
+
+
+def test_stream_response_buffer_watermark():
+    """Test that StreamResponse includes buffer_watermark field with correct values."""
+    # Test that StreamResponse can be created with buffer_watermark
+    response = dspy.streaming.StreamResponse(
+        predict_name="test_predict",
+        signature_field_name="test_field", 
+        chunk="test_chunk",
+        buffer_watermark=5
+    )
+ 
+    assert response.buffer_watermark == 5
+    assert response.predict_name == "test_predict"
+    assert response.signature_field_name == "test_field"
+    assert response.chunk == "test_chunk"
+
+
+@pytest.mark.anyio
+async def test_stream_listener_buffer_watermark_behavior():
+    """Test that StreamListener correctly sets buffer_watermark in StreamResponse."""
+    class MyProgram(dspy.Module):
+        def __init__(self):
+            super().__init__()
+            self.predict = dspy.Predict("question->answer")
+
+        def forward(self, question, **kwargs):
+            return self.predict(question=question, **kwargs)
+
+    async def gpt_4o_mini_stream(*args, **kwargs):
+        # Stream that will test the buffer watermark behavior
+        yield ModelResponseStream(model="gpt-4o-mini", choices=[StreamingChoices(delta=Delta(content="[[ ## answer ## ]]\n"))])
+        yield ModelResponseStream(model="gpt-4o-mini", choices=[StreamingChoices(delta=Delta(content="Hello"))])
+        yield ModelResponseStream(model="gpt-4o-mini", choices=[StreamingChoices(delta=Delta(content=" world"))])
+        yield ModelResponseStream(model="gpt-4o-mini", choices=[StreamingChoices(delta=Delta(content="!"))])
+        yield ModelResponseStream(model="gpt-4o-mini", choices=[StreamingChoices(delta=Delta(content="\n\n[[ ## completed ## ]]"))])
+
+    with mock.patch("litellm.acompletion", side_effect=gpt_4o_mini_stream):
+        # Test with small buffer size to make watermark behavior more observable
+        program = dspy.streamify(
+            MyProgram(),
+            stream_listeners=[
+                dspy.streaming.StreamListener(signature_field_name="answer", end_buffer_size=3),
+            ],
+        )
+ 
+        with dspy.context(lm=dspy.LM("openai/gpt-4o-mini", cache=False), adapter=dspy.ChatAdapter()):
+            output = program(question="What is the answer?")
+            chunks = []
+            async for value in output:
+                if isinstance(value, dspy.streaming.StreamResponse):
+                    chunks.append(value)
+ 
+            # Verify that buffer_watermark is present and reasonable
+            assert len(chunks) > 0
+            for chunk in chunks:
+                assert hasattr(chunk, 'buffer_watermark')
+                assert isinstance(chunk.buffer_watermark, int)
+                # buffer_watermark should be between 0 and end_buffer_size (3)
+                assert 0 <= chunk.buffer_watermark <= 3

diff --git a/tests/streaming/test_streaming.py b/tests/streaming/test_streaming.py
index fe67bd6d..c3dc1ac8 100644
--- a/tests/streaming/test_streaming.py
+++ b/tests/streaming/test_streaming.py
@@ -259,6 +259,424 @@ async def test_streaming_handles_space_correctly():
 
     assert all_chunks[0].chunk == "How are you doing?"
 
+@pytest.mark.anyio
+async def test_stream_listener_debug_mode_basic_functionality():
+    """Test that debug mode can be enabled and custom logger can be provided."""
+    import logging
+ 
+    # Test with default logger
+    listener1 = dspy.streaming.StreamListener(
+        signature_field_name="answer", 
+        debug=True
+    )
+    assert listener1.debug is True
+    assert listener1._logger.name == "dspy.streaming.listener"
+ 
+    # Test with custom logger
+    custom_logger = logging.getLogger("custom.debug.logger")
+    listener2 = dspy.streaming.StreamListener(
+        signature_field_name="answer", 
+        debug=True,
+        debug_logger=custom_logger
+    )
+    assert listener2.debug is True
+    assert listener2._logger is custom_logger
+ 
+    # Test debug mode disabled by default
+    listener3 = dspy.streaming.StreamListener(signature_field_name="answer")
+    assert listener3.debug is False
+
+
+@pytest.mark.anyio
+async def test_stream_listener_debug_logging_during_streaming():
+    """Test that debug logging actually works during real streaming events."""
+    import logging
+    from io import StringIO
+ 
+    # Set up logging capture
+    log_stream = StringIO()
+    handler = logging.StreamHandler(log_stream)
+    handler.setLevel(logging.DEBUG)
+ 
+    logger = logging.getLogger("test.streaming.debug")
+    logger.addHandler(handler)
+    logger.setLevel(logging.DEBUG)
+    logger.propagate = False
+ 
+    class MyProgram(dspy.Module):
+        def __init__(self):
+            super().__init__()
+            self.predict = dspy.Predict("question->answer")
+
+        def forward(self, question, **kwargs):
+            return self.predict(question=question, **kwargs)
+
+    my_program = MyProgram()
+    program = dspy.streamify(
+        my_program, 
+        stream_listeners=[
+            dspy.streaming.StreamListener(
+                signature_field_name="answer",
+                debug=True,
+                debug_logger=logger
+            )
+        ]
+    )
+
+    async def gpt_4o_mini_stream(*args, **kwargs):
+        yield ModelResponseStream(
+            model="gpt-4o-mini", 
+            choices=[StreamingChoices(delta=Delta(content="[[ ## answer ## ]]\n"))]
+        )
+        yield ModelResponseStream(
+            model="gpt-4o-mini", 
+            choices=[StreamingChoices(delta=Delta(content="Hello world!"))]
+        )
+        yield ModelResponseStream(
+            model="gpt-4o-mini", 
+            choices=[StreamingChoices(delta=Delta(content="\n\n[[ ## completed ## ]]"))]
+        )
+
+    with mock.patch("litellm.acompletion", side_effect=gpt_4o_mini_stream):
+        with dspy.context(lm=dspy.LM("openai/gpt-4o-mini", cache=False), adapter=dspy.ChatAdapter()):
+            output = program(question="What is the capital of France?")
+            all_chunks = []
+            async for value in output:
+                if isinstance(value, dspy.streaming.StreamResponse):
+                    all_chunks.append(value)
+
+    # Verify the streaming worked
+    assert len(all_chunks) > 0
+    assert all_chunks[0].chunk == "Hello world!"
+ 
+    # Check that debug logging actually occurred
+    log_output = log_stream.getvalue()
+    assert "Start detection: adapter=ChatAdapter, field='answer', stream_start=True" in log_output
+    assert "Emit chunk: len(token)=" in log_output
+
+
+@pytest.mark.anyio
+async def test_stream_listener_debug_logging_with_json_adapter():
+    """Test debug logging works with JSON adapter during streaming."""
+    import logging
+    from io import StringIO
+ 
+    # Set up logging capture
+    log_stream = StringIO()
+    handler = logging.StreamHandler(log_stream)
+    handler.setLevel(logging.DEBUG)
+ 
+    logger = logging.getLogger("test.json.debug")
+    logger.addHandler(handler)
+    logger.setLevel(logging.DEBUG)
+    logger.propagate = False
+ 
+    class MyProgram(dspy.Module):
+        def __init__(self):
+            super().__init__()
+            self.predict = dspy.Predict("question->answer")
+
+        def forward(self, question, **kwargs):
+            return self.predict(question=question, **kwargs)
+
+    my_program = MyProgram()
+    program = dspy.streamify(
+        my_program, 
+        stream_listeners=[
+            dspy.streaming.StreamListener(
+                signature_field_name="answer",
+                debug=True,
+                debug_logger=logger
+            )
+        ]
+    )
+
+    async def json_stream(*args, **kwargs):
+        yield ModelResponseStream(
+            model="gpt-4o-mini", 
+            choices=[StreamingChoices(delta=Delta(content='{"'))]
+        )
+        yield ModelResponseStream(
+            model="gpt-4o-mini", 
+            choices=[StreamingChoices(delta=Delta(content="answer"))]
+        )
+        yield ModelResponseStream(
+            model="gpt-4o-mini", 
+            choices=[StreamingChoices(delta=Delta(content='":'))]
+        )
+        yield ModelResponseStream(
+            model="gpt-4o-mini", 
+            choices=[StreamingChoices(delta=Delta(content="Hello"))]
+        )
+        yield ModelResponseStream(
+            model="gpt-4o-mini", 
+            choices=[StreamingChoices(delta=Delta(content=" world!"))]
+        )
+        yield ModelResponseStream(
+            model="gpt-4o-mini", 
+            choices=[StreamingChoices(delta=Delta(content='"}'))]
+        )
+
+    with mock.patch("litellm.acompletion", side_effect=json_stream):
+        with dspy.context(lm=dspy.LM("openai/gpt-4o-mini", cache=False), adapter=dspy.JSONAdapter()):
+            output = program(question="What is the capital of France?")
+            all_chunks = []
+            async for value in output:
+                if isinstance(value, dspy.streaming.StreamResponse):
+                    all_chunks.append(value)
+
+    # Verify the streaming worked
+    assert len(all_chunks) > 0
+ 
+    # Check that debug logging occurred with JSON adapter
+    log_output = log_stream.getvalue()
+    assert "Start detection: adapter=JSONAdapter, field='answer', stream_start=True" in log_output
+
+
+@pytest.mark.anyio
+async def test_stream_listener_debug_logging_with_allow_reuse():
+    """Test debug logging works when allow_reuse=True and state reset occurs."""
+    import logging
+    from io import StringIO
+ 
+    # Set up logging capture
+    log_stream = StringIO()
+    handler = logging.StreamHandler(log_stream)
+    handler.setLevel(logging.DEBUG)
+ 
+    logger = logging.getLogger("test.reuse.debug")
+    logger.addHandler(handler)
+    logger.setLevel(logging.DEBUG)
+    logger.propagate = False
+ 
+    class MyProgram(dspy.Module):
+        def __init__(self):
+            super().__init__()
+            self.predict = dspy.Predict("question->answer")
+
+        def forward(self, question, **kwargs):
+            # Call predict twice to trigger reuse
+            result1 = self.predict(question=question, **kwargs)
+            result2 = self.predict(question=question, **kwargs)
+            return result2
+
+    my_program = MyProgram()
+    program = dspy.streamify(
+        my_program, 
+        stream_listeners=[
+            dspy.streaming.StreamListener(
+                signature_field_name="answer",
+                allow_reuse=True,
+                debug=True,
+                debug_logger=logger
+            )
+        ]
+    )
+
+    async def reuse_stream(*args, **kwargs):
+        # First call
+        yield ModelResponseStream(
+            model="gpt-4o-mini", 
+            choices=[StreamingChoices(delta=Delta(content="[[ ## answer ## ]]\n"))]
+        )
+        yield ModelResponseStream(
+            model="gpt-4o-mini", 
+            choices=[StreamingChoices(delta=Delta(content="First answer"))]
+        )
+        yield ModelResponseStream(
+            model="gpt-4o-mini", 
+            choices=[StreamingChoices(delta=Delta(content="\n\n[[ ## completed ## ]]"))]
+        )
+        # Second call (reuse)
+        yield ModelResponseStream(
+            model="gpt-4o-mini", 
+            choices=[StreamingChoices(delta=Delta(content="[[ ## answer ## ]]\n"))]
+        )
+        yield ModelResponseStream(
+            model="gpt-4o-mini", 
+            choices=[StreamingChoices(delta=Delta(content="Second answer"))]
+        )
+        yield ModelResponseStream(
+            model="gpt-4o-mini", 
+            choices=[StreamingChoices(delta=Delta(content="\n\n[[ ## completed ## ]]"))]
+        )
+
+    with mock.patch("litellm.acompletion", side_effect=reuse_stream):
+        with dspy.context(lm=dspy.LM("openai/gpt-4o-mini", cache=False), adapter=dspy.ChatAdapter()):
+            output = program(question="why did a chicken cross the kitchen?")
+            all_chunks = []
+            async for value in output:
+                if isinstance(value, dspy.streaming.StreamResponse):
+                    all_chunks.append(value)
+
+    # Verify the streaming worked with reuse
+    assert len(all_chunks) > 0
+ 
+    # Check that debug logging occurred, including state reset
+    log_output = log_stream.getvalue()
+    assert "State reset for field 'answer' (allow_reuse=True)" in log_output
+
+
+@pytest.mark.anyio
+async def test_stream_listener_debug_logging_performance_guardrails():
+    """Test that debug logging respects performance guardrails during actual streaming."""
+    import logging
+    from io import StringIO
+ 
+    # Set up logging capture with INFO level (not DEBUG)
+    log_stream = StringIO()
+    handler = logging.StreamHandler(log_stream)
+    handler.setLevel(logging.INFO)
+ 
+    logger = logging.getLogger("test.performance.debug")
+    logger.addHandler(handler)
+    logger.setLevel(logging.INFO)
+    logger.propagate = False
+ 
+    class MyProgram(dspy.Module):
+        def __init__(self):
+            super().__init__()
+            self.predict = dspy.Predict("question->answer")
+
+        def forward(self, question, **kwargs):
+            return self.predict(question=question, **kwargs)
+
+    my_program = MyProgram()
+    program = dspy.streamify(
+        my_program, 
+        stream_listeners=[
+            dspy.streaming.StreamListener(
+                signature_field_name="answer",
+                debug=True,  # Debug enabled
+                debug_logger=logger
+            )
+        ]
+    )
+
+    async def performance_stream(*args, **kwargs):
+        yield ModelResponseStream(
+            model="gpt-4o-mini", 
+            choices=[StreamingChoices(delta=Delta(content="[[ ## answer ## ]]\n"))]
+        )
+        yield ModelResponseStream(
+            model="gpt-4o-mini", 
+            choices=[StreamingChoices(delta=Delta(content="Test answer"))]
+        )
+        yield ModelResponseStream(
+            model="gpt-4o-mini", 
+            choices=[StreamingChoices(delta=Delta(content="\n\n[[ ## completed ## ]]"))]
+        )
+
+    with mock.patch("litellm.acompletion", side_effect=performance_stream):
+        with dspy.context(lm=dspy.LM("openai/gpt-4o-mini", cache=False), adapter=dspy.ChatAdapter()):
+            output = program(question="What is the capital of France?")
+            all_chunks = []
+            async for value in output:
+                if isinstance(value, dspy.streaming.StreamResponse):
+                    all_chunks.append(value)
+
+    # Verify the streaming worked
+    assert len(all_chunks) > 0
+ 
+    # Even though debug=True, logging should not occur because logger level is INFO
+    log_output = log_stream.getvalue()
+    assert "Start detection:" not in log_output
+    assert "Emit chunk:" not in log_output
+
+
+@pytest.mark.anyio
+async def test_stream_listener_debug_safe_truncation():
+    """Test that buffer previews are safely truncated to avoid large string formatting."""
+    import logging
+    from io import StringIO
+ 
+    # Set up logging capture
+    log_stream = StringIO()
+    handler = logging.StreamHandler(log_stream)
+    handler.setLevel(logging.DEBUG)
+ 
+    logger = logging.getLogger("test.truncation")
+    logger.addHandler(handler)
+    logger.setLevel(logging.DEBUG)
+    logger.propagate = False
+ 
+    class MyProgram(dspy.Module):
+        def __init__(self):
+            super().__init__()
+            self.predict = dspy.Predict("question->answer")
+
+        def forward(self, question, **kwargs):
+            return self.predict(question=question, **kwargs)
+
+    my_program = MyProgram()
+    program = dspy.streamify(
+        my_program, 
+        stream_listeners=[
+            dspy.streaming.StreamListener(
+                signature_field_name="answer",
+                debug=True,
+                debug_logger=logger
+            )
+        ]
+    )
+
+    async def long_content_stream(*args, **kwargs):
+        # Create a very long message that should be truncated
+        long_message = "A" * 200  # 200 characters, should be truncated to ~80
+        yield ModelResponseStream(
+            model="gpt-4o-mini", 
+            choices=[StreamingChoices(delta=Delta(content=f"[[ ## answer ## ]]{long_message}"))]
+        )
+        yield ModelResponseStream(
+            model="gpt-4o-mini", 
+            choices=[StreamingChoices(delta=Delta(content="\n\n[[ ## completed ## ]]"))]
+        )
+
+    with mock.patch("litellm.acompletion", side_effect=long_content_stream):
+        with dspy.context(lm=dspy.LM("openai/gpt-4o-mini", cache=False), adapter=dspy.ChatAdapter()):
+            output = program(question="What is the capital of France?")
+            all_chunks = []
+            async for value in output:
+                if isinstance(value, dspy.streaming.StreamResponse):
+                    all_chunks.append(value)
+
+    # Verify the streaming worked
+    assert len(all_chunks) > 0
+ 
+    # Check that the log contains truncated content
+    log_output = log_stream.getvalue()
+    assert "Start detection:" in log_output
+    assert "buffer_preview=" in log_output
+ 
+    # The buffer preview should be truncated (around 80 chars)
+    import re
+    match = re.search(r"buffer_preview='([^']*)'", log_output)
+    if match:
+        preview = match.group(1)
+        assert len(preview) <= 85  # Allow some flexibility for "..." and formatting
+        assert "..." in preview  # Should contain truncation indicator
+
+
+@pytest.mark.anyio
+async def test_stream_listener_debug_no_runtime_behavior_change():
+    """Test that debug mode does not change runtime behavior during actual streaming."""
+    # Create two listeners - one with debug enabled, one without
+    listener_no_debug = dspy.streaming.StreamListener(signature_field_name="answer")
+    listener_with_debug = dspy.streaming.StreamListener(
+        signature_field_name="answer", 
+        debug=True
+    )
+ 
+    # Test that they have the same basic attributes
+    assert listener_no_debug.signature_field_name == listener_with_debug.signature_field_name
+    assert listener_no_debug.allow_reuse == listener_with_debug.allow_reuse
+    assert listener_no_debug.adapter_identifiers == listener_with_debug.adapter_identifiers
+ 
+    # Test that debug attributes are properly set
+    assert listener_with_debug.debug is True
+    assert listener_with_debug._logger is not None
+    assert listener_no_debug.debug is False
+
 
 @pytest.mark.llm_call
 def test_sync_streaming(lm_for_test):

diff --git a/dspy/propose/grounded_proposer.py b/dspy/propose/grounded_proposer.py
index 13722b4b..a0613945 100644
--- a/dspy/propose/grounded_proposer.py
+++ b/dspy/propose/grounded_proposer.py
@@ -43,6 +43,11 @@ class DescribeProgram(dspy.Signature):
         desc="Describe what task the program is designed to solve, and how it goes about solving this task.",
         prefix="SUMMARY OF PROGRAM ABOVE:",
     )
+    cache_salt = dspy.InputField(
+        format=str,
+        desc="For caching only. Ignored by the model.",
+        prefix="CACHE SALT (IGNORE):",
+    )
 
 
 class DescribeModule(dspy.Signature):
@@ -70,6 +75,11 @@ class DescribeModule(dspy.Signature):
         desc="Description of the module's role in the broader program.",
         prefix="MODULE DESCRIPTION:",
     )
+    cache_salt = dspy.InputField(
+        format=str,
+        desc="For caching only. Ignored by the model.",
+        prefix="CACHE SALT (IGNORE):",
+    )
 
 
 def generate_instruction_class(
@@ -128,6 +138,13 @@ def generate_instruction_class(
             desc="Propose an instruction that will be used to prompt a Language Model to perform this task.",
             prefix="PROPOSED INSTRUCTION:",
         )
+ 
+        # Always include cache salt for caching purposes
+        cache_salt = dspy.InputField(
+            format=str,
+            desc="For caching only. Ignored by the model.",
+            prefix="CACHE SALT (IGNORE):",
+        )
 
     return dspy.Predict(GenerateSingleModuleInstruction)
 
@@ -173,6 +190,7 @@ class GenerateModuleInstruction(dspy.Module):
         data_summary,
         num_demos_in_context=3,
         tip=None,
+        cache_salt=None,
     ):
         def gather_examples_from_sets(candidate_sets, max_examples):
             """Helper function to gather up to augmented examples from given sets."""
@@ -212,9 +230,14 @@ class GenerateModuleInstruction(dspy.Module):
         module_description = "Not provided"
         if self.program_aware:
             try:
+                # Use the same cache salt for all calls in this instruction proposal
+                current_cache_salt = cache_salt or self._cache_salt(pred_i=pred_i, demo_set_i=demo_set_i)
+ 
                 program_description = strip_prefix(
                     self.describe_program(
-                        program_code=self.program_code_string, program_example=task_demos,
+                        program_code=self.program_code_string, 
+                        program_example=task_demos,
+                        cache_salt=current_cache_salt,
                     ).program_description,
                 )
                 if self.verbose:
@@ -240,6 +263,7 @@ class GenerateModuleInstruction(dspy.Module):
                     program_example=task_demos,
                     module=module_code,
                     max_depth=10,
+                    cache_salt=current_cache_salt,
                 ).module_description
             except Exception as e:
                 if self.verbose:
@@ -260,6 +284,7 @@ class GenerateModuleInstruction(dspy.Module):
             tip=tip,
             basic_instruction=basic_instruction,
             previous_instructions=previous_instructions,
+            cache_salt=cache_salt,
         )
 
         proposed_instruction = strip_prefix(instruct.proposed_instruction)
@@ -323,6 +348,27 @@ class GroundedProposer(Proposer):
                 self.use_dataset_summary = False
                 print("")
 
+    def _cache_salt(self, pred_i: int, demo_set_i: int) -> str:
+        """Generate a deterministic cache salt for instruction proposal.
+ 
+        Args:
+            pred_i: Predictor index
+            demo_set_i: Demo set index
+ 
+        Returns:
+            A 13-character ASCII string derived from seeded RNG and stable IDs
+        """
+        # Deterministic given self.rng (seeded upstream) + stable IDs
+        n = self.rng.getrandbits(64) ^ ((pred_i & 0xFFFF) << 16) ^ (demo_set_i & 0xFFFF)
+        # Base32 (no padding) for compact ASCII salt
+        alphabet = "ABCDEFGHIJKLMNOPQRSTUVWXYZ234567"
+        out = []
+        x = n
+        for _ in range(13):  # 13*5=65 bits coverage
+            out.append(alphabet[x & 31])
+            x >>= 5
+        return "".join(out)
+
     def propose_instructions_for_program(
         self,
         trainset,
@@ -414,14 +460,12 @@ class GroundedProposer(Proposer):
             verbose=self.verbose
         )
 
-        # Generate a new instruction for our predictor, using the temperature specified for this round
-        original_temp = self.prompt_model.kwargs["temperature"]
-
-        epsilon = self.rng.uniform(0.01, 0.05)
-        modified_temp = T + epsilon
-
+        # Generate a new instruction for our predictor
+        # Note: T parameter kept for API compatibility; no kwarg modulation
+        # Compute cache salt for this instruction generation
+        cache_salt = self._cache_salt(pred_i=pred_i, demo_set_i=demo_set_i)
+ 
         with dspy.settings.context(lm=self.prompt_model):
-            self.prompt_model.kwargs["temperature"] = modified_temp
             proposed_instruction = instruction_generator(
                 demo_candidates=demo_candidates,
                 pred_i=pred_i,
@@ -431,8 +475,8 @@ class GroundedProposer(Proposer):
                 previous_instructions=instruction_history,
                 num_demos_in_context = self.num_demos_in_context,
                 tip=tip,
+                cache_salt=cache_salt,
             ).proposed_instruction
-        self.prompt_model.kwargs["temperature"] = original_temp
 
         # Log the trace used to generate the new instruction, along with the new instruction itself
         if self.verbose:


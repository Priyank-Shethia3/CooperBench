diff --git a/dspy/propose/grounded_proposer.py b/dspy/propose/grounded_proposer.py
index 13722b4b..2d60a9b6 100644
--- a/dspy/propose/grounded_proposer.py
+++ b/dspy/propose/grounded_proposer.py
@@ -12,7 +12,6 @@ from dspy.propose.utils import (
 from dspy.teleprompt.utils import get_prompt_model, get_signature
 
 # Hardcoded variables (TODO: update)
-MAX_INSTRUCT_IN_HISTORY = 5  # 10
 
 TIPS = {
         "none": "",
@@ -43,6 +42,11 @@ class DescribeProgram(dspy.Signature):
         desc="Describe what task the program is designed to solve, and how it goes about solving this task.",
         prefix="SUMMARY OF PROGRAM ABOVE:",
     )
+    cache_salt = dspy.InputField(
+        format=str,
+        desc="For caching only. Ignored by the model.",
+        prefix="CACHE SALT (IGNORE):",
+    )
 
 
 class DescribeModule(dspy.Signature):
@@ -70,6 +74,11 @@ class DescribeModule(dspy.Signature):
         desc="Description of the module's role in the broader program.",
         prefix="MODULE DESCRIPTION:",
     )
+    cache_salt = dspy.InputField(
+        format=str,
+        desc="For caching only. Ignored by the model.",
+        prefix="CACHE SALT (IGNORE):",
+    )
 
 
 def generate_instruction_class(
@@ -128,6 +137,11 @@ def generate_instruction_class(
             desc="Propose an instruction that will be used to prompt a Language Model to perform this task.",
             prefix="PROPOSED INSTRUCTION:",
         )
+        cache_salt = dspy.InputField(
+            format=str,
+            desc="For caching only. Ignored by the model.",
+            prefix="CACHE SALT (IGNORE):",
+        )
 
     return dspy.Predict(GenerateSingleModuleInstruction)
 
@@ -143,6 +157,10 @@ class GenerateModuleInstruction(dspy.Module):
         use_instruct_history=True,
         use_tip=True,
         verbose=False,
+        max_description_chars=2000,
+        min_instr_chars=30,
+        max_instr_chars=600,
+        rephrase_when_too_long=False,
     ):
         super().__init__()
         self.use_dataset_summary = use_dataset_summary
@@ -151,6 +169,10 @@ class GenerateModuleInstruction(dspy.Module):
         self.use_instruct_history = use_instruct_history
         self.use_tip = use_tip
         self.verbose = verbose
+        self.max_description_chars = max_description_chars
+        self.min_instr_chars = min_instr_chars
+        self.max_instr_chars = max_instr_chars
+        self.rephrase_when_too_long = rephrase_when_too_long
 
         self.program_code_string = program_code_string
         self.describe_program = dspy.Predict(DescribeProgram)
@@ -162,6 +184,14 @@ class GenerateModuleInstruction(dspy.Module):
             use_instruct_history=use_instruct_history,
             use_tip=use_tip,
         )
+        
+        class ShortenInstruction(dspy.Signature):
+            ("""You will be given a verbose instruction. Rephrase it to be concise, clear, and under the character limit.""")
+            instruction = dspy.InputField(format=str, desc="The long instruction.", prefix="INSTRUCTION:")
+            character_limit = dspy.InputField(format=int, desc="Maximum allowed characters.", prefix="CHAR LIMIT:")
+            proposed_instruction = dspy.OutputField(desc="A concise version under the character limit.", prefix="PROPOSED INSTRUCTION:")
+
+        self._shorten_instruction = dspy.Predict(ShortenInstruction)
 
     def forward(
         self,
@@ -173,6 +203,7 @@ class GenerateModuleInstruction(dspy.Module):
         data_summary,
         num_demos_in_context=3,
         tip=None,
+        cache_salt=None,
     ):
         def gather_examples_from_sets(candidate_sets, max_examples):
             """Helper function to gather up to augmented examples from given sets."""
@@ -214,9 +245,16 @@ class GenerateModuleInstruction(dspy.Module):
             try:
                 program_description = strip_prefix(
                     self.describe_program(
-                        program_code=self.program_code_string, program_example=task_demos,
+                        program_code=self.program_code_string,
+                        program_example=task_demos,
+                        cache_salt=cache_salt,
                     ).program_description,
                 )
+                if self.max_description_chars is not None:
+                    program_description = self._truncate_description_at_sentence_boundary(
+                        program_description,
+                        self.max_description_chars,
+                    )
                 if self.verbose:
                     print(f"PROGRAM DESCRIPTION: {program_description}")
 
@@ -240,7 +278,13 @@ class GenerateModuleInstruction(dspy.Module):
                     program_example=task_demos,
                     module=module_code,
                     max_depth=10,
+                    cache_salt=cache_salt,
                 ).module_description
+                if self.max_description_chars is not None:
+                    module_description = self._truncate_description_at_sentence_boundary(
+                        module_description,
+                        self.max_description_chars,
+                    )
             except Exception as e:
                 if self.verbose:
                     print(f"Error getting program description. Running without program aware proposer. Error: {e}")
@@ -260,12 +304,68 @@ class GenerateModuleInstruction(dspy.Module):
             tip=tip,
             basic_instruction=basic_instruction,
             previous_instructions=previous_instructions,
+            cache_salt=cache_salt,
         )
 
         proposed_instruction = strip_prefix(instruct.proposed_instruction)
 
+        # Enforce instruction length bounds and optional rephrasing
+        if len(proposed_instruction) < self.min_instr_chars:
+            if self.verbose:
+                print(
+                    f"Instruction too short ({len(proposed_instruction)} < {self.min_instr_chars}), using basic instruction."
+                )
+            proposed_instruction = strip_prefix(basic_instruction)
+        elif len(proposed_instruction) > self.max_instr_chars:
+            if self.rephrase_when_too_long:
+                if self.verbose:
+                    print(
+                        f"Instruction too long ({len(proposed_instruction)} > {self.max_instr_chars}), rephrasing."
+                    )
+                shortened = self._shorten_instruction(
+                    instruction=proposed_instruction,
+                    character_limit=self.max_instr_chars,
+                ).proposed_instruction
+                proposed_instruction = strip_prefix(shortened)
+                if len(proposed_instruction) > self.max_instr_chars:
+                    proposed_instruction = self._truncate_description_at_sentence_boundary(
+                        proposed_instruction,
+                        self.max_instr_chars,
+                    )
+            else:
+                if self.verbose:
+                    print(
+                        f"Instruction too long ({len(proposed_instruction)} > {self.max_instr_chars}), truncating."
+                    )
+                proposed_instruction = self._truncate_description_at_sentence_boundary(
+                    proposed_instruction,
+                    self.max_instr_chars,
+                )
+
         return dspy.Prediction(proposed_instruction=proposed_instruction)
 
+    def _truncate_description_at_sentence_boundary(self, text, cap):
+        if text is None:
+            return text
+        if len(text) <= cap:
+            return text
+        boundaries = [". ", "! ", "? "]
+        cutoff = -1
+        for b in boundaries:
+            idx = text.rfind(b, 0, cap)
+            if idx > cutoff:
+                cutoff = idx + len(b)
+        if cutoff <= 0:
+            # Hard truncate to leave space for ellipsis
+            return (text[: max(0, cap - 1)] + "…")
+        # Trim at boundary and append ellipsis if we trimmed anything beyond cap
+        trimmed = text[:cutoff].rstrip()
+        if len(trimmed) > cap:
+            trimmed = trimmed[: max(0, cap - 1)]
+        if len(trimmed) < len(text):
+            return trimmed + "…"
+        return trimmed
+
 ### CLASS USED TO GENERATE THE FULL SET OF INSTRUCTIONS GIVEN THE SPECIFIED CRITERIA ###
 
 class GroundedProposer(Proposer):
@@ -284,7 +384,13 @@ class GroundedProposer(Proposer):
         set_tip_randomly=True,
         set_history_randomly=True,
         verbose=False,
-        rng=None
+        rng=None,
+        max_instruct_history=5,
+        tip_weights=None,
+        max_description_chars=2000,
+        min_instr_chars=30,
+        max_instr_chars=600,
+        rephrase_when_too_long=False,
     ):
         super().__init__()
         self.program_aware = program_aware
@@ -297,6 +403,15 @@ class GroundedProposer(Proposer):
         self.set_history_randomly=set_history_randomly
         self.verbose = verbose
         self.rng = rng or random
+        # Clamp history window
+        try:
+            max_instruct_history = int(max_instruct_history)
+        except Exception:
+            max_instruct_history = 5
+        self.max_instruct_history = max(0, min(20, max_instruct_history))
+
+        # Optional weighted tip selection
+        self.tip_weights = tip_weights
 
         self.prompt_model = get_prompt_model(prompt_model)
 
@@ -323,6 +438,35 @@ class GroundedProposer(Proposer):
                 self.use_dataset_summary = False
                 print("")
 
+        # Settings that affect GenerateModuleInstruction
+        self.max_description_chars = max_description_chars
+        self.min_instr_chars = min_instr_chars
+        self.max_instr_chars = max_instr_chars
+        self.rephrase_when_too_long = rephrase_when_too_long
+
+    def _truncate_description_at_sentence_boundary(self, text, cap):
+        if cap is None:
+            return text
+        if text is None:
+            return text
+        if len(text) <= cap:
+            return text
+        boundaries = [". ", "! ", "? "]
+        cutoff = -1
+        for b in boundaries:
+            idx = text.rfind(b, 0, cap)
+            if idx > cutoff:
+                cutoff = idx + len(b)
+        if cutoff <= 0:
+            return (text[: max(0, cap - 1)] + "…")
+        trimmed = text[:cutoff]
+        # Ensure final result length is strictly less than cap after adding ellipsis
+        if len(trimmed) + 1 >= cap:
+            trimmed = trimmed[: max(0, cap - 2)]
+        if len(trimmed) < len(text):
+            return trimmed + "…"
+        return trimmed
+
     def propose_instructions_for_program(
         self,
         trainset,
@@ -361,14 +505,18 @@ class GroundedProposer(Proposer):
                 if self.set_tip_randomly:
                     if self.verbose:
                         print("Using a randomly generated configuration for our grounded proposer.")
-                    # Randomly select the tip
-                    selected_tip_key = self.rng.choice(list(TIPS.keys()))
+                    # Select the tip (uniform or weighted)
+                    if self.tip_weights is None:
+                        selected_tip_key = self.rng.choice(list(TIPS.keys()))
+                    else:
+                        selected_tip_key = self._select_tip_with_weights()
                     selected_tip = TIPS[selected_tip_key]
                     self.use_tip = bool(
                         selected_tip,
                     )
                     if self.verbose:
                         print(f"Selected tip: {selected_tip_key}")
+                        print(f"Chosen tip: {selected_tip} (tip_key={selected_tip_key})")
 
                 proposed_instructions[pred_i].append(
                     self.propose_instruction_for_predictor(
@@ -390,7 +538,7 @@ class GroundedProposer(Proposer):
         program,
         predictor,
         pred_i,
-        T, # noqa: N803
+        T, # noqa: N803  # kept for API compatibility; no kwarg modulation
         demo_candidates,
         demo_set_i,
         trial_logs,
@@ -400,7 +548,7 @@ class GroundedProposer(Proposer):
 
         # Create an instruction history string for our predictor
         instruction_history = create_predictor_level_history_string(
-            program, pred_i, trial_logs, MAX_INSTRUCT_IN_HISTORY,
+            program, pred_i, trial_logs, self.max_instruct_history,
         )
 
         # Create our instruction generator class (given specific criteria for this round of proposal)
@@ -411,17 +559,16 @@ class GroundedProposer(Proposer):
             use_task_demos=self.use_task_demos and demo_candidates,
             use_instruct_history=self.use_instruct_history and instruction_history,
             use_tip=self.use_tip,
-            verbose=self.verbose
+            verbose=self.verbose,
+            max_description_chars=self.max_description_chars,
+            min_instr_chars=self.min_instr_chars,
+            max_instr_chars=self.max_instr_chars,
+            rephrase_when_too_long=self.rephrase_when_too_long,
         )
 
-        # Generate a new instruction for our predictor, using the temperature specified for this round
-        original_temp = self.prompt_model.kwargs["temperature"]
-
-        epsilon = self.rng.uniform(0.01, 0.05)
-        modified_temp = T + epsilon
-
+        # Generate a new instruction for our predictor
+        cache_salt = self._cache_salt(pred_i=pred_i, demo_set_i=demo_set_i)
         with dspy.settings.context(lm=self.prompt_model):
-            self.prompt_model.kwargs["temperature"] = modified_temp
             proposed_instruction = instruction_generator(
                 demo_candidates=demo_candidates,
                 pred_i=pred_i,
@@ -431,8 +578,8 @@ class GroundedProposer(Proposer):
                 previous_instructions=instruction_history,
                 num_demos_in_context = self.num_demos_in_context,
                 tip=tip,
+                cache_salt=cache_salt,
             ).proposed_instruction
-        self.prompt_model.kwargs["temperature"] = original_temp
 
         # Log the trace used to generate the new instruction, along with the new instruction itself
         if self.verbose:
@@ -440,3 +587,45 @@ class GroundedProposer(Proposer):
             print(f"PROPOSED INSTRUCTION: {proposed_instruction}")
 
         return strip_prefix(proposed_instruction)
+
+    def _select_tip_with_weights(self):
+        # Weighted random selection using self.rng
+        keys = list(TIPS.keys())
+        weights = []
+        total = 0.0
+        for k in keys:
+            w = 1.0
+            if isinstance(self.tip_weights, dict) and k in self.tip_weights and isinstance(self.tip_weights[k], (int, float)):
+                w = float(self.tip_weights[k])
+            if w < 0:
+                w = 0.0
+            weights.append(w)
+            total += w
+        if total <= 0:
+            # Fallback to uniform if all weights are zero/invalid
+            return self.rng.choice(keys)
+        # Normalize and sample
+        threshold = self.rng.random()
+        cumulative = 0.0
+        for k, w in zip(keys, weights):
+            p = w / total
+            cumulative += p
+            if threshold <= cumulative:
+                return k
+        return keys[-1]
+
+    def _cache_salt(self, pred_i: int, demo_set_i: int) -> str:
+        # Deterministic given self.rng (seeded upstream) + stable IDs
+        if hasattr(self.rng, "getrandbits"):
+            base = self.rng.getrandbits(64)
+        else:
+            # Fallback if RNG does not support getrandbits (e.g., simple mocks)
+            base = ((1103515245 * ((pred_i << 16) ^ demo_set_i) + 12345) & ((1 << 31) - 1))
+        n = base ^ ((pred_i & 0xFFFF) << 16) ^ (demo_set_i & 0xFFFF)
+        # Base32 (no padding) for compact ASCII salt
+        alphabet = "ABCDEFGHIJKLMNOPQRSTUVWXYZ234567"
+        out = []
+        x = n
+        for _ in range(13):  # 13*5=65 bits coverage
+            out.append(alphabet[x & 31]); x >>= 5
+        return "".join(out)

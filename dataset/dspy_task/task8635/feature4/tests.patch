diff --git a/tests/propose/test_grounded_proposer4.py b/tests/propose/test_grounded_proposer4.py
new file mode 100644
index 00000000..3d13ee04
--- /dev/null
+++ b/tests/propose/test_grounded_proposer4.py
@@ -0,0 +1,123 @@
+import pytest
+
+import dspy
+from dspy.predict import Predict
+from dspy.propose.grounded_proposer import GroundedProposer
+from dspy.utils.dummies import DummyLM
+
+
+def test_max_description_chars_parameter_default():
+    """Test that max_description_chars parameter defaults to 2000"""
+    prompt_model = DummyLM([{"proposed_instruction": "instruction"}] * 10)
+    program = Predict("question -> answer")
+    trainset = []
+ 
+    proposer = GroundedProposer(prompt_model=prompt_model, program=program, trainset=trainset, verbose=False)
+    assert hasattr(proposer, 'max_description_chars')
+    assert proposer.max_description_chars == 2000
+
+
+def test_max_description_chars_parameter_custom():
+    """Test that max_description_chars parameter can be set to custom value"""
+    prompt_model = DummyLM([{"proposed_instruction": "instruction"}] * 10)
+    program = Predict("question -> answer")
+    trainset = []
+ 
+    proposer = GroundedProposer(
+        prompt_model=prompt_model, 
+        program=program, 
+        trainset=trainset, 
+        verbose=False,
+        max_description_chars=1000
+    )
+    assert proposer.max_description_chars == 1000
+
+
+def test_max_description_chars_parameter_none():
+    """Test that max_description_chars parameter can be set to None to disable truncation"""
+    prompt_model = DummyLM([{"proposed_instruction": "instruction"}] * 10)
+    program = Predict("question -> answer")
+    trainset = []
+ 
+    proposer = GroundedProposer(
+        prompt_model=prompt_model, 
+        program=program, 
+        trainset=trainset, 
+        verbose=False,
+        max_description_chars=None
+    )
+    assert proposer.max_description_chars is None
+
+
+def test_description_truncation_at_sentence_boundary():
+    """Test that descriptions over the cap are truncated at nearest sentence boundary with ellipsis"""
+    # Create a long description that's over 2000 chars
+    long_description = "This is the first sentence. " * 100  # ~2500 chars
+ 
+    prompt_model = DummyLM([{"proposed_instruction": "instruction"}] * 10)
+    program = Predict("question -> answer")
+    trainset = []
+ 
+    proposer = GroundedProposer(
+        prompt_model=prompt_model, 
+        program=program, 
+        trainset=trainset, 
+        verbose=False,
+        max_description_chars=2000
+    )
+ 
+    # Test the truncation logic directly
+    truncated = proposer._truncate_description_at_sentence_boundary(long_description, 2000)
+ 
+    # Should be under 2000 chars
+    assert len(truncated) < 2000
+    # Should end with ellipsis
+    assert truncated.endswith("…")
+    # Should end at a sentence boundary (period + space)
+    assert truncated.rstrip("…").endswith(". ")
+
+
+def test_description_no_truncation_when_under_cap():
+    """Test that descriptions under the cap are left unchanged"""
+    short_description = "This is a short description. It has only two sentences."
+ 
+    prompt_model = DummyLM([{"proposed_instruction": "instruction"}] * 10)
+    program = Predict("question -> answer")
+    trainset = []
+ 
+    proposer = GroundedProposer(
+        prompt_model=prompt_model, 
+        program=program, 
+        trainset=trainset, 
+        verbose=False,
+        max_description_chars=2000
+    )
+ 
+    # Test the truncation logic directly
+    result = proposer._truncate_description_at_sentence_boundary(short_description, 2000)
+ 
+    # Should be unchanged
+    assert result == short_description
+
+
+def test_description_truncation_disabled_when_none():
+    """Test that when max_description_chars is None, descriptions are not truncated"""
+    long_description = "This is the first sentence. " * 100  # ~2500 chars
+ 
+    prompt_model = DummyLM([{"proposed_instruction": "instruction"}] * 10)
+    program = Predict("question -> answer")
+    trainset = []
+ 
+    proposer = GroundedProposer(
+        prompt_model=prompt_model, 
+        program=program, 
+        trainset=trainset, 
+        verbose=False,
+        max_description_chars=None
+    )
+ 
+    # Test the truncation logic directly
+    result = proposer._truncate_description_at_sentence_boundary(long_description, None)
+ 
+    # Should be unchanged
+    assert result == long_description

diff --git a/tests/propose/test_grounded_proposer.py b/tests/propose/test_grounded_proposer.py
index 252afe8a..f43ee891 100644
--- a/tests/propose/test_grounded_proposer.py
+++ b/tests/propose/test_grounded_proposer.py
@@ -5,6 +5,176 @@ from dspy.predict import Predict
 from dspy.propose.grounded_proposer import GroundedProposer
 from dspy.utils.dummies import DummyLM
 
+def test_tip_weights_constructor_parameter():
+    """Test that tip_weights parameter is properly added to constructor"""
+    prompt_model = DummyLM([{"proposed_instruction": "instruction"}])
+    program = Predict("question -> answer")
+    trainset = []
+ 
+    # Test with None (default behavior)
+    proposer = GroundedProposer(
+        prompt_model=prompt_model, 
+        program=program, 
+        trainset=trainset, 
+        tip_weights=None
+    )
+    assert hasattr(proposer, 'tip_weights')
+    assert proposer.tip_weights is None
+ 
+    # Test with specific weights
+    weights = {"simple": 0.8, "description": 0.2}
+    proposer = GroundedProposer(
+        prompt_model=prompt_model, 
+        program=program, 
+        trainset=trainset, 
+        tip_weights=weights
+    )
+    assert proposer.tip_weights == weights
+
+import random
+def test_uniform_tip_selection_when_weights_none():
+    """Test that when tip_weights is None, uniform selection is preserved"""
+    prompt_model = DummyLM([{"proposed_instruction": "instruction"}])
+    program = Predict("question -> answer")
+    trainset = []
+ 
+    # Use fixed seed for deterministic testing
+    rng = random.Random(42)
+    proposer = GroundedProposer(
+        prompt_model=prompt_model, 
+        program=program, 
+        trainset=trainset, 
+        tip_weights=None,
+        rng=rng,
+        set_tip_randomly=True
+    )
+ 
+    # Test the weighted selection method directly
+    tip_counts = {}
+    for _ in range(100):
+        selected_tip = proposer._select_tip_with_weights()
+        tip_counts[selected_tip] = tip_counts.get(selected_tip, 0) + 1
+ 
+    # Verify that all tips are selected (uniform distribution)
+    assert len(tip_counts) > 1  # Multiple tips should be selected
+    # With 100 samples, we should see multiple tips selected
+
+
+def test_weighted_tip_selection_with_fixed_seed():
+    """Test weighted tip selection with fixed seed for deterministic results"""
+    prompt_model = DummyLM([{"proposed_instruction": "instruction"}])
+    program = Predict("question -> answer")
+    trainset = []
+ 
+    weights = {"simple": 8.0, "description": 2.0}
+    rng = random.Random(42)
+ 
+    proposer = GroundedProposer(
+        prompt_model=prompt_model, 
+        program=program, 
+        trainset=trainset, 
+        tip_weights=weights,
+        rng=rng,
+        set_tip_randomly=True
+    )
+ 
+    # Test the weighted selection method directly
+    tip_counts = {}
+    for _ in range(100):
+        selected_tip = proposer._select_tip_with_weights()
+        tip_counts[selected_tip] = tip_counts.get(selected_tip, 0) + 1
+ 
+    # Verify that weighted selection works
+    assert len(tip_counts) > 1  # Multiple tips should be selected
+    # With weights {"simple": 8.0, "description": 2.0}, "simple" should be selected more often
+    assert tip_counts["simple"] > tip_counts["description"]
+
+
+def test_missing_weight_key_fallback():
+    """Test that missing weight keys default to 1.0"""
+    prompt_model = DummyLM([{"proposed_instruction": "instruction"}])
+    program = Predict("question -> answer")
+    trainset = []
+ 
+    # Provide weights for only some tips
+    weights = {"simple": 0.8, "description": 0.2}
+    # Note: "creative", "high_stakes", "persona", "none" are missing from weights
+ 
+    proposer = GroundedProposer(
+        prompt_model=prompt_model, 
+        program=program, 
+        trainset=trainset, 
+        tip_weights=weights,
+        set_tip_randomly=True
+    )
+ 
+    # Test that the method handles missing keys gracefully
+    selected_tip = proposer._select_tip_with_weights()
+    assert selected_tip in ["none", "creative", "simple", "description", "high_stakes", "persona"]
+
+
+def test_weighted_selection_preserves_determinism():
+    """Test that weighted selection with same seed produces same results"""
+    prompt_model = DummyLM([{"proposed_instruction": "instruction"}])
+    program = Predict("question -> answer")
+    trainset = []
+ 
+    weights = {"simple": 0.8, "description": 0.2}
+ 
+    # Test with same seed multiple times
+    results = []
+    for seed in [42, 42, 42]:  # Same seed repeated
+        rng = random.Random(seed)
+        proposer = GroundedProposer(
+            prompt_model=prompt_model, 
+            program=program, 
+            trainset=trainset, 
+            tip_weights=weights,
+            rng=rng,
+            set_tip_randomly=True
+        )
+ 
+        # Test multiple selections to verify determinism
+        selections = []
+        for _ in range(10):
+            selections.append(proposer._select_tip_with_weights())
+        results.append(selections)
+ 
+    # All results should be identical with the same seed
+    assert results[0] == results[1] == results[2]
+
+
+def test_weighted_selection_different_seeds_produce_different_results():
+    """Test that different seeds produce different results"""
+    prompt_model = DummyLM([{"proposed_instruction": "instruction"}])
+    program = Predict("question -> answer")
+    trainset = []
+ 
+    weights = {"simple": 0.8, "description": 0.2}
+ 
+    # Test with different seeds
+    results = []
+    for seed in [42, 123, 456]:  # Different seeds
+        rng = random.Random(seed)
+        proposer = GroundedProposer(
+            prompt_model=prompt_model, 
+            program=program, 
+            trainset=trainset, 
+            tip_weights=weights,
+            rng=rng,
+            set_tip_randomly=True
+        )
+ 
+        # Test multiple selections
+        selections = []
+        for _ in range(10):
+            selections.append(proposer._select_tip_with_weights())
+        results.append(selections)
+ 
+    # Different seeds should produce different results (with high probability)
+    # Note: This test might occasionally fail due to randomness, but it's very unlikely
+    assert not (results[0] == results[1] == results[2])
+
 
 @pytest.mark.parametrize(
     "demo_candidates",

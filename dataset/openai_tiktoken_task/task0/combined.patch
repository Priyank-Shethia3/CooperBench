diff --git a/tiktoken/core.py b/tiktoken/core.py
index 6bc9736..a15c50c 100644
--- a/tiktoken/core.py
+++ b/tiktoken/core.py
@@ -2,7 +2,18 @@ from __future__ import annotations
 
 import functools
 from concurrent.futures import ThreadPoolExecutor
-from typing import TYPE_CHECKING, AbstractSet, Collection, Literal, NoReturn, Sequence
+from typing import (
+    TYPE_CHECKING,
+    AbstractSet,
+    Collection,
+    Literal,
+    NoReturn,
+    Sequence,
+    Callable,
+    Dict,
+    Tuple,
+    Union,
+)
 
 import regex
 
@@ -76,13 +87,158 @@ class Encoding:
             text = text.encode("utf-16", "surrogatepass").decode("utf-16", "replace")
             return self._core_bpe.encode_ordinary(text)
 
+    def _apply_transformers(
+        self,
+        text: str,
+        transformers: Sequence[Callable[[str], str]] | None,
+    ) -> str:
+        if not transformers:
+            return text
+
+        for transformer in transformers:
+            text = transformer(text)
+        return text
+
+    def _encode_text(
+        self,
+        text: str,
+        allowed_special: AbstractSet[str],
+    ) -> list[int]:
+        try:
+            return self._core_bpe.encode(text, allowed_special)
+        except UnicodeEncodeError:
+            text = text.encode("utf-16", "surrogatepass").decode("utf-16", "replace")
+            return self._core_bpe.encode(text, allowed_special)
+
+    def _filter_tokens(
+        self,
+        tokens: list[int],
+        filter_tokens: AbstractSet[int] | None,
+        positions: list[int] | None = None,
+    ) -> Tuple[list[int], list[int] | None]:
+        if not filter_tokens:
+            return tokens, positions
+
+        filtered_tokens: list[int] = []
+        filtered_positions: list[int] | None = [] if positions is not None else None
+        for idx, token in enumerate(tokens):
+            if token in filter_tokens:
+                continue
+            filtered_tokens.append(token)
+            if filtered_positions is not None and positions is not None:
+                filtered_positions.append(positions[idx])
+
+        return filtered_tokens, filtered_positions
+
+    def _compress_tokens(
+        self,
+        tokens: list[int],
+        positions: list[int] | None = None,
+    ) -> Tuple[list[int], list[int] | None]:
+        if not tokens:
+            return tokens, [] if positions is not None else None
+
+        compressed = [tokens[0]]
+        compressed_positions: list[int] | None = None
+        if positions is not None:
+            compressed_positions = [positions[0]]
+
+        for idx in range(1, len(tokens)):
+            token = tokens[idx]
+            if token == compressed[-1]:
+                continue
+            compressed.append(token)
+            if compressed_positions is not None and positions is not None:
+                compressed_positions.append(positions[idx])
+
+        return compressed, compressed_positions
+
+    def _count_token_frequency(self, tokens: list[int]) -> Dict[int, int]:
+        freq: Dict[int, int] = {}
+        for token in tokens:
+            freq[token] = freq.get(token, 0) + 1
+        return freq
+
+    def _compute_repeated_patterns(self, tokens: list[int]) -> Dict[Tuple[int, int], int]:
+        pair_counts: Dict[Tuple[int, int], int] = {}
+        for idx in range(len(tokens) - 1):
+            pair = (tokens[idx], tokens[idx + 1])
+            pair_counts[pair] = pair_counts.get(pair, 0) + 1
+
+        if len(pair_counts) > 3:
+            pair_counts = dict(sorted(pair_counts.items(), key=lambda item: item[1], reverse=True)[:3])
+        return pair_counts
+
+    def _compute_token_positions(
+        self,
+        text: str,
+        allowed_special: AbstractSet[str],
+    ) -> Tuple[list[int], list[int]]:
+        tokens = self._encode_text(text, allowed_special)
+        positions: list[int] = []
+        search_start = 0
+
+        for token in tokens:
+            token_bytes = self.decode_single_token_bytes(token)
+            piece = token_bytes.decode("utf-8", errors="replace")
+            if piece:
+                pos = text.find(piece, search_start)
+                if pos == -1:
+                    pos = search_start
+            else:
+                pos = search_start
+
+            positions.append(pos)
+            search_start = max(pos + len(piece), search_start)
+
+        return tokens, positions
+
+    def _apply_validators(
+        self,
+        tokens: list[int],
+        validators: Sequence[Callable[[list[int]], object]],
+    ) -> None:
+        for index, validator in enumerate(validators):
+            result = validator(tokens)
+            if result is False:
+                raise ValueError(f"Token validation failed for validator at index {index}")
+
+    def _split_into_chunks(self, text: str, chunk_size: int) -> list[str]:
+        return [text[i : i + chunk_size] for i in range(0, len(text), chunk_size)]
+
+    def _create_cache_key(
+        self,
+        text: str,
+        allowed_special: AbstractSet[str],
+        disallowed_special: AbstractSet[str],
+    ) -> tuple:
+        allowed_key: Union[str, frozenset[str]] = (
+            "all" if allowed_special == self.special_tokens_set else frozenset(allowed_special)
+        )
+        disallowed_key: Union[str, frozenset[str]]
+        if disallowed_special == self.special_tokens_set - allowed_special:
+            disallowed_key = "all"
+        else:
+            disallowed_key = frozenset(disallowed_special)
+        return (text, allowed_key, disallowed_key)
+
     def encode(
         self,
         text: str,
         *,
         allowed_special: Literal["all"] | AbstractSet[str] = set(),  # noqa: B006
         disallowed_special: Literal["all"] | Collection[str] = "all",
-    ) -> list[int]:
+        max_tokens: int | None = None,
+        return_positions: bool = False,
+        return_frequency: bool = False,
+        chunk_size: int | None = None,
+        filter_tokens: list[int] | None = None,
+        transformers: Sequence[Callable[[str], str]] | None = None,
+        validators: Sequence[Callable[[list[int]], object]] | None = None,
+        return_repeated_pattern: bool = False,
+        compression: bool = False,
+        use_cache: bool = False,
+    ) -> Union[list[int], list[list[int]], tuple[list[int], object]]:
         """Encodes a string into tokens.
 
         Special tokens are artificial tokens used to unlock capabilities from a model,
@@ -110,27 +266,121 @@ class Encoding:
         [27, 91, 437, 1659, 5239, 91, 29]
         ```
         """
+
+        if max_tokens is not None and max_tokens <= 0:
+            raise ValueError(f"max_tokens must be a positive integer, got {max_tokens}")
+
+        transformers = tuple(transformers) if transformers else tuple()
+        validators = tuple(validators) if validators else tuple()
+        filter_tokens = list(filter_tokens) if filter_tokens else []
+        filter_set = set(filter_tokens)
+
+        text = self._apply_transformers(text, transformers)
+
         if allowed_special == "all":
-            allowed_special = self.special_tokens_set
+            allowed_special_set = self.special_tokens_set
+        else:
+            allowed_special_set = set(allowed_special)
+
         if disallowed_special == "all":
-            disallowed_special = self.special_tokens_set - allowed_special
-        if disallowed_special:
-            if not isinstance(disallowed_special, frozenset):
-                disallowed_special = frozenset(disallowed_special)
-            if match := _special_token_regex(disallowed_special).search(text):
+            disallowed_special_set: AbstractSet[str] = self.special_tokens_set - allowed_special_set
+        else:
+            disallowed_special_set = disallowed_special
+
+        if disallowed_special_set:
+            if not isinstance(disallowed_special_set, frozenset):
+                disallowed_special_set = frozenset(disallowed_special_set)
+            if match := _special_token_regex(disallowed_special_set).search(text):
                 raise_disallowed_special_token(match.group())
 
-        try:
-            return self._core_bpe.encode(text, allowed_special)
-        except UnicodeEncodeError:
-            # BPE operates on bytes, but the regex operates on unicode. If we pass a str that is
-            # invalid UTF-8 to Rust, it will rightfully complain. Here we do a quick and dirty
-            # fixup for any surrogate pairs that may have sneaked their way into the text.
-            # Technically, this introduces a place where encode + decode doesn't roundtrip a Python
-            # string, but given that this is input we want to support, maybe that's okay.
-            # Also we use errors="replace" to handle weird things like lone surrogates.
-            text = text.encode("utf-16", "surrogatepass").decode("utf-16", "replace")
-            return self._core_bpe.encode(text, allowed_special)
+        can_use_cache = (
+            use_cache
+            and chunk_size is None
+            and not filter_set
+            and not validators
+            and not transformers
+            and not compression
+            and not return_positions
+            and not return_frequency
+            and not return_repeated_pattern
+            and max_tokens is None
+        )
+
+        cache_key = None
+        cache = None
+        if can_use_cache:
+            cache = getattr(self, "cache", None)
+            if cache is None or not isinstance(cache, dict):
+                cache = self.cache = {}
+            cache_key = self._create_cache_key(text, allowed_special_set, set(disallowed_special_set))
+            cached_result = cache.get(cache_key)
+            if cached_result is not None:
+                return cached_result.copy()
+
+        if chunk_size is not None:
+            if chunk_size <= 0:
+                raise ValueError("chunk_size must be a positive integer")
+
+            encoded_chunks: list[list[int]] = []
+            total_tokens = 0
+            for chunk_text in self._split_into_chunks(text, chunk_size):
+                chunk_tokens = self._encode_text(chunk_text, allowed_special_set)
+                chunk_tokens, _ = self._filter_tokens(chunk_tokens, filter_set, None)
+                chunk_tokens, _ = self._compress_tokens(chunk_tokens, None) if compression else (chunk_tokens, None)
+
+                if validators:
+                    self._apply_validators(chunk_tokens, validators)
+
+                total_tokens += len(chunk_tokens)
+                encoded_chunks.append(chunk_tokens)
+
+            if max_tokens is not None and total_tokens > max_tokens:
+                raise ValueError(
+                    f"Token count ({total_tokens}) exceeds maximum allowed tokens ({max_tokens})"
+                )
+
+            return encoded_chunks
+
+        tokens = self._encode_text(text, allowed_special_set)
+
+        positions: list[int] | None = None
+        if return_positions:
+            tokens, positions = self._compute_token_positions(text, allowed_special_set)
+
+        tokens, positions = self._filter_tokens(tokens, filter_set, positions)
+
+        if compression:
+            tokens, positions = self._compress_tokens(tokens, positions)
+
+        if validators:
+            self._apply_validators(tokens, validators)
+
+        if max_tokens is not None and len(tokens) > max_tokens:
+            raise ValueError(
+                f"Token count ({len(tokens)}) exceeds maximum allowed tokens ({max_tokens})"
+            )
+
+        metadata: Dict[str, object] = {}
+
+        if return_positions:
+            metadata["positions"] = positions or []
+
+        if return_frequency:
+            metadata["frequency"] = self._count_token_frequency(tokens)
+
+        if return_repeated_pattern:
+            metadata["pattern"] = self._compute_repeated_patterns(tokens)
+
+        if can_use_cache and cache is not None and cache_key is not None:
+            cache[cache_key] = tokens.copy()
+
+        if not metadata:
+            return tokens
+
+        if len(metadata) == 1:
+            return tokens, next(iter(metadata.values()))
+
+        return tokens, metadata
 
     def encode_to_numpy(
         self,

diff --git a/tiktoken/core.py b/tiktoken/core.py
index 6bc9736..1b369a8 100644
--- a/tiktoken/core.py
+++ b/tiktoken/core.py
@@ -76,13 +76,98 @@ class Encoding:
             text = text.encode("utf-16", "surrogatepass").decode("utf-16", "replace")
             return self._core_bpe.encode_ordinary(text)
 
+    def _track_token_positions(
+        self,
+        text: str,
+        *,
+        allowed_special: AbstractSet[str] = set(),
+    ) -> Tuple[List[int], List[int]]:
+        """Encodes a string into tokens while tracking the character positions.
+ 
+        Returns a tuple of (tokens, positions) where positions are the character 
+        indices in the original text where each token starts.
+        """
+        tokens = []
+        positions = []
+        current_position = 0
+ 
+        # Compile regex pattern for finding special tokens and text chunks
+        pattern = regex.compile(self._pat_str)
+ 
+        # First handle special tokens
+        special_tokens_pattern = None
+        if allowed_special:
+            special_tokens_regex = "|".join(regex.escape(token) for token in allowed_special)
+            special_tokens_pattern = regex.compile(f"({special_tokens_regex})")
+ 
+        # Process the text chunk by chunk
+        remaining_text = text
+        while remaining_text:
+            # Check for special tokens first
+            special_match = None
+            if special_tokens_pattern:
+                special_match = special_tokens_pattern.match(remaining_text)
+ 
+            if special_match:
+                # Found a special token
+                special_token = special_match.group(0)
+                token_value = self._special_tokens[special_token]
+                tokens.append(token_value)
+                positions.append(current_position)
+ 
+                # Move past this special token
+                special_token_len = len(special_token)
+                remaining_text = remaining_text[special_token_len:]
+                current_position += special_token_len
+            else:
+                # Process next regular chunk
+                match = pattern.match(remaining_text)
+                if not match:
+                    # If no match, just take the next character
+                    chunk = remaining_text[0]
+                    chunk_len = 1
+                else:
+                    chunk = match.group(0)
+                    chunk_len = len(chunk)
+ 
+                # Encode this chunk into tokens
+                try:
+                    chunk_bytes = chunk.encode("utf-8")
+                    chunk_tokens = self._encode_single_piece(chunk_bytes)
+ 
+                    # Add all tokens from this chunk with their positions
+                    if chunk_tokens:
+                        # For multi-token chunks, we need to calculate positions
+                        # by decoding each token to find its byte length
+                        byte_offset = 0
+                        for token in chunk_tokens:
+                            token_bytes = self.decode_single_token_bytes(token)
+ 
+                            # Find the character position by counting UTF-8 characters
+                            char_pos = len(chunk_bytes[:byte_offset].decode("utf-8", errors="replace"))
+                            tokens.append(token)
+                            positions.append(current_position + char_pos)
+ 
+                            # Move to next token's position
+                            byte_offset += len(token_bytes)
+                except Exception:
+                    # If encoding fails, skip this chunk
+                    pass
+ 
+                # Move past this chunk
+                remaining_text = remaining_text[chunk_len:]
+                current_position += chunk_len
+ 
+        return tokens, positions
+
     def encode(
         self,
         text: str,
         *,
         allowed_special: Literal["all"] | AbstractSet[str] = set(),  # noqa: B006
         disallowed_special: Literal["all"] | Collection[str] = "all",
-    ) -> list[int]:
+        return_positions: bool = False,
+    ) -> list[int] | tuple[list[int], list[int]]:
         """Encodes a string into tokens.
 
         Special tokens are artificial tokens used to unlock capabilities from a model,
@@ -96,6 +181,8 @@ class Encoding:
           cause all text corresponding to special tokens to be encoded as natural text.
         - Setting `allowed_special` to "all" will cause this function to treat all text
           corresponding to special tokens to be encoded as special tokens.
+        - Setting `return_positions` to True will return a tuple of (tokens, positions) where
+          positions are the character indices in the original text where each token starts.
 
         ```
         >>> enc.encode("hello world")
@@ -108,6 +195,8 @@ class Encoding:
         # Raises ValueError
         >>> enc.encode("<|endoftext|>", disallowed_special=())
         [27, 91, 437, 1659, 5239, 91, 29]
+        >>> enc.encode("hello world", return_positions=True)
+        ([31373, 995], [0, 5])
         ```
         """
         if allowed_special == "all":
@@ -120,6 +209,16 @@ class Encoding:
             if match := _special_token_regex(disallowed_special).search(text):
                 raise_disallowed_special_token(match.group())
 
+        # If we need to track positions, use our custom implementation
+        if return_positions:
+            try:
+                return self._track_token_positions(text, allowed_special=allowed_special)
+            except UnicodeEncodeError:
+                # Handle encoding errors similar to the standard encode method
+                text = text.encode("utf-16", "surrogatepass").decode("utf-16", "replace")
+                return self._track_token_positions(text, allowed_special=allowed_special)
+ 
+        # Otherwise, use the standard implementation
         try:
             return self._core_bpe.encode(text, allowed_special)
         except UnicodeEncodeError:

diff --git a/outlines/backends/xgrammar.py b/outlines/backends/xgrammar.py
index fcb1672d..650c33a2 100644
--- a/outlines/backends/xgrammar.py
+++ b/outlines/backends/xgrammar.py
@@ -1,6 +1,6 @@
 """Backend class for XGrammar."""

-from typing import TYPE_CHECKING
+from typing import TYPE_CHECKING, Optional

 from outlines.backends.base import BaseBackend
 from outlines.models import SteerableModel
@@ -22,18 +22,21 @@ class XGrammarLogitsProcessor(OutlinesLogitsProcessor):

     """

-    def __init__(self, compiled_grammar: str):
+    def __init__(self, compiled_grammar: str, max_batch_size: Optional[int] = None):
         """
         Parameters
         ----------
         compiled_grammar: str
             The compiled grammar to use to create the logits processor.
+        max_batch_size: int, optional
+            Maximum allowed batch size. If None, no limit is enforced.

         """
         import xgrammar as xgr

         self.xgr = xgr
         self.compiled_grammar = compiled_grammar
+        self.max_batch_size = max_batch_size
         self.xgrammar_logits_processor = None
         super().__init__("torch")

@@ -44,6 +47,14 @@ class XGrammarLogitsProcessor(OutlinesLogitsProcessor):
     def process_logits(self, input_ids: TensorType, logits: TensorType) -> TensorType:
         """Bias the logits."""
         if self.xgrammar_logits_processor is None:
+            # Validate batch size if max_batch_size is set
+            if self.max_batch_size is not None:
+                batch_size = self.tensor_adapter.shape(logits)[0]
+                if batch_size > self.max_batch_size:
+                    raise ValueError(
+                        f"Batch size {batch_size} exceeds maximum allowed batch size {self.max_batch_size}"
+                    )
+
             self.xgrammar_logits_processor = self.xgr.contrib.hf.LogitsProcessor(
                 self.compiled_grammar
             )


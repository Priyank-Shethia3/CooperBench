diff --git a/tests/backends/test_xgrammar.py b/tests/backends/test_xgrammar.py
index 1133b9f0..ff146528 100644
--- a/tests/backends/test_xgrammar.py
+++ b/tests/backends/test_xgrammar.py
@@ -8,6 +8,7 @@ from outlines.backends.xgrammar import XGrammarLogitsProcessor
 
 import outlines
 from outlines.backends.xgrammar import XGrammarBackend
+from outlines.processors.tensor_adapters.base import TensorAdapter
 
 try:
     import mlx_lm
@@ -16,6 +17,66 @@ except ImportError:
     HAS_MLX = False
 
 
+class MockCustomTensorAdapter(TensorAdapter):
+    """Mock custom tensor adapter for testing."""
+
+    library_name = "custom_mock"
+
+    def __init__(self):
+        self.method_calls = []
+
+    def shape(self, tensor):
+        self.method_calls.append("shape")
+        return [2, 10]  # Mock shape
+
+    def unsqueeze(self, tensor):
+        self.method_calls.append("unsqueeze")
+        return tensor
+
+    def squeeze(self, tensor):
+        self.method_calls.append("squeeze")
+        return tensor
+
+    def to_list(self, tensor):
+        self.method_calls.append("to_list")
+        return tensor.tolist() if hasattr(tensor, 'tolist') else list(tensor)
+
+    def to_scalar(self, tensor):
+        self.method_calls.append("to_scalar")
+        return tensor.item() if hasattr(tensor, 'item') else tensor
+
+    def full_like(self, tensor, fill_value):
+        self.method_calls.append("full_like")
+        import torch
+        return torch.full_like(tensor, fill_value)
+
+    def concatenate(self, tensors):
+        self.method_calls.append("concatenate")
+        import torch
+        return torch.cat(tensors, dim=0)
+
+    def get_device(self, tensor):
+        self.method_calls.append("get_device")
+        return "cpu"
+
+    def to_device(self, tensor, device):
+        self.method_calls.append("to_device")
+        return tensor
+
+    def boolean_ones_like(self, tensor):
+        self.method_calls.append("boolean_ones_like")
+        import torch
+        return torch.ones_like(tensor, dtype=torch.bool)
+
+    def apply_mask(self, tensor, mask, value):
+        self.method_calls.append("apply_mask")
+        return tensor.masked_fill(mask, value)
+
+    def argsort_descending(self, tensor):
+        self.method_calls.append("argsort_descending")
+        return tensor.argsort(dim=-1, descending=True)
+
+
 @pytest.fixture
 def model_transformers():
     return outlines.from_transformers(
@@ -61,7 +122,6 @@ answer ::= "yes" | "no"
 def test_xgrammar_backend(model_transformers, json_schema, regex, cfg):
     # initialization
     backend = XGrammarBackend(model_transformers)
-    assert isinstance(backend.grammar_compiler, GrammarCompiler)
 
     # json schema
     processor = backend.get_json_schema_logits_processor(json_schema)
@@ -93,3 +153,183 @@ def test_xgrammar_backend_invalid_model(model_llamacpp):
         match="The xgrammar backend only supports Transformers models",
     ):
         XGrammarBackend(model_llamacpp)
+
+
+# Custom tensor adapter tests
+def test_xgrammar_logits_processor_custom_adapter():
+    """Test XGrammarLogitsProcessor with custom tensor adapter."""
+    compiled_grammar = "mock_grammar"
+    custom_adapter = MockCustomTensorAdapter()
+
+    processor = XGrammarLogitsProcessor(
+        compiled_grammar,
+        tensor_library_name="custom",
+        custom_adapter=custom_adapter
+    )
+
+    # Test that the processor was created successfully with custom adapter
+    assert isinstance(processor, XGrammarLogitsProcessor)
+
+
+def test_xgrammar_logits_processor_custom_missing_adapter():
+    """Test error when custom tensor library specified without adapter."""
+    compiled_grammar = "mock_grammar"
+
+    with pytest.raises(ValueError, match="custom_adapter must be provided"):
+        XGrammarLogitsProcessor(
+            compiled_grammar,
+            tensor_library_name="custom",
+            custom_adapter=None
+        )
+
+
+def test_xgrammar_logits_processor_custom_adapter_with_wrong_library():
+    """Test error when custom adapter provided with non-custom library."""
+    compiled_grammar = "mock_grammar"
+    custom_adapter = MockCustomTensorAdapter()
+
+    with pytest.raises(ValueError, match="custom_adapter should only be provided"):
+        XGrammarLogitsProcessor(
+            compiled_grammar,
+            tensor_library_name="torch",
+            custom_adapter=custom_adapter
+        )
+
+
+def test_xgrammar_logits_processor_default_torch():
+    """Test default tensor library is torch."""
+    compiled_grammar = "mock_grammar"
+
+    processor = XGrammarLogitsProcessor(compiled_grammar)
+
+    # Test that processor was created successfully with default torch library
+    assert isinstance(processor, XGrammarLogitsProcessor)
+
+
+@pytest.mark.skipif(not HAS_MLX, reason="MLX not available")
+def test_xgrammar_logits_processor_mlx_library():
+    """Test XGrammarLogitsProcessor with mlx tensor library."""
+    compiled_grammar = "mock_grammar"
+
+    processor = XGrammarLogitsProcessor(
+        compiled_grammar,
+        tensor_library_name="mlx"
+    )
+
+    # Test that processor was created successfully with mlx library
+    assert isinstance(processor, XGrammarLogitsProcessor)
+
+
+def test_xgrammar_backend_custom_adapter_json_schema(model_transformers, json_schema):
+    """Test XGrammarBackend with custom adapter for JSON schema."""
+    backend = XGrammarBackend(model_transformers)
+    custom_adapter = MockCustomTensorAdapter()
+
+    processor = backend.get_json_schema_logits_processor(
+        json_schema,
+        tensor_library_name="custom",
+        custom_adapter=custom_adapter
+    )
+
+    assert isinstance(processor, XGrammarLogitsProcessor)
+    # Test that processor accepts the custom adapter without error
+    # The actual behavior is tested through functional tests
+
+
+def test_xgrammar_backend_custom_adapter_regex(model_transformers, regex):
+    """Test XGrammarBackend with custom adapter for regex."""
+    backend = XGrammarBackend(model_transformers)
+    custom_adapter = MockCustomTensorAdapter()
+
+    processor = backend.get_regex_logits_processor(
+        regex,
+        tensor_library_name="custom",
+        custom_adapter=custom_adapter
+    )
+
+    assert isinstance(processor, XGrammarLogitsProcessor)
+    # Test that processor accepts the custom adapter without error
+    # The actual behavior is tested through functional tests
+
+
+def test_xgrammar_backend_custom_adapter_cfg(model_transformers, cfg):
+    """Test XGrammarBackend with custom adapter for CFG."""
+    backend = XGrammarBackend(model_transformers)
+    custom_adapter = MockCustomTensorAdapter()
+
+    processor = backend.get_cfg_logits_processor(
+        cfg,
+        tensor_library_name="custom",
+        custom_adapter=custom_adapter
+    )
+
+    assert isinstance(processor, XGrammarLogitsProcessor)
+    # Test that processor accepts the custom adapter without error
+    # The actual behavior is tested through functional tests
+
+
+@pytest.mark.skipif(not HAS_MLX, reason="MLX not available")
+def test_xgrammar_backend_different_tensor_libraries(model_transformers, json_schema):
+    """Test XGrammarBackend with different tensor libraries."""
+    backend = XGrammarBackend(model_transformers)
+
+    # Test torch (default)
+    processor_torch = backend.get_json_schema_logits_processor(json_schema)
+    assert isinstance(processor_torch, XGrammarLogitsProcessor)
+
+    # Test mlx
+    processor_mlx = backend.get_json_schema_logits_processor(
+        json_schema, tensor_library_name="mlx"
+    )
+    assert isinstance(processor_mlx, XGrammarLogitsProcessor)
+
+    # Test numpy
+    processor_numpy = backend.get_json_schema_logits_processor(
+        json_schema, tensor_library_name="numpy"
+    )
+    assert isinstance(processor_numpy, XGrammarLogitsProcessor)
+
+
+def test_custom_adapter_interface_compliance():
+    """Test that MockCustomTensorAdapter implements required interface."""
+    adapter = MockCustomTensorAdapter()
+
+    # Test all required methods exist
+    assert hasattr(adapter, 'shape')
+    assert hasattr(adapter, 'unsqueeze')
+    assert hasattr(adapter, 'squeeze')
+    assert hasattr(adapter, 'to_list')
+    assert hasattr(adapter, 'to_scalar')
+    assert hasattr(adapter, 'full_like')
+    assert hasattr(adapter, 'concatenate')
+    assert hasattr(adapter, 'get_device')
+    assert hasattr(adapter, 'to_device')
+    assert hasattr(adapter, 'boolean_ones_like')
+    assert hasattr(adapter, 'apply_mask')
+    assert hasattr(adapter, 'argsort_descending')
+    assert hasattr(adapter, 'library_name')
+
+    # Test library_name
+    assert adapter.library_name == "custom_mock"
+
+
+def test_xgrammar_backend_parameter_validation_errors(model_transformers, json_schema):
+    """Test parameter validation errors for custom adapters."""
+    backend = XGrammarBackend(model_transformers)
+
+    # Test missing custom_adapter with custom library
+    with pytest.raises(ValueError, match="custom_adapter must be provided"):
+        backend.get_json_schema_logits_processor(
+            json_schema,
+            tensor_library_name="custom",
+            custom_adapter=None
+        )
+
+    # Test custom_adapter with non-custom library
+    custom_adapter = MockCustomTensorAdapter()
+    with pytest.raises(ValueError, match="custom_adapter should only be provided"):
+        backend.get_json_schema_logits_processor(
+            json_schema,
+            tensor_library_name="torch",
+            custom_adapter=custom_adapter
+        )

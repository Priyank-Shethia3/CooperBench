diff --git a/tests/backends/test_xgrammar.py b/tests/backends/test_xgrammar.py
index 1133b9f0..a6bb4ac7 100644
--- a/tests/backends/test_xgrammar.py
+++ b/tests/backends/test_xgrammar.py
@@ -1,4 +1,6 @@
 import pytest
+import torch
+from unittest.mock import Mock, patch
 
 import llama_cpp
 import transformers
@@ -8,6 +10,7 @@ from outlines.backends.xgrammar import XGrammarLogitsProcessor
 
 import outlines
 from outlines.backends.xgrammar import XGrammarBackend
+from outlines.processors.base_logits_processor import OutlinesLogitsProcessor
 
 try:
     import mlx_lm
@@ -16,6 +19,23 @@ except ImportError:
     HAS_MLX = False
 
 
+class MockFallbackProcessor(OutlinesLogitsProcessor):
+    """Mock fallback processor for testing."""
+
+    def __init__(self):
+        super().__init__("torch")
+        self.reset_called = False
+        self.process_called = False
+
+    def reset(self):
+        self.reset_called = True
+
+    def process_logits(self, input_ids, logits):
+        self.process_called = True
+        # Return unmodified logits
+        return logits
+
+
 @pytest.fixture
 def model_transformers():
     return outlines.from_transformers(
@@ -93,3 +113,180 @@ def test_xgrammar_backend_invalid_model(model_llamacpp):
         match="The xgrammar backend only supports Transformers models",
     ):
         XGrammarBackend(model_llamacpp)
+
+
+def test_xgrammar_processor_with_fallback():
+    """Test XGrammarLogitsProcessor with fallback processor."""
+    compiled_grammar = "test_grammar"
+    fallback_processor = MockFallbackProcessor()
+
+    processor = XGrammarLogitsProcessor(compiled_grammar, fallback_processor)
+
+    # Test that constructor accepts fallback without error
+    assert processor is not None
+
+    # Test that fallback works when needed (behavioral test)
+    input_ids = torch.tensor([[1, 2, 3]])
+    logits = torch.tensor([[0.1, 0.2, 0.3, 0.4]])
+
+    with patch.object(processor, 'xgr') as mock_xgr:
+        mock_xgr.contrib.hf.LogitsProcessor.side_effect = Exception("Grammar error")
+        result = processor.process_logits(input_ids, logits)
+        # Verify fallback was actually used by checking the result
+        assert fallback_processor.process_called
+        assert torch.equal(result, logits)
+
+
+def test_xgrammar_processor_fallback_reset():
+    """Test that reset method calls fallback processor reset."""
+    compiled_grammar = "test_grammar"
+    fallback_processor = MockFallbackProcessor()
+
+    processor = XGrammarLogitsProcessor(compiled_grammar, fallback_processor)
+    processor.reset()
+
+    # Test that fallback processor reset was called
+    assert fallback_processor.reset_called
+
+
+def test_xgrammar_processor_fallback_on_error():
+    """Test that fallback processor is used when primary processor fails."""
+    compiled_grammar = "test_grammar"
+    fallback_processor = MockFallbackProcessor()
+
+    processor = XGrammarLogitsProcessor(compiled_grammar, fallback_processor)
+
+    # Mock input_ids and logits
+    input_ids = torch.tensor([[1, 2, 3]])
+    logits = torch.tensor([[0.1, 0.2, 0.3, 0.4]])
+
+    # Mock xgrammar to raise an exception
+    with patch.object(processor, 'xgr') as mock_xgr:
+        mock_xgr.contrib.hf.LogitsProcessor.side_effect = Exception("Grammar error")
+
+        # Process logits should use fallback
+        result = processor.process_logits(input_ids, logits)
+
+        # Verify fallback was used by checking observable behavior
+        assert fallback_processor.process_called
+        assert torch.equal(result, logits)  # MockFallbackProcessor returns unmodified logits
+
+
+def test_xgrammar_processor_fallback_continues_after_error():
+    """Test that once fallback is activated, it continues to be used."""
+    compiled_grammar = "test_grammar"
+    fallback_processor = MockFallbackProcessor()
+
+    processor = XGrammarLogitsProcessor(compiled_grammar, fallback_processor)
+
+    input_ids = torch.tensor([[1, 2, 3]])
+    logits = torch.tensor([[0.1, 0.2, 0.3, 0.4]])
+
+    # First call - trigger fallback activation
+    with patch.object(processor, 'xgr') as mock_xgr:
+        mock_xgr.contrib.hf.LogitsProcessor.side_effect = Exception("Grammar error")
+        result1 = processor.process_logits(input_ids, logits)
+        assert fallback_processor.process_called
+        assert torch.equal(result1, logits)
+
+    # Reset the mock flag for second test
+    fallback_processor.process_called = False
+
+    # Second call - should continue using fallback without triggering primary processor
+    result2 = processor.process_logits(input_ids, logits)
+    assert fallback_processor.process_called
+    assert torch.equal(result2, logits)
+
+
+def test_xgrammar_processor_no_fallback_reraises_error():
+    """Test that errors are re-raised when no fallback processor is provided."""
+    compiled_grammar = "test_grammar"
+
+    processor = XGrammarLogitsProcessor(compiled_grammar)  # No fallback
+
+    input_ids = torch.tensor([[1, 2, 3]])
+    logits = torch.tensor([[0.1, 0.2, 0.3, 0.4]])
+
+    # Mock xgrammar to raise an exception
+    with patch.object(processor, 'xgr') as mock_xgr:
+        mock_xgr.contrib.hf.LogitsProcessor.side_effect = Exception("Grammar error")
+
+        # Should re-raise the exception
+        with pytest.raises(Exception, match="Grammar error"):
+            processor.process_logits(input_ids, logits)
+
+
+def test_xgrammar_backend_with_fallback_processor(model_transformers, json_schema):
+    """Test backend methods accept fallback processor parameter."""
+    backend = XGrammarBackend(model_transformers)
+    fallback_processor = MockFallbackProcessor()
+
+    input_ids = torch.tensor([[1, 2, 3]])
+    logits = torch.tensor([[0.1, 0.2, 0.3, 0.4]])
+
+    # Test JSON schema processor with fallback
+    processor = backend.get_json_schema_logits_processor(json_schema, fallback_processor)
+    assert isinstance(processor, XGrammarLogitsProcessor)
+    # Test that the processor actually uses the fallback when needed
+    with patch.object(processor, 'xgr') as mock_xgr:
+        mock_xgr.contrib.hf.LogitsProcessor.side_effect = Exception("Grammar error")
+        result = processor.process_logits(input_ids, logits)
+        assert fallback_processor.process_called
+
+    # Reset for next test
+    fallback_processor.process_called = False
+
+    # Test regex processor with fallback
+    processor = backend.get_regex_logits_processor(r"[0-9]+", fallback_processor)
+    assert isinstance(processor, XGrammarLogitsProcessor)
+    # Test that the processor actually uses the fallback when needed
+    with patch.object(processor, 'xgr') as mock_xgr:
+        mock_xgr.contrib.hf.LogitsProcessor.side_effect = Exception("Grammar error")
+        result = processor.process_logits(input_ids, logits)
+        assert fallback_processor.process_called
+
+    # Reset for next test
+    fallback_processor.process_called = False
+
+    # Test CFG processor with fallback
+    cfg = 'root ::= "test"'
+    processor = backend.get_cfg_logits_processor(cfg, fallback_processor)
+    assert isinstance(processor, XGrammarLogitsProcessor)
+    # Test that the processor actually uses the fallback when needed
+    with patch.object(processor, 'xgr') as mock_xgr:
+        mock_xgr.contrib.hf.LogitsProcessor.side_effect = Exception("Grammar error")
+        result = processor.process_logits(input_ids, logits)
+        assert fallback_processor.process_called
+
+
+def test_xgrammar_processor_edge_cases():
+    """Test edge cases for fallback processor."""
+    compiled_grammar = "test_grammar"
+
+    # Test with None fallback processor - should re-raise errors
+    processor = XGrammarLogitsProcessor(compiled_grammar, None)
+    input_ids = torch.tensor([[1, 2, 3]])
+    logits = torch.tensor([[0.1, 0.2, 0.3, 0.4]])
+
+    # Test that when no fallback is provided, errors are re-raised
+    with patch.object(processor, 'xgr') as mock_xgr:
+        mock_xgr.contrib.hf.LogitsProcessor.side_effect = Exception("Grammar error")
+        with pytest.raises(Exception, match="Grammar error"):
+            processor.process_logits(input_ids, logits)
+
+    # Test reset with None fallback
+    processor.reset()  # Should not raise error
+
+    # Test empty input handling with fallback
+    fallback_processor = MockFallbackProcessor()
+    processor = XGrammarLogitsProcessor(compiled_grammar, fallback_processor)
+
+    # Test with minimal tensors
+    input_ids = torch.tensor([[1]])
+    logits = torch.tensor([[0.5]])
+
+    with patch.object(processor, 'xgr') as mock_xgr:
+        mock_xgr.contrib.hf.LogitsProcessor.side_effect = Exception("Error")
+        result = processor.process_logits(input_ids, logits)
+        assert torch.equal(result, logits)
+        assert fallback_processor.process_called

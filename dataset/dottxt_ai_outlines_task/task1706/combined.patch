diff --git a/outlines/backends/xgrammar.py b/outlines/backends/xgrammar.py
index fcb1672d..cedc3234 100644
--- a/outlines/backends/xgrammar.py
+++ b/outlines/backends/xgrammar.py
@@ -1,6 +1,10 @@
 """Backend class for XGrammar."""
 
-from typing import TYPE_CHECKING
+import hashlib
+import logging
+import psutil
+import threading
+from typing import TYPE_CHECKING, Callable, Optional, Union
 
 from outlines.backends.base import BaseBackend
 from outlines.models import SteerableModel
@@ -9,57 +13,303 @@ from outlines.processors.base_logits_processor import (
     OutlinesLogitsProcessor,
     TensorType
 )
+from outlines.processors.tensor_adapters.base import TensorAdapter
 
 if TYPE_CHECKING:
     from xgrammar.contrib.hf import LogitsProcessor
 
+logger = logging.getLogger(__name__)
+
 
 class XGrammarLogitsProcessor(OutlinesLogitsProcessor):
-    """Logits processor for XGrammar.
+    """Custom logits processor for XGrammar with multi-tensor support."""
 
-    This class wraps the `xgr.contrib.hf.LogitsProcessor` class and adds
-    a `reset` method to reset the logits processor for a new generation.
+    def __init__(
+        self,
+        compiled_grammar: str,
+        tensor_library_name_or_fallback: Union[str, "OutlinesLogitsProcessor", None] = "torch",
+        temperature: float = 1.0,
+        max_batch_size: Optional[int] = None,
+        custom_adapter: Optional[TensorAdapter] = None,
+        fallback_processor: Optional["OutlinesLogitsProcessor"] = None,
+        track_memory_usage: bool = False,
+        tensor_library_name: Optional[str] = None  # Alias for compatibility with feature 5 tests
+    ):
+        """Initialize with compiled grammar and configuration options."""
+        import xgrammar as xgr
 
-    """
+        # Handle tensor_library_name as keyword argument alias (feature 5 compatibility)
+        if tensor_library_name is not None:
+            tensor_library_name_or_fallback = tensor_library_name
 
-    def __init__(self, compiled_grammar: str):
-        """
-        Parameters
-        ----------
-        compiled_grammar: str
-            The compiled grammar to use to create the logits processor.
+        # Handle backward compatibility: second arg can be tensor_library_name or fallback_processor
+        if isinstance(tensor_library_name_or_fallback, OutlinesLogitsProcessor):
+            actual_tensor_library = "torch"
+            fallback_processor = tensor_library_name_or_fallback
+        elif tensor_library_name_or_fallback is None:
+            actual_tensor_library = "torch"
+        else:
+            actual_tensor_library = str(tensor_library_name_or_fallback)
 
-        """
-        import xgrammar as xgr
+        # Validate custom adapter parameters
+        if actual_tensor_library == "custom" and custom_adapter is None:
+            raise ValueError(
+                "custom_adapter must be provided when tensor_library_name is 'custom'"
+            )
+        if actual_tensor_library != "custom" and custom_adapter is not None:
+            raise ValueError(
+                "custom_adapter should only be provided when tensor_library_name is 'custom'"
+            )
 
         self.xgr = xgr
         self.compiled_grammar = compiled_grammar
+        self.tensor_library_name = actual_tensor_library
+        self.temperature = temperature
+        self.max_batch_size = max_batch_size
+        self.custom_adapter = custom_adapter
+        self.fallback_processor = fallback_processor
+        self.track_memory_usage = track_memory_usage
+
+        # State management
+        self.is_first_token = True
+        self._matchers = None
+        self._bitmask = None
+        self._bias_logits = None
+        self._hf_fallback = None
+        self._using_fallback = False
         self.xgrammar_logits_processor = None
-        super().__init__("torch")
+
+        # Memory tracking
+        self.memory_stats = {
+            "peak_memory_mb": 0.0,
+            "total_calls": 0,
+            "avg_memory_per_call_mb": 0.0,
+            "memory_history": []
+        }
+        self.logger = logging.getLogger(__name__)
+
+        super().__init__(actual_tensor_library)
+
+        if custom_adapter is not None:
+            self.tensor_adapter = custom_adapter
 
     def reset(self):
-        """Reset the logits processor for a new generation."""
+        """Reset processor state for new generation sequence."""
+        self.is_first_token = True
+        self._matchers = None
+        self._bitmask = None
+        self._bias_logits = None
+        self._hf_fallback = None
+        self._using_fallback = False
         self.xgrammar_logits_processor = None
 
+        if self.fallback_processor is not None:
+            self.fallback_processor.reset()
+
+        if self.track_memory_usage:
+            self.memory_stats = {
+                "peak_memory_mb": 0.0,
+                "total_calls": 0,
+                "avg_memory_per_call_mb": 0.0,
+                "memory_history": []
+            }
+
+    def _get_memory_usage(self) -> float:
+        """Get current memory usage in MB."""
+        try:
+            process = psutil.Process()
+            return process.memory_info().rss / 1024 / 1024
+        except Exception:
+            return 0.0
+
+    def _update_memory_stats(self, memory_mb: float):
+        """Update memory statistics."""
+        self.memory_stats["total_calls"] += 1
+        self.memory_stats["memory_history"].append(memory_mb)
+        if memory_mb > self.memory_stats["peak_memory_mb"]:
+            self.memory_stats["peak_memory_mb"] = memory_mb
+        total_memory = sum(self.memory_stats["memory_history"])
+        self.memory_stats["avg_memory_per_call_mb"] = total_memory / self.memory_stats["total_calls"]
+
+    def get_memory_stats(self) -> dict:
+        """Get current memory usage statistics."""
+        return self.memory_stats.copy()
+
+    def _setup(self, batch_size: int, vocab_size: int) -> None:
+        """Initialize matchers, bitmask and bias function on first token."""
+        try:
+            GrammarMatcher = getattr(self.xgr, "GrammarMatcher")
+            self._matchers = [GrammarMatcher(self.compiled_grammar) for _ in range(batch_size)]
+        except Exception:
+            try:
+                self._hf_fallback = self.xgr.contrib.hf.LogitsProcessor(self.compiled_grammar)
+                self.xgrammar_logits_processor = self._hf_fallback
+                return
+            except Exception:
+                # Re-raise the original exception to preserve backward compatibility
+                raise
+
+        lib = self.tensor_library_name
+        if lib == "torch":
+            self._setup_torch(batch_size, vocab_size)
+        elif lib == "mlx":
+            self._setup_mlx(batch_size, vocab_size)
+        elif lib == "custom":
+            self._setup_torch(batch_size, vocab_size)
+        else:
+            raise TypeError(f"Unsupported tensor library for XGrammar: {lib}")
+
+    def _setup_torch(self, batch_size: int, vocab_size: int) -> None:
+        """Torch-specific setup for bitmask and bias function."""
+        allocate = fill = apply_inplace = None
+        for path in ("xgrammar.torch", "xgrammar.kernels.torch", "xgr.torch"):
+            try:
+                mod = __import__(path, fromlist=["allocate_token_bitmask"])
+                allocate = getattr(mod, "allocate_token_bitmask", None)
+                fill = getattr(mod, "fill_next_token_bitmask", None)
+                apply_inplace = getattr(mod, "apply_token_bitmask_inplace", None)
+                if allocate and fill and apply_inplace:
+                    break
+            except Exception:
+                continue
+
+        if not (allocate and fill and apply_inplace):
+            try:
+                self._hf_fallback = self.xgr.contrib.hf.LogitsProcessor(self.compiled_grammar)
+                self.xgrammar_logits_processor = self._hf_fallback
+                return
+            except Exception:
+                # Re-raise the original exception to preserve backward compatibility
+                raise
+
+        self._bitmask = allocate(batch_size, vocab_size)
+
+        def _bias_logits_torch(input_ids, logits):
+            for i in range(self.tensor_adapter.shape(input_ids)[0]):
+                fill(self._matchers[i], self._bitmask, i)
+                apply_inplace(logits[i], self._bitmask[i])
+            return logits
+
+        self._bias_logits = _bias_logits_torch
+
+    def _setup_mlx(self, batch_size: int, vocab_size: int) -> None:
+        """MLX-specific setup for bitmask and bias function."""
+        allocate_np = fill_np = apply_mlx = None
+        for path in ("xgrammar.numpy", "xgrammar.kernels.numpy"):
+            try:
+                mod_np = __import__(path, fromlist=["allocate_token_bitmask"])
+                allocate_np = getattr(mod_np, "allocate_token_bitmask", None)
+                fill_np = getattr(mod_np, "fill_next_token_bitmask", None)
+                if allocate_np and fill_np:
+                    break
+            except Exception:
+                continue
+
+        for path, name in (("xgrammar.mlx", "apply_token_bitmask"),
+                           ("xgrammar.kernels.mlx", "apply_token_bitmask"),
+                           ("xgrammar.kernels.apply_token_bitmask_mlx", "apply_token_bitmask_mlx")):
+            try:
+                mod_mlx = __import__(path, fromlist=[name])
+                apply_mlx = getattr(mod_mlx, name, None)
+                if apply_mlx:
+                    break
+            except Exception:
+                continue
+
+        if not (allocate_np and fill_np and apply_mlx):
+            raise TypeError("XGrammar MLX kernels not available.")
+
+        self._bitmask = allocate_np(batch_size, vocab_size)
+
+        def _bias_logits_mlx(input_ids, logits):
+            biased_rows = []
+            for i in range(self.tensor_adapter.shape(input_ids)[0]):
+                fill_np(self._matchers[i], self._bitmask, i)
+                biased = apply_mlx(logits[i], self._bitmask[i])
+                biased_rows.append(biased)
+            return self.tensor_adapter.concatenate(biased_rows)
+
+        self._bias_logits = _bias_logits_mlx
+
+    def _consume_token(self, matcher, token_id: int) -> None:
+        """Advance matcher state with the accepted token."""
+        if hasattr(matcher, "consume_token"):
+            matcher.consume_token(token_id)
+        elif hasattr(matcher, "accept_token"):
+            matcher.accept_token(token_id)
+        elif hasattr(matcher, "advance"):
+            try:
+                matcher.advance(token_id)
+            except TypeError:
+                matcher.advance(token_id=token_id, return_tokens=False)
+
     def process_logits(self, input_ids: TensorType, logits: TensorType) -> TensorType:
-        """Bias the logits."""
-        if self.xgrammar_logits_processor is None:
-            self.xgrammar_logits_processor = self.xgr.contrib.hf.LogitsProcessor(
-                self.compiled_grammar
-            )
-        return self.xgrammar_logits_processor(input_ids, logits) # type: ignore
+        """Apply grammar constraints to logits and return biased logits."""
+        if self._using_fallback and self.fallback_processor is not None:
+            return self.fallback_processor.process_logits(input_ids, logits)
+
+        if self.track_memory_usage:
+            memory_before = self._get_memory_usage()
+
+        try:
+            batch_size = self.tensor_adapter.shape(input_ids)[0]
+            vocab_size = self.tensor_adapter.shape(logits)[1]
+
+            if self.is_first_token and self.max_batch_size is not None:
+                if batch_size > self.max_batch_size:
+                    raise ValueError(
+                        f"Batch size {batch_size} exceeds maximum allowed batch size {self.max_batch_size}"
+                    )
+
+            if self.temperature != 1.0:
+                logits = logits / self.temperature
+
+            if self.is_first_token:
+                self._setup(batch_size, vocab_size)
+                self.is_first_token = False
+            else:
+                if self._matchers is not None:
+                    for i in range(batch_size):
+                        last_token = self.tensor_adapter.to_scalar(input_ids[i][-1])
+                        self._consume_token(self._matchers[i], int(last_token))
+
+            if self._hf_fallback is not None:
+                result = self._hf_fallback(input_ids, logits)
+            else:
+                assert self._bias_logits is not None
+                result = self._bias_logits(input_ids, logits)
+
+            if self.track_memory_usage:
+                memory_after = self._get_memory_usage()
+                self._update_memory_stats(memory_after)
+                if self.memory_stats["total_calls"] % 10 == 0:
+                    self.logger.info(
+                        f"XGrammar memory - Current: {memory_after:.2f}MB, "
+                        f"Peak: {self.memory_stats['peak_memory_mb']:.2f}MB"
+                    )
+
+            return result
+
+        except Exception as e:
+            logger.warning(f"XGrammar processor failed: {e}. Switching to fallback.")
+            if self.fallback_processor is not None:
+                self._using_fallback = True
+                return self.fallback_processor.process_logits(input_ids, logits)
+            else:
+                raise
 
 
 class XGrammarBackend(BaseBackend):
     """Backend for XGRammar."""
 
-    def __init__(self, model: SteerableModel):
+    def __init__(self, model: SteerableModel, cache_compiled_grammars: bool = False):
         """
         Parameters
         ----------
         model
             The Outlines model of the user.
-
+        cache_compiled_grammars
+            Enable in-memory caching of compiled grammars. Defaults to False.
         """
         import xgrammar as xgr
         from transformers import AutoConfig
@@ -77,58 +327,129 @@ class XGrammarBackend(BaseBackend):
             vocab_size=vocab_size
         )
         self.grammar_compiler = xgr.GrammarCompiler(tokenizer_info)
+        self.tensor_library_name = model.tensor_library_name
+        self._cache_enabled = bool(cache_compiled_grammars)
+        self._grammar_cache = {}
+        self._cache_lock = threading.Lock() if self._cache_enabled else None
+
+    def _compile_with_cache(self, kind: str, source: str, compiler_func):
+        """Compile a grammar with optional caching."""
+        if not self._cache_enabled:
+            return compiler_func(source)
+        key_material = f"{kind}\0{source}".encode("utf-8")
+        cache_key = hashlib.sha256(key_material).hexdigest()
+        assert self._cache_lock is not None
+        with self._cache_lock:
+            if cache_key in self._grammar_cache:
+                return self._grammar_cache[cache_key]
+            compiled = compiler_func(source)
+            self._grammar_cache[cache_key] = compiled
+            return compiled
 
     def get_json_schema_logits_processor(
-        self, json_schema: str
+        self,
+        json_schema: str,
+        fallback_or_temp: Union["OutlinesLogitsProcessor", float, None] = None,
+        temperature: float = 1.0,
+        max_batch_size: Optional[int] = None,
+        tensor_library_name: Optional[str] = None,
+        custom_adapter: Optional[TensorAdapter] = None,
+        validate_grammar: Optional[Callable[[str], None]] = None,
+        fallback_processor: Optional["OutlinesLogitsProcessor"] = None,
+        track_memory_usage: bool = False
     ) -> "LogitsProcessor":
-        """Create a logits processor from a JSON schema.
-
-        Parameters
-        ----------
-        json_schema: str
-            The JSON schema to create a logits processor from.
+        """Create a logits processor from a JSON schema."""
+        actual_fallback = fallback_processor
+        actual_temperature = temperature
+        if isinstance(fallback_or_temp, OutlinesLogitsProcessor):
+            actual_fallback = fallback_or_temp
+        elif isinstance(fallback_or_temp, (int, float)):
+            actual_temperature = float(fallback_or_temp)
 
-        Returns
-        -------
-        LogitsProcessor
-            The logits processor to use to constrain the generation.
+        if validate_grammar is not None:
+            validate_grammar(json_schema)
 
-        """
-        compiled_grammar = self.grammar_compiler.compile_json_schema(
-            json_schema
+        compiled_grammar = self._compile_with_cache(
+            "json", json_schema, self.grammar_compiler.compile_json_schema
+        )
+        return XGrammarLogitsProcessor(
+            compiled_grammar,
+            tensor_library_name or self.tensor_library_name,
+            temperature=actual_temperature,
+            max_batch_size=max_batch_size,
+            custom_adapter=custom_adapter,
+            fallback_processor=actual_fallback,
+            track_memory_usage=track_memory_usage
         )
-        return XGrammarLogitsProcessor(compiled_grammar)
-
-    def get_regex_logits_processor(self, regex: str) -> "LogitsProcessor":
-        """Create a logits processor from a regex.
-
-        Parameters
-        ----------
-        regex: str
-            The regex to create a logits processor from.
 
-        Returns
-        -------
-        LogitsProcessor
-            The logits processor to use to constrain the generation.
+    def get_regex_logits_processor(
+        self,
+        regex: str,
+        fallback_or_temp: Union["OutlinesLogitsProcessor", float, None] = None,
+        temperature: float = 1.0,
+        max_batch_size: Optional[int] = None,
+        tensor_library_name: Optional[str] = None,
+        custom_adapter: Optional[TensorAdapter] = None,
+        validate_grammar: Optional[Callable[[str], None]] = None,
+        fallback_processor: Optional["OutlinesLogitsProcessor"] = None,
+        track_memory_usage: bool = False
+    ) -> "LogitsProcessor":
+        """Create a logits processor from a regex."""
+        actual_fallback = fallback_processor
+        actual_temperature = temperature
+        if isinstance(fallback_or_temp, OutlinesLogitsProcessor):
+            actual_fallback = fallback_or_temp
+        elif isinstance(fallback_or_temp, (int, float)):
+            actual_temperature = float(fallback_or_temp)
 
-        """
-        compiled_grammar = self.grammar_compiler.compile_regex(regex)
-        return XGrammarLogitsProcessor(compiled_grammar)
+        if validate_grammar is not None:
+            validate_grammar(regex)
 
-    def get_cfg_logits_processor(self, grammar: str) -> "LogitsProcessor":
-        """Create a logits processor from a context-free grammar.
+        compiled_grammar = self._compile_with_cache(
+            "regex", regex, self.grammar_compiler.compile_regex
+        )
+        return XGrammarLogitsProcessor(
+            compiled_grammar,
+            tensor_library_name or self.tensor_library_name,
+            temperature=actual_temperature,
+            max_batch_size=max_batch_size,
+            custom_adapter=custom_adapter,
+            fallback_processor=actual_fallback,
+            track_memory_usage=track_memory_usage
+        )
 
-        Parameters
-        ----------
-        grammar: str
-            The context-free grammar to create a logits processor from.
+    def get_cfg_logits_processor(
+        self,
+        grammar: str,
+        fallback_or_temp: Union["OutlinesLogitsProcessor", float, None] = None,
+        temperature: float = 1.0,
+        max_batch_size: Optional[int] = None,
+        tensor_library_name: Optional[str] = None,
+        custom_adapter: Optional[TensorAdapter] = None,
+        validate_grammar: Optional[Callable[[str], None]] = None,
+        fallback_processor: Optional["OutlinesLogitsProcessor"] = None,
+        track_memory_usage: bool = False
+    ) -> "LogitsProcessor":
+        """Create a logits processor from a context-free grammar."""
+        actual_fallback = fallback_processor
+        actual_temperature = temperature
+        if isinstance(fallback_or_temp, OutlinesLogitsProcessor):
+            actual_fallback = fallback_or_temp
+        elif isinstance(fallback_or_temp, (int, float)):
+            actual_temperature = float(fallback_or_temp)
 
-        Returns
-        -------
-        LogitsProcessor
-            The logits processor to use to constrain the generation.
+        if validate_grammar is not None:
+            validate_grammar(grammar)
 
-        """
-        compiled_grammar = self.grammar_compiler.compile_grammar(grammar)
-        return XGrammarLogitsProcessor(compiled_grammar)
+        compiled_grammar = self._compile_with_cache(
+            "cfg", grammar, self.grammar_compiler.compile_grammar
+        )
+        return XGrammarLogitsProcessor(
+            compiled_grammar,
+            tensor_library_name or self.tensor_library_name,
+            temperature=actual_temperature,
+            max_batch_size=max_batch_size,
+            custom_adapter=custom_adapter,
+            fallback_processor=actual_fallback,
+            track_memory_usage=track_memory_usage
+        )
diff --git a/outlines/processors/base_logits_processor.py b/outlines/processors/base_logits_processor.py
index 9b4e4fe5..f25f5032 100644
--- a/outlines/processors/base_logits_processor.py
+++ b/outlines/processors/base_logits_processor.py
@@ -28,8 +28,9 @@ class OutlinesLogitsProcessor:
         ----------
         tensor_library_name
             The name of the library to use to manipulate tensors. Possible
-            values are "jax", "mlx", "numpy", "tensorflow" and "torch". You
-            must choose the library that your model is using.
+            values are "jax", "mlx", "numpy", "tensorflow", "torch", and "custom". 
+            You must choose the library that your model is using. When using "custom",
+            the tensor_adapter should be set manually by the subclass.
         """
         # Temporary fix as torch raises a warning that can cause can an error
         # with python 3.12.
@@ -38,6 +39,11 @@ class OutlinesLogitsProcessor:
 
             torch._dynamo.config.suppress_errors = True
 
+        # Skip tensor adapter initialization for custom adapters
+        if tensor_library_name == "custom":
+            # tensor_adapter will be set by the subclass
+            return
+
         tensor_adapter_class = tensor_adapters.get(tensor_library_name)
         if tensor_adapter_class is None:
             raise NotImplementedError(
diff --git a/outlines/processors/tensor_adapters/__init__.py b/outlines/processors/tensor_adapters/__init__.py
index 8e3afe7f..2b7458af 100644
--- a/outlines/processors/tensor_adapters/__init__.py
+++ b/outlines/processors/tensor_adapters/__init__.py
@@ -2,6 +2,7 @@
 
 from typing import Union
 
+from .base import TensorAdapter
 from .jax import JAXTensorAdapter
 from .mlx import MLXTensorAdapter
 from .numpy import NumpyTensorAdapter
@@ -23,4 +24,5 @@ TensorAdapterImplementation = Union[
     NumpyTensorAdapter,
     TensorFlowTensorAdapter,
     TorchTensorAdapter,
+    TensorAdapter,
 ]

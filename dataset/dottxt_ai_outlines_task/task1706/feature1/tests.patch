diff --git a/tests/backends/test_xgrammar.py b/tests/backends/test_xgrammar.py
index 1133b9f0..b0d7ef28 100644
--- a/tests/backends/test_xgrammar.py
+++ b/tests/backends/test_xgrammar.py
@@ -1,20 +1,28 @@
 import pytest
+import numpy as np
 
 import llama_cpp
 import transformers
 from xgrammar import GrammarCompiler
 
-from outlines.backends.xgrammar import XGrammarLogitsProcessor
+from outlines.backends.xgrammar import XGrammarLogitsProcessor, XGrammarBackend
+from outlines.processors.base_logits_processor import OutlinesLogitsProcessor
 
 import outlines
-from outlines.backends.xgrammar import XGrammarBackend
 
 try:
     import mlx_lm
+    import mlx.core as mx
     HAS_MLX = True
 except ImportError:
     HAS_MLX = False
 
+try:
+    import torch
+    HAS_TORCH = True
+except ImportError:
+    HAS_TORCH = False
+
 
 @pytest.fixture
 def model_transformers():
@@ -34,6 +42,8 @@ def model_llamacpp():
 
 @pytest.fixture
 def model_mlxlm():
+    if not HAS_MLX:
+        pytest.skip("MLX not available")
     return outlines.from_mlxlm(
         *mlx_lm.load("mlx-community/SmolLM-135M-Instruct-4bit")
     )
@@ -57,6 +67,18 @@ root ::= answer
 answer ::= "yes" | "no"
 """
 
+@pytest.fixture
+def compiled_grammar(json_schema):
+    """Create a compiled grammar for testing."""
+    import xgrammar as xgr
+    from transformers import AutoTokenizer
+ 
+    tokenizer = AutoTokenizer.from_pretrained("erwanf/gpt2-mini")
+    vocab_size = len(tokenizer.get_vocab())
+    tokenizer_info = xgr.TokenizerInfo.from_huggingface(tokenizer, vocab_size=vocab_size)
+    compiler = xgr.GrammarCompiler(tokenizer_info)
+    return compiler.compile_json_schema(json_schema)
+
 
 def test_xgrammar_backend(model_transformers, json_schema, regex, cfg):
     # initialization
@@ -87,9 +109,226 @@ def test_xgrammar_backend(model_transformers, json_schema, regex, cfg):
     assert response == "yes" or response == "no"
 
 
-def test_xgrammar_backend_invalid_model(model_llamacpp):
-    with pytest.raises(
-        ValueError,
-        match="The xgrammar backend only supports Transformers models",
-    ):
-        XGrammarBackend(model_llamacpp)
+# XGrammarLogitsProcessor API Tests
+class TestXGrammarLogitsProcessor:
+    """Test the XGrammarLogitsProcessor API as specified in feature.md."""
+
+    def test_processor_initialization(self, compiled_grammar):
+        """Test processor initialization with compiled grammar and tensor library name."""
+        # Test torch initialization
+        processor = XGrammarLogitsProcessor(compiled_grammar, "torch")
+        assert processor.compiled_grammar == compiled_grammar
+        assert processor.tensor_library_name == "torch"
+        assert processor.is_first_token is True
+        assert isinstance(processor, OutlinesLogitsProcessor)
+
+        # Test mlx initialization if available
+        if HAS_MLX:
+            processor_mlx = XGrammarLogitsProcessor(compiled_grammar, "mlx")
+            assert processor_mlx.tensor_library_name == "mlx"
+
+    def test_processor_reset(self, compiled_grammar):
+        """Test that reset() properly resets processor state."""
+        processor = XGrammarLogitsProcessor(compiled_grammar, "torch")
+ 
+        # Simulate processing to change internal state
+        if HAS_TORCH:
+            input_ids = torch.randint(0, 100, (1, 5))
+            logits = torch.randn(1, 100)
+            processor.process_logits(input_ids, logits)
+            # After processing, should no longer be first token
+            assert processor.is_first_token is False
+ 
+        # Reset should restore initial state
+        processor.reset()
+        assert processor.is_first_token is True
+ 
+        # After reset, should behave as if it's the first token again
+        if HAS_TORCH:
+            input_ids = torch.randint(0, 100, (1, 5))
+            logits = torch.randn(1, 100)
+            result = processor.process_logits(input_ids, logits)
+            # Should successfully process and change state again
+            assert processor.is_first_token is False
+            assert result.shape == logits.shape
+
+    @pytest.mark.skipif(not HAS_TORCH, reason="PyTorch not available")
+    def test_processor_torch_tensors(self, compiled_grammar):
+        """Test process_logits with torch tensors."""
+        processor = XGrammarLogitsProcessor(compiled_grammar, "torch")
+ 
+        # Create mock torch tensors
+        batch_size, seq_len, vocab_size = 2, 5, 1000
+        input_ids = torch.randint(0, vocab_size, (batch_size, seq_len))
+        logits = torch.randn(batch_size, vocab_size)
+ 
+        # Process logits should return tensor of same shape
+        result = processor.process_logits(input_ids, logits)
+        assert result.shape == logits.shape
+        assert isinstance(result, torch.Tensor)
+
+    @pytest.mark.skipif(not HAS_MLX, reason="MLX not available")
+    def test_processor_mlx_tensors(self, compiled_grammar, monkeypatch):
+        """Test process_logits with MLX tensors."""
+        from unittest.mock import MagicMock, patch
+ 
+        # Mock the specific modules that would be imported
+        mock_numpy_module = MagicMock()
+        mock_numpy_module.allocate_token_bitmask = MagicMock(return_value="mock_bitmask")
+        mock_numpy_module.fill_next_token_bitmask = MagicMock()
+ 
+        mock_mlx_module = MagicMock()
+        mock_mlx_module.apply_token_bitmask_mlx = MagicMock(side_effect=lambda x, mask: x)
+ 
+        # Patch the specific import paths
+        with patch.dict('sys.modules', {
+            'xgrammar.numpy': mock_numpy_module,
+            'xgrammar.kernels.numpy': mock_numpy_module,
+            'xgrammar.kernels.apply_token_bitmask_mlx': mock_mlx_module
+        }):
+            processor = XGrammarLogitsProcessor(compiled_grammar, "mlx")
+ 
+            # Create mock MLX tensors
+            batch_size, seq_len, vocab_size = 2, 5, 1000
+            input_ids = mx.random.randint(0, vocab_size, (batch_size, seq_len))
+            logits = mx.random.normal((batch_size, vocab_size))
+ 
+            # Mock the tensor adapter's concatenate method to return the original logits
+            original_concatenate = processor.tensor_adapter.concatenate
+            processor.tensor_adapter.concatenate = MagicMock(return_value=logits)
+ 
+            # Process logits should return tensor of same shape
+            result = processor.process_logits(input_ids, logits)
+            assert result.shape == logits.shape
+            assert isinstance(result, mx.array)
+ 
+            # Restore original method
+            processor.tensor_adapter.concatenate = original_concatenate
+
+    def test_processor_unsupported_tensor_library(self, compiled_grammar):
+        """Test error handling for unsupported tensor libraries."""
+        # The error is raised at the base class level during initialization
+        with pytest.raises(NotImplementedError, match="Library unsupported is not available"):
+            XGrammarLogitsProcessor(compiled_grammar, "unsupported")
+
+    def test_processor_state_management(self, compiled_grammar):
+        """Test first token processing and state transitions."""
+        processor = XGrammarLogitsProcessor(compiled_grammar, "torch")
+ 
+        # Initially should be first token
+        assert processor.is_first_token is True
+ 
+        if HAS_TORCH:
+            # After first process_logits call, should no longer be first token
+            input_ids = torch.randint(0, 100, (1, 5))
+            logits = torch.randn(1, 100)
+            processor.process_logits(input_ids, logits)
+            assert processor.is_first_token is False
+
+
+# XGrammarBackend API Tests
+class TestXGrammarBackend:
+    """Test the XGrammarBackend API as specified in feature.md."""
+
+    def test_backend_transformers_support(self, model_transformers):
+        """Test backend initialization with Transformers models."""
+        backend = XGrammarBackend(model_transformers)
+        assert isinstance(backend.grammar_compiler, GrammarCompiler)
+        assert backend.tensor_library_name == "torch"
+
+    def test_backend_tokenizer_extraction_transformers(self, model_transformers):
+        """Test tokenizer extraction from Transformers models."""
+        backend = XGrammarBackend(model_transformers)
+        # Backend should successfully extract tokenizer and create compiler
+        assert backend.grammar_compiler is not None
+
+    def test_backend_processor_creation(self, model_transformers, json_schema, regex, cfg):
+        """Test that backend creates processors with correct tensor library."""
+        backend = XGrammarBackend(model_transformers)
+ 
+        # JSON schema processor
+        json_processor = backend.get_json_schema_logits_processor(json_schema)
+        assert isinstance(json_processor, XGrammarLogitsProcessor)
+        assert json_processor.tensor_library_name == backend.tensor_library_name
+ 
+        # Regex processor
+        regex_processor = backend.get_regex_logits_processor(regex)
+        assert isinstance(regex_processor, XGrammarLogitsProcessor)
+        assert regex_processor.tensor_library_name == backend.tensor_library_name
+ 
+        # CFG processor
+        cfg_processor = backend.get_cfg_logits_processor(cfg)
+        assert isinstance(cfg_processor, XGrammarLogitsProcessor)
+        assert cfg_processor.tensor_library_name == backend.tensor_library_name
+
+    def test_backend_unsupported_model_error_message(self, model_llamacpp):
+        """Test that error message mentions supported model types."""
+        with pytest.raises(ValueError) as exc_info:
+            XGrammarBackend(model_llamacpp)
+ 
+        error_msg = str(exc_info.value)
+        assert "Transformers" in error_msg
+
+    def test_backend_method_signatures_compatibility(self, model_transformers, json_schema, regex, cfg):
+        """Test that all public methods maintain expected signatures for backward compatibility."""
+        backend = XGrammarBackend(model_transformers)
+ 
+        # These should not raise any signature-related errors
+        json_processor = backend.get_json_schema_logits_processor(json_schema)
+        regex_processor = backend.get_regex_logits_processor(regex)
+        cfg_processor = backend.get_cfg_logits_processor(cfg)
+ 
+        # All should return XGrammarLogitsProcessor instances
+        assert all(isinstance(p, XGrammarLogitsProcessor) for p in [json_processor, regex_processor, cfg_processor])
+
+
+# Integration Tests
+class TestXGrammarIntegration:
+    """Test end-to-end functionality with different model types."""
+
+    def test_transformers_json_generation(self, model_transformers, json_schema):
+        """Test JSON generation with Transformers model."""
+        backend = XGrammarBackend(model_transformers)
+        processor = backend.get_json_schema_logits_processor(json_schema)
+        generator = outlines.Generator(model_transformers, backend="xgrammar", processor=processor)
+ 
+        response = generator("Generate a person:")
+        # Should generate valid JSON structure
+        assert response.startswith("{")
+        assert "name" in response or "age" in response
+
+    def test_transformers_regex_generation(self, model_transformers, regex):
+        """Test regex generation with Transformers model."""
+        backend = XGrammarBackend(model_transformers)
+        processor = backend.get_regex_logits_processor(regex)
+        generator = outlines.Generator(model_transformers, backend="xgrammar", processor=processor)
+ 
+        response = generator("Generate a 3-digit number:")
+        # Should match regex pattern
+        assert len(response) == 3
+        assert response.isdigit()
+
+    def test_transformers_cfg_generation(self, model_transformers, cfg):
+        """Test CFG generation with Transformers model."""
+        backend = XGrammarBackend(model_transformers)
+        processor = backend.get_cfg_logits_processor(cfg)
+        generator = outlines.Generator(model_transformers, backend="xgrammar", processor=processor)
+ 
+        response = generator("Answer yes or no:")
+        # Should match CFG constraints
+        assert response in ["yes", "no"]
+
+    # Note: MLXLM integration tests are not included yet as the current implementation
+    # only supports Transformers models. These will be added when MLXLM support is implemented.
+
+    def test_backward_compatibility(self, model_transformers, json_schema):
+        """Test that existing Transformers models continue to work without changes."""
+        # This should work exactly as before
+        backend = XGrammarBackend(model_transformers)
+        processor = backend.get_json_schema_logits_processor(json_schema)
+        generator = outlines.Generator(model_transformers, backend="xgrammar", processor=processor)
+ 
+        # Should generate without errors
+        response = generator("Hello, how are you?")
+        assert isinstance(response, str)
+        assert len(response) > 0

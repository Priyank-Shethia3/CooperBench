diff --git a/outlines/backends/xgrammar.py b/outlines/backends/xgrammar.py
index fcb1672d..64606f44 100644
--- a/outlines/backends/xgrammar.py
+++ b/outlines/backends/xgrammar.py
@@ -15,39 +15,200 @@ if TYPE_CHECKING:
 
 
 class XGrammarLogitsProcessor(OutlinesLogitsProcessor):
-    """Logits processor for XGrammar.
-
-    This class wraps the `xgr.contrib.hf.LogitsProcessor` class and adds
-    a `reset` method to reset the logits processor for a new generation.
+    """Custom logits processor for XGrammar with multi-tensor support.
 
+    This processor uses XGrammar's core matcher and token-bitmask APIs to
+    constrain logits according to a compiled grammar. It supports multiple
+    tensor libraries (currently torch and mlx) and maintains backward
+    compatibility with existing Transformers models.
     """
 
-    def __init__(self, compiled_grammar: str):
-        """
+    def __init__(self, compiled_grammar: str, tensor_library_name: str):
+        """Initialize with compiled grammar and tensor library type.
+
         Parameters
         ----------
         compiled_grammar: str
-            The compiled grammar to use to create the logits processor.
-
+            The compiled grammar produced by `xgrammar.GrammarCompiler`.
+        tensor_library_name: str
+            The tensor library used by the model (e.g., "torch", "mlx").
         """
         import xgrammar as xgr
 
         self.xgr = xgr
         self.compiled_grammar = compiled_grammar
-        self.xgrammar_logits_processor = None
-        super().__init__("torch")
+        self.tensor_library_name = tensor_library_name
+
+        self.is_first_token = True
+        self._matchers = None
+        self._bitmask = None
+        self._bias_logits = None
+        self._hf_fallback = None  # used only if low-level API is unavailable
+
+        super().__init__(tensor_library_name)
 
     def reset(self):
-        """Reset the logits processor for a new generation."""
-        self.xgrammar_logits_processor = None
+        """Reset processor state for new generation sequence."""
+        self.is_first_token = True
+        self._matchers = None
+        self._bitmask = None
+        self._bias_logits = None
+        self._hf_fallback = None
 
-    def process_logits(self, input_ids: TensorType, logits: TensorType) -> TensorType:
-        """Bias the logits."""
-        if self.xgrammar_logits_processor is None:
-            self.xgrammar_logits_processor = self.xgr.contrib.hf.LogitsProcessor(
-                self.compiled_grammar
+    def _setup(self, batch_size: int, vocab_size: int) -> None:
+        """Initialize matchers, bitmask and bias function on first token."""
+        # Initialize GrammarMatchers per batch item
+        try:
+            GrammarMatcher = getattr(self.xgr, "GrammarMatcher")
+            self._matchers = [GrammarMatcher(self.compiled_grammar) for _ in range(batch_size)]
+        except Exception:
+            # As a last resort, fall back to the HF-compatible processor for torch
+            # This preserves backward compatibility in environments where the
+            # lower-level API isn't available.
+            try:
+                self._hf_fallback = self.xgr.contrib.hf.LogitsProcessor(self.compiled_grammar)
+                return
+            except Exception as e:
+                raise RuntimeError(
+                    "Failed to initialize XGrammar matcher. Ensure xgrammar is up to date."
+                ) from e
+
+        # Tensor-library specific setup
+        lib = self.tensor_library_name
+        if lib == "torch":
+            self._setup_torch(batch_size, vocab_size)
+        elif lib == "mlx":
+            self._setup_mlx(batch_size, vocab_size)
+        else:
+            # Other tensor libs are not supported by XGrammar backend
+            raise TypeError(f"Unsupported tensor library for XGrammar: {lib}")
+
+    def _setup_torch(self, batch_size: int, vocab_size: int) -> None:
+        """Torch-specific setup for bitmask and bias function."""
+        # Try several import locations to maximize compatibility across xgrammar versions
+        allocate = fill = apply_inplace = None
+        for path in (
+            "xgrammar.torch",
+            "xgrammar.kernels.torch",
+            "xgr.torch",
+        ):
+            try:
+                mod = __import__(path, fromlist=["allocate_token_bitmask", "fill_next_token_bitmask", "apply_token_bitmask_inplace"])  # type: ignore
+                allocate = getattr(mod, "allocate_token_bitmask", None)
+                fill = getattr(mod, "fill_next_token_bitmask", None)
+                apply_inplace = getattr(mod, "apply_token_bitmask_inplace", None)
+                if allocate and fill and apply_inplace:
+                    break
+            except Exception:
+                continue
+
+        if not (allocate and fill and apply_inplace):
+            # Fallback to HF processor if available
+            try:
+                self._hf_fallback = self.xgr.contrib.hf.LogitsProcessor(self.compiled_grammar)
+                return
+            except Exception as e:
+                raise RuntimeError(
+                    "XGrammar torch kernels not available and HF fallback failed."
+                ) from e
+
+        self._bitmask = allocate(batch_size, vocab_size)
+
+        def _bias_logits_torch(input_ids: TensorType, logits: TensorType) -> TensorType:
+            for i in range(self.tensor_adapter.shape(input_ids)[0]):
+                fill(self._matchers[i], self._bitmask, i)  # type: ignore
+                apply_inplace(logits[i], self._bitmask[i])  # type: ignore
+            return logits
+
+        self._bias_logits = _bias_logits_torch
+
+    def _setup_mlx(self, batch_size: int, vocab_size: int) -> None:
+        """MLX-specific setup for bitmask and bias function."""
+        # We use numpy bitmask buffer and MLX apply kernel
+        allocate_np = fill_np = apply_mlx = None
+
+        # numpy helpers
+        for path in ("xgrammar.numpy", "xgrammar.kernels.numpy"):
+            try:
+                mod_np = __import__(path, fromlist=["allocate_token_bitmask", "fill_next_token_bitmask"])  # type: ignore
+                allocate_np = getattr(mod_np, "allocate_token_bitmask", None)
+                fill_np = getattr(mod_np, "fill_next_token_bitmask", None)
+                if allocate_np and fill_np:
+                    break
+            except Exception:
+                continue
+
+        # mlx kernel
+        # Try a few common paths; different xgrammar versions may expose different symbols
+        for path, name in (
+            ("xgrammar.mlx", "apply_token_bitmask"),
+            ("xgrammar.kernels.mlx", "apply_token_bitmask"),
+            ("xgrammar.kernels.apply_token_bitmask_mlx", "apply_token_bitmask_mlx"),
+        ):
+            try:
+                mod_mlx = __import__(path, fromlist=[name])  # type: ignore
+                apply_mlx = getattr(mod_mlx, name, None)
+                if apply_mlx:
+                    break
+            except Exception:
+                continue
+
+        if not (allocate_np and fill_np and apply_mlx):
+            raise TypeError(
+                "XGrammar MLX kernels not available; ensure xgrammar provides MLX support."
             )
-        return self.xgrammar_logits_processor(input_ids, logits) # type: ignore
+
+        self._bitmask = allocate_np(batch_size, vocab_size)
+
+        def _bias_logits_mlx(input_ids: TensorType, logits: TensorType) -> TensorType:
+            biased_rows = []
+            for i in range(self.tensor_adapter.shape(input_ids)[0]):
+                fill_np(self._matchers[i], self._bitmask, i)  # type: ignore
+                biased = apply_mlx(logits[i], self._bitmask[i])  # type: ignore
+                biased_rows.append(biased)
+            return self.tensor_adapter.concatenate(biased_rows)
+
+        self._bias_logits = _bias_logits_mlx
+
+    def _consume_token(self, matcher, token_id: int) -> None:
+        """Advance matcher state with the accepted token, trying common APIs."""
+        # Try a few method names/signatures to maximize compatibility
+        if hasattr(matcher, "consume_token"):
+            matcher.consume_token(token_id)
+            return
+        if hasattr(matcher, "accept_token"):
+            matcher.accept_token(token_id)
+            return
+        if hasattr(matcher, "advance"):
+            try:
+                matcher.advance(token_id)
+            except TypeError:
+                matcher.advance(token_id=token_id, return_tokens=False)
+            return
+        # If no known method, ignore; low-level kernels may still use only bitmask fills
+
+    def process_logits(self, input_ids: TensorType, logits: TensorType) -> TensorType:
+        """Apply grammar constraints to logits and return biased logits."""
+        batch_size = self.tensor_adapter.shape(input_ids)[0]
+        vocab_size = self.tensor_adapter.shape(logits)[1]
+
+        if self.is_first_token:
+            self._setup(batch_size, vocab_size)
+            self.is_first_token = False
+        else:
+            # Accept last generated token for each sequence
+            if self._matchers is not None:
+                for i in range(batch_size):
+                    last_token = self.tensor_adapter.to_scalar(input_ids[i][-1])  # type: ignore
+                    self._consume_token(self._matchers[i], int(last_token))
+
+        # If we had to fall back to HF-compatible processor, delegate
+        if self._hf_fallback is not None:
+            return self._hf_fallback(input_ids, logits)  # type: ignore
+
+        # Otherwise, use the selected tensor-library-specific path
+        assert self._bias_logits is not None
+        return self._bias_logits(input_ids, logits)
 
 
 class XGrammarBackend(BaseBackend):
@@ -77,6 +238,8 @@ class XGrammarBackend(BaseBackend):
             vocab_size=vocab_size
         )
         self.grammar_compiler = xgr.GrammarCompiler(tokenizer_info)
+        # Track tensor library for processor creation (torch for Transformers)
+        self.tensor_library_name = model.tensor_library_name
 
     def get_json_schema_logits_processor(
         self, json_schema: str
@@ -97,7 +260,7 @@ class XGrammarBackend(BaseBackend):
         compiled_grammar = self.grammar_compiler.compile_json_schema(
             json_schema
         )
-        return XGrammarLogitsProcessor(compiled_grammar)
+        return XGrammarLogitsProcessor(compiled_grammar, self.tensor_library_name)
 
     def get_regex_logits_processor(self, regex: str) -> "LogitsProcessor":
         """Create a logits processor from a regex.
@@ -114,7 +277,7 @@ class XGrammarBackend(BaseBackend):
 
         """
         compiled_grammar = self.grammar_compiler.compile_regex(regex)
-        return XGrammarLogitsProcessor(compiled_grammar)
+        return XGrammarLogitsProcessor(compiled_grammar, self.tensor_library_name)
 
     def get_cfg_logits_processor(self, grammar: str) -> "LogitsProcessor":
         """Create a logits processor from a context-free grammar.
@@ -131,4 +294,4 @@ class XGrammarBackend(BaseBackend):
 
         """
         compiled_grammar = self.grammar_compiler.compile_grammar(grammar)
-        return XGrammarLogitsProcessor(compiled_grammar)
+        return XGrammarLogitsProcessor(compiled_grammar, self.tensor_library_name)
